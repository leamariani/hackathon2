{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **HACKATHON 2 : LLM fine-tuné pour l’Analyse de Sentiment autour du climat et les Réponses Contextuelles**"
      ],
      "metadata": {
        "id": "gCaW_SHQlaLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un monde saturé de messages sur le climat, les opinions s’expriment partout : réseaux sociaux, forums, blogs, commentaires…\n",
        "Mais comment savoir ce que les gens ressentent vraiment, ce qu’ils demandent, et comment réagir intelligemment ?\n",
        "\n",
        "Ce projet propose un pipeline IA complet pour comprendre en profondeur les discours climatiques en ligne, grâce à :\n",
        "\n",
        "* L’analyse de sentiment fine (positif, neutre, négatif)\n",
        "\n",
        "* La détection d’intention (plainte, question, engagement…)\n",
        "\n",
        "Et la génération de réponses adaptées au ton et au contexte.\n",
        "\n",
        "Objectif métier : permettre aux acteurs de la communication, de la RSE ou de la veille sociétale d’extraire du sens et d’agir rapidement face aux signaux faibles.\n",
        "\n"
      ],
      "metadata": {
        "id": "BJOefjiulBEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. **Module Core - core_modules.py**"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py - Configuration optimisée\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralisée et optimisée\"\"\"\n",
        "    model_name: str = \"distilbert-base-uncased\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 2e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    output_dir: str = \"outputs/final_model\"\n",
        "\n",
        "    # Configuration Q&A\n",
        "    qa_model: str = \"all-MiniLM-L6-v2\"\n",
        "    similarity_threshold: float = 0.3\n",
        "    max_results: int = 5"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d66356-abbd-4742-8ff0-5eb3c7157f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Centrale – `core_modules.py`\n",
        "\n",
        "Ce module définit une classe `ClimateConfig` qui regroupe tous les paramètres nécessaires au fonctionnement du pipeline IA. Grâce à l'utilisation de `@dataclass`, cette configuration est propre, facilement modifiable, et rend le code plus modulaire.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et Démarche\n",
        "- Centraliser tous les paramètres dans un seul fichier pour éviter leur dispersion dans le code\n",
        "- Faciliter les ajustements lors des tests (modèles, batch size, seuils, etc.)\n",
        "- Garantir la reproductibilité et la cohérence des résultats\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et Challenges\n",
        "- Organiser des paramètres très variés (modèle, LoRA, Q&A)\n",
        "- Assurer la flexibilité pour différents cas d’usage\n",
        "- Permettre un fine-tuning léger et efficace\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des Choix Techniques\n",
        "\n",
        "| Paramètre | Rôle | Justification |\n",
        "|----------|------|----------------|\n",
        "| `model_name` | Modèle de base | `distilbert-base-uncased` : léger et efficace |\n",
        "| `max_length` | Longueur max. des séquences | 128 tokens suffisent pour tweets/commentaires |\n",
        "| `batch_size` | Taille des lots | 16 = équilibre mémoire/performance |\n",
        "| `epochs` | Nb d’itérations d’entraînement | 3 = limite le surapprentissage |\n",
        "| `learning_rate` | Taux d’apprentissage | 2e-5 = valeur courante pour transformers |\n",
        "| `lora_r` & `lora_alpha` | Paramètres PEFT | Optimisation fine avec LoRA |\n",
        "| `qa_model` | Modèle de similarité sémantique | `all-MiniLM-L6-v2` : rapide et robuste |\n",
        "| `similarity_threshold` | Seuil de Q&A | 0.3 = bon équilibre entre pertinence et bruit |\n",
        "| `max_results` | Nb max de réponses suggérées | 5 = clair pour l'utilisateur |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation\n",
        "Cette configuration structure l'ensemble du pipeline : classification de sentiments, détection d’intentions, génération de réponse et Q&A. Elle garantit cohérence, reproductibilité et efficacité.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices Métier\n",
        "- Paramétrage centralisé, facile à ajuster\n",
        "- Gain de temps en expérimentation\n",
        "- Réutilisable pour d’autres contextes (RH, politique, produits…)\n",
        "- Compatible avec Streamlit pour un déploiement rapide\n"
      ],
      "metadata": {
        "id": "ESY88Eo3m2dN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. **Module Data Processing - data_modules.py**"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "        self.reverse_label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        text_keywords = ['text', 'content', 'message', 'comment', 'body', 'description', 'self_text']\n",
        "        label_keywords = ['label', 'sentiment', 'category', 'class', 'target', 'comment_sentiment']\n",
        "        text_col = next((c for c in df.columns if any(k in str(c).lower() for k in text_keywords)), None)\n",
        "        label_col = next((c for c in df.columns if any(k in str(c).lower() for k in label_keywords)), None)\n",
        "        if not text_col:\n",
        "            text_col = df.select_dtypes(include=['object']).columns[0]\n",
        "        if not label_col:\n",
        "            label_col = df.columns[-1]\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '', 'null']:\n",
        "            return None\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'&gt;|&lt;|&amp;', lambda m: {'&gt;': '>', '&lt;': '<', '&amp;': '&'}[m.group()], text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip() if text.strip() else None\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "        df_clean = df[[self.text_col, self.label_col]].copy()\n",
        "        df_clean.columns = ['text', 'label']\n",
        "        df_clean['text'] = df_clean['text'].apply(self.clean_text)\n",
        "        df_clean['label'] = df_clean['label'].astype(str)\n",
        "        df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "        df_clean = df_clean[df_clean['text'].str.len() >= 10]\n",
        "\n",
        "        if len(df_clean) > sample_size:\n",
        "            df_clean = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        unique_labels = sorted(df_clean['label'].unique())\n",
        "        self.label_mapping = {str(l): i for i, l in enumerate(unique_labels)}\n",
        "        df_clean['label_id'] = df_clean['label'].map(self.label_mapping)\n",
        "\n",
        "        # Nettoyage final NaN\n",
        "        df_clean = df_clean.dropna(subset=['label_id'])\n",
        "        df_clean['label_id'] = df_clean['label_id'].astype(int)\n",
        "\n",
        "        if df_clean.empty:\n",
        "            raise ValueError(\"❌ Aucune donnée valide après nettoyage.\")\n",
        "\n",
        "        train_df, temp = train_test_split(df_clean, test_size=0.4, stratify=df_clean['label_id'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label_id'], random_state=42)\n",
        "\n",
        "        return (\n",
        "            Dataset.from_pandas(train_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(val_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(test_df[['text', 'label_id']])\n",
        "        )"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ca7507-c2bc-43a3-b0bf-6b4078afd0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Préparation des Données – `data_modules.py`\n",
        "\n",
        "Ce module définit la classe `DataProcessor`, responsable de la **détection automatique des colonnes**, du **nettoyage du texte**, et de la **préparation des datasets** pour l'entraînement, la validation et le test.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et Démarche\n",
        "- Faciliter l'intégration de **n'importe quel fichier de données textuelles** sans devoir spécifier manuellement les colonnes.\n",
        "- Nettoyer le texte pour enlever les parasites (liens, caractères HTML, espaces multiples).\n",
        "- Créer des splits équilibrés pour un entraînement robuste (train / val / test).\n",
        "- Mapper automatiquement les labels en identifiants numériques.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et Challenges\n",
        "- Gérer des fichiers CSV aux **colonnes nommées différemment** selon les sources (`message`, `body`, `comment`, etc.).\n",
        "- Nettoyer les textes tout en conservant leur **contenu sémantique pertinent**.\n",
        "- Éviter les **données bruitées ou vides** qui peuvent fausser l’apprentissage.\n",
        "- Préserver un **équilibre des classes** pour garantir la qualité du modèle.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des Choix Techniques\n",
        "\n",
        "| Fonction | Rôle | Pourquoi ce choix |\n",
        "|----------|------|-------------------|\n",
        "| `detect_columns()` | Détection automatique texte/label | Rend le code adaptable à n’importe quelle source |\n",
        "| `clean_text()` | Nettoyage ciblé | Supprime les liens, HTML, espaces, et filtres vides |\n",
        "| `sample_size=8000` | Limite d’échantillon | Contrôle le coût de calcul pour les tests |\n",
        "| `train_test_split()` | Stratification par label | Assure un équilibre de classes dans chaque split |\n",
        "| `Dataset.from_pandas()` | Conversion au bon format | Compatible avec les Transformers de Hugging Face |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du Résultat\n",
        "\n",
        "À partir d’un fichier brut, on obtient :\n",
        "- Trois sous-ensembles (`train`, `val`, `test`) propres et équilibrés\n",
        "- Des textes nettoyés, prêts à être tokenisés\n",
        "- Des labels convertis en entiers (`label_id`) pour la classification\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module permet de transformer automatiquement n'importe quel fichier brut en **données exploitables par un modèle NLP**, avec une qualité de préparation conforme aux standards du Machine Learning.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices Métier\n",
        "- Gain de temps : aucune configuration manuelle nécessaire\n",
        "- Robustesse : nettoyage automatique + gestion des cas extrêmes (données vides, déséquilibre)\n",
        "- Réutilisabilité : fonctionne sur différents jeux de données avec très peu d’adaptations\n",
        "- Sécurité : contrôle intégré des données valides, avec gestion des erreurs\n"
      ],
      "metadata": {
        "id": "KTigalatnifr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. **Module Modèle - model_modules.py**"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float32,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "        }\n",
        "\n",
        "    def setup_training_args(self):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_full_eval=False,\n",
        "            bf16_full_eval=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        return Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.setup_training_args(),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfebadb-018c-41b8-d989-1a65a9789030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Modélisation – `model_modules.py`\n",
        "\n",
        "Ce module contient la classe `ModelManager`, chargée de configurer, entraîner et évaluer un modèle de classification de texte à l’aide de Transformers et de la méthode de fine-tuning légère LoRA.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est de proposer un cadre unifié et automatisé pour :\n",
        "- Initialiser un tokenizer compatible avec le modèle choisi,\n",
        "- Charger un modèle préentraîné pour la classification de séquences,\n",
        "- Appliquer un fine-tuning efficace avec **LoRA (Low-Rank Adaptation)**,\n",
        "- Définir une stratégie d'entraînement robuste et évaluable,\n",
        "- Suivre les performances sur des métriques clés via un `Trainer`.\n",
        "\n",
        "Ce module structure toutes les étapes critiques de modélisation de manière modulaire et réutilisable.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Adapter un grand modèle NLP aux contraintes d’entraînement locales (temps, mémoire GPU),\n",
        "- Permettre un fine-tuning **léger mais performant**, évitant de réentraîner tous les poids,\n",
        "- Assurer une bonne gestion du padding, du truncation, et des tokens spéciaux pour garantir la stabilité,\n",
        "- Obtenir un modèle **généralisable** malgré un jeu de données souvent limité,\n",
        "- Surveiller les performances de façon continue avec un `Trainer` configuré intelligemment.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément | Justification |\n",
        "|--------|---------------|\n",
        "| `AutoTokenizer` et `AutoModelForSequenceClassification` | Compatibilité large avec les modèles Transformers |\n",
        "| `LoRA (PEFT)` | Réduction significative du coût d'entraînement tout en conservant de bonnes performances |\n",
        "| `Trainer` de Hugging Face | Simplifie le processus d'entraînement, de validation et de sauvegarde |\n",
        "| `EarlyStoppingCallback` | Évite le surapprentissage en arrêtant tôt l'entraînement si les performances stagnent |\n",
        "| `compute_metrics()` | Utilise des métriques standard (accuracy, f1, precision, recall) pour une évaluation équilibrée |\n",
        "| `DataCollatorWithPadding` | Gère automatiquement le padding dynamique pour les batchs |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "Une fois exécuté, ce module produit :\n",
        "- Un modèle entraîné, optimisé avec LoRA, prêt pour l’inférence,\n",
        "- Des scores de performance clairs sur le jeu de validation (f1, précision, recall),\n",
        "- Un modèle sauvegardé dans un répertoire dédié, réutilisable pour d’autres prédictions.\n",
        "\n",
        "Les métriques permettent de comparer différents modèles ou configurations de manière objective.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module encapsule l’ensemble de la logique de modélisation dans une structure cohérente, reproductible et modulaire. Il permet un entraînement efficace même avec des ressources limitées, tout en maintenant des standards élevés de performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- Réduction des coûts d'entraînement grâce à LoRA,\n",
        "- Modèle réentraînable facilement sur d'autres thématiques ou jeux de données,\n",
        "- Code structuré facilitant la maintenance, la reproductibilité et l’adaptation à d’autres projets,\n",
        "- Résultats exploitables rapidement pour des applications concrètes : analyse d’opinion, modération, veille.\n"
      ],
      "metadata": {
        "id": "P1vYclLAoL1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. **visualization_modules.py**"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- NLTK / BLEU / ROUGE ---\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Téléchargement silencieux des ressources NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestionnaire de visualisations pour Climate Analyzer.\"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Outils internes BLEU / ROUGE\n",
        "    # --------------------------------------------------\n",
        "    _smoothie = SmoothingFunction().method4\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _bleu(ref: str, hyp: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        ref_tok = nltk.word_tokenize(ref.lower())\n",
        "        hyp_tok = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu([ref_tok], hyp_tok, smoothing_function=VisualizationManager._smoothie)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rouge_score(ref: str, hyp: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        scores = VisualizationManager._rouge_scorer.score(ref.lower(), hyp.lower())\n",
        "        return {'rouge-1': scores['rouge1'].fmeasure,\n",
        "                'rouge-l': scores['rougeL'].fmeasure}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) Courbes d’entraînement\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str = \"outputs/final_model\"):\n",
        "        try:\n",
        "            log_file = os.path.join(log_dir, \"trainer_state.json\")\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"📄 Aucun log d'entraînement trouvé.\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"📉 Aucune donnée d'historique trouvée.\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_acc, eval_f1 = [], [], [], [], []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_acc.append(entry.get('eval_accuracy', 0))\n",
        "                    eval_f1.append(entry.get('eval_f1_weighted', 0))\n",
        "                elif 'train_loss' in entry:\n",
        "                    train_loss.append(entry.get('train_loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"📈 Évolution de l'entraînement\", fontsize=16)\n",
        "\n",
        "            if train_loss and eval_loss:\n",
        "                train_steps = np.linspace(0, max(epochs) if epochs else 1, len(train_loss))\n",
        "                axes[0, 0].plot(train_steps, train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-o', label='Eval Loss', markersize=4)\n",
        "                axes[0, 0].set_title('Loss Evolution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc:\n",
        "                axes[0, 1].plot(epochs[:len(eval_acc)], eval_acc, 'g-o', label='Accuracy', markersize=4)\n",
        "                axes[0, 1].set_title('Accuracy Evolution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_f1:\n",
        "                axes[1, 0].plot(epochs[:len(eval_f1)], eval_f1, 'm-o', label='F1-Weighted', markersize=4)\n",
        "                axes[1, 0].set_title('F1-Score Evolution')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc and eval_f1:\n",
        "                final_metrics = ['Accuracy', 'F1-Score']\n",
        "                final_values = [eval_acc[-1], eval_f1[-1]]\n",
        "                bars = axes[1, 1].bar(final_metrics, final_values, color=['green', 'purple'], alpha=0.7)\n",
        "                axes[1, 1].set_title('Final Metrics')\n",
        "                axes[1, 1].set_ylim(0, 1)\n",
        "                for bar, value in zip(bars, final_values):\n",
        "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) Matrice de confusion\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names: List[str]):\n",
        "        try:\n",
        "            predictions_output = trainer.predict(test_dataset)\n",
        "            predictions = predictions_output.predictions.argmax(axis=1)\n",
        "            true_labels = predictions_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax1)\n",
        "            ax1.set_title(\"Matrice de confusion\")\n",
        "            ax1.set_xlabel(\"Prédictions\")\n",
        "            ax1.set_ylabel(\"Vraies valeurs\")\n",
        "\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax2)\n",
        "            ax2.set_title(\"Matrice de confusion (normalisée)\")\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            report = classification_report(true_labels, predictions,\n",
        "                                         target_names=label_names,\n",
        "                                         output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.subheader(\"📊 Rapport de classification\")\n",
        "            st.dataframe(report_df.round(3))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la matrice de confusion : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3) Distribution des classes\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names: List[str] = None, title: str = \"Distribution des classes\"):\n",
        "        try:\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist()\n",
        "            labels = [int(x) for x in labels]\n",
        "            label_counts = Counter(labels)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            if label_names:\n",
        "                x_labels = [label_names[i] if i < len(label_names) else f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "            else:\n",
        "                x_labels = [f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "\n",
        "            bars = ax.bar(range(len(x_labels)), counts, color=plt.cm.Set3(np.linspace(0, 1, len(x_labels))))\n",
        "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel(\"Classes\")\n",
        "            ax.set_ylabel(\"Nombre d'échantillons\")\n",
        "            ax.set_xticks(range(len(x_labels)))\n",
        "            ax.set_xticklabels(x_labels, rotation=45 if max(map(len, x_labels)) > 10 else 0)\n",
        "\n",
        "            for bar, count in zip(bars, counts):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
        "                       str(count), ha='center', va='bottom')\n",
        "            total = sum(counts)\n",
        "            ax.text(0.02, 0.98, f\"Total: {total}\\nClasses: {len(x_labels)}\",\n",
        "                   transform=ax.transAxes, va='top', ha='left',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage de la distribution : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4) Analyse des résultats Q&A\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_qa_results_analysis(qa_results: List[Dict], question: str):\n",
        "        if not qa_results:\n",
        "            st.info(\"Aucun résultat à analyser\")\n",
        "            return\n",
        "        try:\n",
        "            scores = [r['score'] for r in qa_results]\n",
        "            ranks = [r['rank'] for r in qa_results]\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(f\"Analyse des résultats pour: '{question[:50]}...'\", fontsize=14)\n",
        "\n",
        "            axes[0, 0].hist(scores, bins=min(10, len(scores)), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[0, 0].set_title(\"Distribution des scores de similarité\")\n",
        "            axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Moyenne: {np.mean(scores):.3f}')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "            axes[0, 1].bar(ranks, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[0, 1].set_title(\"Scores par rang\")\n",
        "            axes[0, 1].set_xlabel(\"Rang\")\n",
        "\n",
        "            text_lengths = [len(r['text']) for r in qa_results]\n",
        "            axes[1, 0].scatter(text_lengths, scores, alpha=0.6, color='green')\n",
        "            axes[1, 0].set_title(\"Score vs Longueur du texte\")\n",
        "            axes[1, 0].set_xlabel(\"Longueur du texte\")\n",
        "\n",
        "            top_scores = scores[:min(5, len(scores))]\n",
        "            top_ranks = ranks[:min(5, len(ranks))]\n",
        "            axes[1, 1].barh(range(len(top_scores)), top_scores, color='purple', alpha=0.7)\n",
        "            axes[1, 1].set_title(\"Top 5 des scores\")\n",
        "            axes[1, 1].set_yticks(range(len(top_scores)))\n",
        "            axes[1, 1].set_yticklabels([f\"Rang {r}\" for r in top_ranks])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.subheader(\"📈 Statistiques détaillées\")\n",
        "            stats_df = pd.DataFrame({\n",
        "                \"Métrique\": [\"Score moyen\", \"Score médian\", \"Score max\", \"Score min\", \"Écart-type\"],\n",
        "                \"Valeur\": [np.mean(scores), np.median(scores), np.max(scores), np.min(scores), np.std(scores)]\n",
        "            })\n",
        "            st.dataframe(stats_df.round(4))\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'analyse des résultats Q&A : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5) Méthodes BLEU / ROUGE manquantes\n",
        "    # --------------------------------------------------\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        return self._bleu(reference, candidate)\n",
        "\n",
        "    def calculate_rouge_score(self, reference: str, candidate: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        return self._rouge_score(reference, candidate)\n",
        "\n",
        "    def visualize_bleu_rouge_scores(self, qa_results, references):\n",
        "        \"\"\"Visualisation BLEU & ROUGE pour chaque paire (ref, résultat).\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for ref, res in zip(references, qa_results):\n",
        "            bleus.append(self._bleu(ref, res['text']))\n",
        "            r1s.append(self._rouge_score(ref, res['text'])['rouge-1'])\n",
        "            rls.append(self._rouge_score(ref, res['text'])['rouge-l'])\n",
        "\n",
        "        x = list(range(1, len(bleus)+1))\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar([i-0.2 for i in x], bleus, 0.4, label='BLEU')\n",
        "        plt.bar([i+0.2 for i in x], r1s, 0.4, label='ROUGE-1')\n",
        "        plt.xlabel('Rang')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('BLEU & ROUGE vs références')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "    def evaluate_qa_performance(self, qa_module, questions, references):\n",
        "        \"\"\"Évaluation complète Q-A avec scores BLEU/ROUGE.\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for q, ref in zip(questions, references):\n",
        "            res = qa_module.query_with_fallback(q, top_k=1)\n",
        "            if res:\n",
        "                cand = res[0]['text']\n",
        "                bleus.append(self._bleu(ref, cand))\n",
        "                r1s.append(self._rouge_score(ref, cand)['rouge-1'])\n",
        "                rls.append(self._rouge_score(ref, cand)['rouge-l'])\n",
        "\n",
        "        st.write(\"### 📊 Global Q-A metrics\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Avg BLEU\", f\"{np.mean(bleus):.4f}\")\n",
        "        col2.metric(\"Avg ROUGE-1\", f\"{np.mean(r1s):.4f}\")\n",
        "        col3.metric(\"Avg ROUGE-L\", f\"{np.mean(rls):.4f}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351ea5ad-0d81-4ca3-b979-242c4c8bf88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Visualisation – `visualization_modules.py`\n",
        "\n",
        "Ce module contient la classe `VisualizationManager`, qui regroupe l’ensemble des outils nécessaires pour analyser, visualiser et interpréter les résultats du pipeline IA (entraînement, classification, Q&A).\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est de rendre les résultats **visibles, interprétables et exploitables** à toutes les étapes du pipeline :\n",
        "- Visualisation des courbes d'entraînement pour suivre l'apprentissage du modèle,\n",
        "- Analyse fine de la classification via matrices de confusion et rapports détaillés,\n",
        "- Évaluation des performances des réponses générées (Q&A) avec des scores BLEU et ROUGE,\n",
        "- Affichage intégré dans Streamlit pour une interface interactive tout-en-un.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Faciliter la compréhension des performances sans expertise en machine learning,\n",
        "- Offrir des visualisations pertinentes et adaptées aux cas d’usage texte (classification, Q&A),\n",
        "- Rendre l’analyse compatible avec un **déploiement Streamlit** pour usage métier,\n",
        "- Gérer la diversité des types de résultats (courbes, matrices, histogrammes, rapports).\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Composant | Justification |\n",
        "|-----------|----------------|\n",
        "| `matplotlib`, `seaborn` | Bibliothèques standards et flexibles pour des graphiques clairs |\n",
        "| `Streamlit` | Intégration simple d’éléments visuels dans une application web interactive |\n",
        "| `classification_report` et `confusion_matrix` | Outils classiques pour évaluer la qualité de la classification |\n",
        "| `BLEU` / `ROUGE` (NLTK, rouge_score) | Références en NLP pour comparer textes générés et attendus |\n",
        "| `Counter`, `json`, `os` | Pour l’analyse dynamique de résultats (logs, distributions) |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "Ce module permet de :\n",
        "- Visualiser la dynamique de l’apprentissage (perte, précision, f1),\n",
        "- Comprendre les erreurs du modèle grâce à la matrice de confusion,\n",
        "- Analyser les scores des résultats de questions-réponses selon plusieurs angles,\n",
        "- Quantifier la qualité des réponses générées en les comparant à des références humaines (BLEU/ROUGE),\n",
        "- Explorer la distribution des classes dans les données.\n",
        "\n",
        "Les graphiques facilitent la **prise de décision** pour ajuster le modèle ou identifier ses limites.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module joue un rôle clé pour **rendre l’IA explicable**, accessible et opérationnelle dans un cadre métier. Il transforme des sorties numériques en **indicateurs visuels** utiles pour la validation, l’amélioration continue et la communication autour du projet.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- Permet aux équipes non techniques (RSE, communication, marketing) de **visualiser facilement les résultats**,\n",
        "- Identifie rapidement les classes mal prédites ou les biais du modèle,\n",
        "- Facilite l’évaluation qualitative et quantitative des réponses générées dans des contextes sensibles,\n",
        "- Contribue à une meilleure adoption des solutions IA grâce à une interprétabilité renforcée.\n"
      ],
      "metadata": {
        "id": "rllSG2_FovTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. **qa_modules.py**"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py - Version corrigée et optimisée\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "class OptimizedQAModule:\n",
        "    \"\"\"Version optimisée du module Q&A avec caching et performance améliorée\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(model_name, device='cpu')\n",
        "            self.similarity_threshold = 0.3\n",
        "\n",
        "            # Structures optimisées\n",
        "            self.corpus_texts: List[str] = []\n",
        "            self.corpus_labels: List[int] = []\n",
        "            self.corpus_embeddings: Optional[np.ndarray] = None\n",
        "            self.corpus_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Base de connaissances optimisée\n",
        "            self.knowledge_base: List[Dict[str, Any]] = []\n",
        "            self.kb_embeddings: Optional[np.ndarray] = None\n",
        "            self.kb_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Cache pour les requêtes fréquentes\n",
        "            self._query_cache: Dict[str, List[Dict]] = {}\n",
        "\n",
        "            self._setup_knowledge_base()\n",
        "            logging.info(\"✅ Module Q&A initialisé avec succès\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur initialisation Q&A: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_knowledge_base(self):\n",
        "        \"\"\"Initialisation optimisée de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            {\n",
        "                \"text\": \"Le réchauffement climatique est principalement causé par les émissions de CO2 humaines.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"CO2\", \"émissions\", \"humaines\", \"causes\"],\n",
        "                \"weight\": 1.0\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Les énergies renouvelables (solaire, éolien, hydro) réduisent drastiquement les émissions.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"renouvelables\", \"solaire\", \"éolien\", \"hydro\"],\n",
        "                \"weight\": 1.2\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La déforestation est responsable de 15% des émissions mondiales de CO2.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"déforestation\", \"forêts\", \"15%\"],\n",
        "                \"weight\": 0.9\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Le transport représente 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "                \"category\": \"secteurs\",\n",
        "                \"keywords\": [\"transport\", \"24%\", \"véhicules\"],\n",
        "                \"weight\": 1.1\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"L'isolation thermique peut réduire la consommation énergétique jusqu'à 50%.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"isolation\", \"thermique\", \"50%\"],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "        ]\n",
        "        self._rebuild_kb_index()\n",
        "\n",
        "    def _rebuild_kb_index(self):\n",
        "        \"\"\"Reconstruction optimisée de l'index FAISS\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            texts = [item[\"text\"] for item in self.knowledge_base]\n",
        "            self.kb_embeddings = self.model.encode(\n",
        "                texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "            # Index FAISS optimisé\n",
        "            d = self.kb_embeddings.shape[1]\n",
        "            self.kb_index = faiss.IndexFlatIP(d)\n",
        "            self.kb_index.add(self.kb_embeddings.astype(np.float32))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur reconstruction index: {e}\")\n",
        "\n",
        "    def fit(self, dataset: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Indexation optimisée du corpus d'entraînement\"\"\"\n",
        "        try:\n",
        "            if not dataset:\n",
        "                logging.warning(\"⚠️ Dataset vide, rien à indexer\")\n",
        "                return False\n",
        "\n",
        "            self.corpus_texts = [d[\"text\"] for d in dataset]\n",
        "            self.corpus_labels = [d.get(\"label_id\", 0) for d in dataset]\n",
        "\n",
        "            # Encodage optimisé avec batch processing\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(self.corpus_texts), batch_size):\n",
        "                batch = self.corpus_texts[i:i+batch_size]\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_numpy=True\n",
        "                )\n",
        "                embeddings.append(batch_embeddings)\n",
        "\n",
        "            self.corpus_embeddings = np.vstack(embeddings)\n",
        "\n",
        "            # Index FAISS\n",
        "            d = self.corpus_embeddings.shape[1]\n",
        "            self.corpus_index = faiss.IndexFlatIP(d)\n",
        "            self.corpus_index.add(self.corpus_embeddings.astype(np.float32))\n",
        "\n",
        "            # Mise à jour du cache\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"✅ {len(dataset)} documents indexés avec succès\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur indexation: {e}\")\n",
        "            return False\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def _get_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Cache des embeddings de requêtes fréquentes\"\"\"\n",
        "        return self.model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "    def _search_index(self, query: str, index: faiss.Index, texts: List[str],\n",
        "                     source: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans un index FAISS\"\"\"\n",
        "        if index is None or not texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_embedding = self._get_query_embedding(query)\n",
        "            scores, indices = index.search(\n",
        "                query_embedding.reshape(1, -1).astype(np.float32),\n",
        "                min(top_k, len(texts))\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "            for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
        "                if idx < len(texts) and score > self.similarity_threshold:\n",
        "                    result = {\n",
        "                        \"text\": texts[idx],\n",
        "                        \"score\": float(score),\n",
        "                        \"rank\": rank,\n",
        "                        \"source\": source\n",
        "                    }\n",
        "\n",
        "                    # Ajout des métadonnées si disponible\n",
        "                    if source == \"knowledge_base\" and idx < len(self.knowledge_base):\n",
        "                        result.update({\n",
        "                            \"category\": self.knowledge_base[idx][\"category\"],\n",
        "                            \"keywords\": self.knowledge_base[idx][\"keywords\"]\n",
        "                        })\n",
        "                    elif source == \"training_corpus\" and idx < len(self.corpus_labels):\n",
        "                        result[\"label_id\"] = int(self.corpus_labels[idx])\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche index: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_knowledge_base(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans la base de connaissances\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return []\n",
        "        return self._search_index(query, self.kb_index,\n",
        "                                [item[\"text\"] for item in self.knowledge_base],\n",
        "                                \"knowledge_base\", top_k)\n",
        "\n",
        "    def query_corpus(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimisée dans le corpus d'entraînement\"\"\"\n",
        "        if not self.corpus_texts:\n",
        "            return []\n",
        "        return self._search_index(query, self.corpus_index, self.corpus_texts,\n",
        "                                \"training_corpus\", top_k)\n",
        "\n",
        "    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par mots-clés optimisée avec scoring avancé\"\"\"\n",
        "        if not query:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "            if not query_words:\n",
        "                return []\n",
        "\n",
        "            scored = []\n",
        "\n",
        "            # Recherche dans la base de connaissances\n",
        "            for item in self.knowledge_base:\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', item[\"text\"].lower()))\n",
        "                keyword_words = set(item[\"keywords\"])\n",
        "\n",
        "                # Score combiné TF-IDF like\n",
        "                text_score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "                keyword_score = len(query_words & keyword_words) / max(len(keyword_words), 1)\n",
        "                combined_score = (text_score * 0.7 + keyword_score * 0.3) * item.get(\"weight\", 1.0)\n",
        "\n",
        "                if combined_score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": item[\"text\"],\n",
        "                        \"category\": item[\"category\"],\n",
        "                        \"keywords\": item[\"keywords\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"source\": \"knowledge_base\"\n",
        "                    })\n",
        "\n",
        "            # Recherche dans le corpus\n",
        "            for i, text in enumerate(self.corpus_texts):\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "                score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "\n",
        "                if score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": text,\n",
        "                        \"label_id\": int(self.corpus_labels[i]),\n",
        "                        \"score\": score,\n",
        "                        \"source\": \"training_corpus\"\n",
        "                    })\n",
        "\n",
        "            scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i, result in enumerate(scored[:top_k], 1):\n",
        "                result[\"rank\"] = i\n",
        "\n",
        "            return scored[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche mots-clés: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_with_fallback(self, question: str, top_k: int = 5,\n",
        "                          search_mode: str = \"hybrid\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche avec fallback optimisé\"\"\"\n",
        "        # Clé de cache\n",
        "        cache_key = f\"{question}_{search_mode}_{top_k}\"\n",
        "        if cache_key in self._query_cache:\n",
        "            return self._query_cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            # Sélection du mode de recherche\n",
        "            if search_mode == \"knowledge_only\":\n",
        "                results = self.query_knowledge_base(question, top_k)\n",
        "            elif search_mode == \"corpus_only\":\n",
        "                results = self.query_corpus(question, top_k)\n",
        "            elif search_mode == \"keywords\":\n",
        "                results = self.keyword_search(question, top_k)\n",
        "            else:  # hybrid\n",
        "                kb_results = self.query_knowledge_base(question, top_k // 2 + 1)\n",
        "                corpus_results = self.query_corpus(question, top_k // 2 + 1)\n",
        "\n",
        "                # Fusion et déduplication\n",
        "                seen_texts = set()\n",
        "                results = []\n",
        "\n",
        "                for item in kb_results + corpus_results:\n",
        "                    if item[\"text\"] not in seen_texts:\n",
        "                        seen_texts.add(item[\"text\"])\n",
        "                        results.append(item)\n",
        "\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Fallback si nécessaire\n",
        "            if not results or (results and max(r[\"score\"] for r in results) < self.similarity_threshold):\n",
        "                fallback = self.keyword_search(question, top_k)\n",
        "                seen = {r[\"text\"] for r in results}\n",
        "                for item in fallback:\n",
        "                    if item[\"text\"] not in seen:\n",
        "                        results.append(item)\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Mise en cache\n",
        "            self._query_cache[cache_key] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche avec fallback: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_by_category(self, category: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par catégorie optimisée\"\"\"\n",
        "        try:\n",
        "            results = [\n",
        "                {\n",
        "                    \"text\": item[\"text\"],\n",
        "                    \"category\": item[\"category\"],\n",
        "                    \"keywords\": item[\"keywords\"],\n",
        "                    \"source\": \"knowledge_base\",\n",
        "                    \"rank\": i + 1,\n",
        "                    \"score\": 1.0\n",
        "                }\n",
        "                for i, item in enumerate([\n",
        "                    it for it in self.knowledge_base\n",
        "                    if it[\"category\"].lower() == category.lower()\n",
        "                ][:top_k])\n",
        "            ]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur recherche catégorie: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, text: str, category: str = \"custom\",\n",
        "                     keywords: List[str] = None) -> bool:\n",
        "        \"\"\"Ajout optimisé de nouvelles connaissances\"\"\"\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return False\n",
        "\n",
        "            # Vérification des doublons\n",
        "            if any(item[\"text\"].strip() == text.strip() for item in self.knowledge_base):\n",
        "                return False\n",
        "\n",
        "            new_item = {\n",
        "                \"text\": text.strip(),\n",
        "                \"category\": category,\n",
        "                \"keywords\": keywords or [],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "\n",
        "            self.knowledge_base.append(new_item)\n",
        "            self._rebuild_kb_index()\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"✅ Nouvelle connaissance ajoutée: {text[:50]}...\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur ajout connaissance: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_categories(self) -> List[str]:\n",
        "        \"\"\"Retourne les catégories disponibles\"\"\"\n",
        "        try:\n",
        "            return list({item[\"category\"] for item in self.knowledge_base})\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Statistiques détaillées et optimisées\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                \"knowledge_base_size\": len(self.knowledge_base),\n",
        "                \"training_corpus_size\": len(self.corpus_texts),\n",
        "                \"total_documents\": len(self.knowledge_base) + len(self.corpus_texts),\n",
        "                \"categories\": self.get_categories(),\n",
        "                \"avg_kb_length\": np.mean([len(item[\"text\"]) for item in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "                \"avg_corpus_length\": np.mean([len(t) for t in self.corpus_texts]) if self.corpus_texts else 0,\n",
        "                \"cache_size\": len(self._query_cache)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Nettoyage manuel du cache\"\"\"\n",
        "        self._query_cache.clear()\n",
        "        logging.info(\"🗑️ Cache vidé\")"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ffb7a4-ab03-44d4-aedd-31d5ff4885ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Q&A Optimisé – `qa_modules.py`\n",
        "\n",
        "Ce module définit la classe `OptimizedQAModule`, qui met en œuvre un moteur de questions-réponses sémantique, basé sur la recherche vectorielle avec FAISS et l'encodage de texte via Sentence Transformers.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est d'offrir un système intelligent capable de :\n",
        "- Répondre automatiquement à des questions en langage naturel,\n",
        "- S’appuyer à la fois sur un corpus d’entraînement et une base de connaissances manuelle,\n",
        "- Utiliser des embeddings sémantiques pour trouver les textes les plus pertinents,\n",
        "- Fournir des résultats rapides, cohérents et justifiés, même avec un petit jeu de données.\n",
        "\n",
        "Le module repose sur des stratégies hybrides combinant recherche vectorielle, mots-clés, catégories et cache intelligent.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Trouver une **représentation vectorielle efficace** du langage naturel,\n",
        "- Gérer la **performance et la rapidité** des requêtes avec FAISS,\n",
        "- Construire une base de connaissances **explicable et modifiable**,\n",
        "- Répondre de manière robuste même en cas de question inédite (grâce au fallback),\n",
        "- Optimiser les ressources via le **caching** et la gestion des doublons.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément | Justification |\n",
        "|--------|----------------|\n",
        "| `SentenceTransformer` (`MiniLM`) | Modèle rapide et précis pour la similarité sémantique |\n",
        "| `FAISS` | Moteur de recherche vectorielle ultra-performant, adapté aux grands volumes |\n",
        "| `query_with_fallback()` | Combine plusieurs stratégies de recherche pour maximiser la robustesse |\n",
        "| `knowledge_base` | Base structurée avec catégories, mots-clés, pondérations |\n",
        "| `lru_cache` + `query_cache` | Accélération des requêtes répétées, amélioration des performances |\n",
        "| `keyword_search()` | Alternative lexicale si la recherche sémantique échoue |\n",
        "| `add_knowledge()` | Permet d’enrichir dynamiquement la base sans redéployer le système |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "Le module retourne une **liste classée de réponses pertinentes**, issues :\n",
        "- Soit de la base de connaissances (texte + métadonnées),\n",
        "- Soit du corpus d’entraînement,\n",
        "- Avec un score de similarité et un rang attribué à chaque résultat.\n",
        "\n",
        "Il est possible de filtrer par catégorie, d’analyser les performances (via d’autres modules) ou d’ajouter de nouvelles données en direct.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module constitue le cœur sémantique du système : il permet de **transformer une simple question en une réponse intelligente**, fondée sur les connaissances disponibles. Il est à la fois **modulaire**, **interprétable**, et **personnalisable**.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- Automatisation de la recherche d’information dans des textes non structurés,\n",
        "- Gain de temps pour les analystes, communicants ou chargés de mission,\n",
        "- Réponses contextualisées même sans supervision humaine,\n",
        "- Extension facile pour s’adapter à de nouveaux domaines (RH, juridique, environnement…),\n",
        "- Outil stratégique pour la veille, la médiation, ou la communication de crise.\n"
      ],
      "metadata": {
        "id": "Zlfd7s-1pP8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. **Module Knowledge Base - knowledge_modules.py**"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le réchauffement climatique est principalement causé par les émissions de gaz à effet de serre d'origine humaine.\",\n",
        "            \"Les énergies renouvelables comme le solaire et l'éolien sont essentielles pour décarboner notre économie.\",\n",
        "            \"La déforestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports représente environ 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "            \"L'amélioration de l'efficacité énergétique des bâtiments peut réduire jusqu'à 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et régénératrice peut séquestrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les océans absorbent 25% du CO2 atmosphérique mais s'acidifient, menaçant les écosystèmes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises à réduire leurs émissions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'atténuation des émissions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralité carbone.\"\n",
        "        ]\n",
        "        print(\"✅ Base de connaissances initialisée avec recherche par mots-clés\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarité textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarité basé sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score décroissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"✅ Nouvelle connaissance ajoutée: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2088e44-fc08-4ffe-bc9e-28d6eae12078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Base de Connaissances Simple – `knowledge_modules.py`\n",
        "\n",
        "Ce module propose une version allégée de moteur de recherche de connaissances, basée uniquement sur une similarité lexicale (sans embeddings), utilisable pour des cas simples ou des environnements sans GPU ni dépendances lourdes.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est de fournir une **base de connaissances consultable** rapidement, même dans un contexte sans infrastructure complexe. Cette version ne repose pas sur des modèles comme `sentence-transformers`, mais sur des techniques classiques de traitement de texte (nettoyage, tokenisation, score de similarité par Jaccard).\n",
        "\n",
        "Elle permet de répondre à une question en retrouvant les phrases les plus proches lexicalement, selon les mots en commun.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Permettre une **recherche légère**, rapide à exécuter et facile à interpréter,\n",
        "- Gérer des documents courts, de type \"faits\" ou \"connaissances environnementales\",\n",
        "- Éviter la complexité des modèles sémantiques tout en conservant une pertinence minimale,\n",
        "- Trouver un compromis acceptable entre **simplicité** et **qualité des résultats**.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément | Justification |\n",
        "|--------|----------------|\n",
        "| Liste Python de phrases | Structure simple, modifiable à chaud, idéale pour des données statiques |\n",
        "| Score de Jaccard | Mesure classique des similarités lexicales (intersection / union) |\n",
        "| Seuil à 0.1 | Permet de filtrer les résultats trop faibles ou bruités |\n",
        "| `find_context()` | Fonction centrale pour extraire les 3 connaissances les plus proches |\n",
        "| `add_knowledge()` | Possibilité d’enrichir dynamiquement la base |\n",
        "| `get_stats()` | Outil utile pour évaluer la couverture et la densité de la base |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "À chaque requête, le module retourne **jusqu'à 3 phrases** de la base considérées comme pertinentes, avec un score implicite basé sur les mots en commun avec la question. Ce système est robuste pour des requêtes simples ou guidées, moins adapté à des formulations complexes ou implicites.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module constitue une **brique légère, interprétable et autonome** du système. Il peut servir de fallback ou de moteur Q&A de premier niveau, utilisable dans des environnements contraints (API rapide, edge computing, notebook pédagogique, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- **Facile à intégrer** dans une application ou un outil low-code,\n",
        "- **Aucune dépendance lourde** : compatible avec des infrastructures minimales,\n",
        "- **Maintenance simple** : la base peut être enrichie ou modifiée par des profils non techniques,\n",
        "- Peut servir de moteur de recherche de **FAQ, définitions ou glossaire** dans un contexte métier ou éducatif.\n"
      ],
      "metadata": {
        "id": "fHUWrpLKpqtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. **update_kb_rss.py**"
      ],
      "metadata": {
        "id": "duM3V0Q02w8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile update_kb_rss.py\n",
        "import feedparser\n",
        "from qa_modules import OptimizedQAModule\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import schedule\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Configuration des URLs RSS valides\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.carbonbrief.org/feed/\",\n",
        "    \"https://climate.nasa.gov/news/rss.xml\",\n",
        "    \"https://unfccc.int/news/rss.xml\"\n",
        "]\n",
        "\n",
        "class RSSUpdater:\n",
        "    def __init__(self, qa_module):\n",
        "        self.qa = qa_module\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=400,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\", \". \", \"? \", \"! \"]\n",
        "        )\n",
        "\n",
        "    def fetch_and_process_feed(self, feed_url, max_entries=5):\n",
        "        \"\"\"Récupère et traite un flux RSS\"\"\"\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            if feed.bozo:\n",
        "                logging.warning(f\"⚠️ Flux RSS mal formaté: {feed_url}\")\n",
        "                return 0\n",
        "\n",
        "            processed = 0\n",
        "            for entry in feed.entries[:max_entries]:\n",
        "                try:\n",
        "                    # Extraction des informations\n",
        "                    title = entry.title if hasattr(entry, 'title') else \"Sans titre\"\n",
        "                    summary = entry.summary if hasattr(entry, 'summary') else \"\"\n",
        "                    link = entry.link if hasattr(entry, 'link') else \"Lien manquant\"\n",
        "\n",
        "                    # Nettoyage du texte\n",
        "                    content = f\"{title}. {summary}\".strip()\n",
        "                    if len(content) < 50:  # Skip trop courts\n",
        "                        continue\n",
        "\n",
        "                    # Création du document\n",
        "                    doc = Document(\n",
        "                        page_content=content,\n",
        "                        metadata={\n",
        "                            \"url\": link,\n",
        "                            \"title\": title,\n",
        "                            \"source\": \"rss_feed\",\n",
        "                            \"date\": datetime.datetime.now().isoformat()\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    # Découpage en chunks\n",
        "                    chunks = self.text_splitter.split_documents([doc])\n",
        "\n",
        "                    # Ajout à la base de connaissances\n",
        "                    for chunk in chunks:\n",
        "                        success = self.qa.add_knowledge(\n",
        "                            text=chunk.page_content,\n",
        "                            category=\"rss_news\",\n",
        "                            keywords=[\"rss\", \"news\", \"climate\", \"update\"]\n",
        "                        )\n",
        "                        if success:\n",
        "                            processed += 1\n",
        "                            logging.info(f\"✅ Ajouté: {title[:60]}...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"❌ Erreur traitement article: {e}\")\n",
        "                    continue\n",
        "\n",
        "            return processed\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"❌ Erreur flux RSS {feed_url}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def update_all_feeds(self):\n",
        "        \"\"\"Mise à jour de tous les flux RSS\"\"\"\n",
        "        total_added = 0\n",
        "        logging.info(f\"🔄 Début mise à jour RSS - {datetime.datetime.now()}\")\n",
        "\n",
        "        for feed_url in RSS_FEEDS:\n",
        "            added = self.fetch_and_process_feed(feed_url)\n",
        "            total_added += added\n",
        "            logging.info(f\"📊 {feed_url}: {added} articles ajoutés\")\n",
        "\n",
        "        logging.info(f\"✅ Mise à jour terminée - Total: {total_added} nouveaux articles\")\n",
        "        return total_added\n",
        "\n",
        "    def start_scheduler(self):\n",
        "        \"\"\"Démarre le planificateur RSS\"\"\"\n",
        "        # Planification quotidienne à 9h\n",
        "        schedule.every().day.at(\"09:00\").do(self.update_all_feeds)\n",
        "\n",
        "        # Test immédiat\n",
        "        self.update_all_feeds()\n",
        "\n",
        "        logging.info(\"📅 Planificateur RSS démarré - mise à jour quotidienne à 09:00\")\n",
        "\n",
        "        # Boucle d'exécution\n",
        "        while True:\n",
        "            schedule.run_pending()\n",
        "            time.sleep(3600)  # Vérification toutes les heures\n",
        "\n",
        "# Fonction utilitaire pour Streamlit\n",
        "def init_rss_updater(qa_module):\n",
        "    \"\"\"Initialise le RSS updater pour Streamlit\"\"\"\n",
        "    updater = RSSUpdater(qa_module)\n",
        "    return updater\n",
        "\n",
        "def manual_rss_update(qa_module):\n",
        "    \"\"\"Mise à jour manuelle via Streamlit\"\"\"\n",
        "    try:\n",
        "        updater = RSSUpdater(qa_module)\n",
        "        return updater.update_all_feeds()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Erreur mise à jour RSS: {e}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "L726nrJs2wq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfae87c-419f-4e0f-b69a-e94483713618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing update_kb_rss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de mise à jour RSS – `update_kb_rss.py`\n",
        "\n",
        "Ce module permet d’enrichir dynamiquement la base de connaissances à partir de flux d’actualités RSS fiables sur le climat (NASA, UNFCCC, CarbonBrief), en utilisant le moteur Q&A sémantique déjà en place.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est de rendre le système **vivant et actualisé**, en automatisant l’ajout de contenus récents, crédibles et contextualisés dans la base de connaissances.  \n",
        "Le module extrait, nettoie, découpe et indexe chaque article de manière autonome, afin de rendre les nouvelles informations immédiatement interrogeables via le module Q&A.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Accéder à des contenus récents **sans API complexe** (RSS = standard léger et universel),\n",
        "- Nettoyer et découper les articles pour produire des **chunks exploitables sémantiquement**,\n",
        "- Filtrer les contenus trop courts ou bruités,\n",
        "- **Préserver la cohérence** sémantique des ajouts (catégorisation, mots-clés),\n",
        "- Intégrer les nouvelles connaissances **sans duplication** ni perte de performances.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément | Rôle / justification |\n",
        "|---------|----------------------|\n",
        "| `feedparser` | Lecture universelle de flux RSS au format XML |\n",
        "| `langchain.text_splitter` | Découpe optimisée en chunks de 400 tokens avec recouvrement |\n",
        "| `Document(metadata=...)` | Standardisation des articles sous forme structurée |\n",
        "| `add_knowledge()` (Q&A) | Intégration immédiate dans l’index sémantique FAISS |\n",
        "| `schedule` | Planification automatisée quotidienne (9h) |\n",
        "| `manual_rss_update()` | Fonction de déclenchement manuel depuis Streamlit |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "Chaque exécution ajoute **des dizaines de nouveaux documents** (articles, paragraphes) dans la base, indexés avec score, catégorie, date et provenance.  \n",
        "Lorsqu’un utilisateur pose une question, ces nouvelles connaissances seront automatiquement proposées en priorité si elles sont pertinentes.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module transforme le moteur Q&A en **assistant d’actualités climatiques**, toujours au courant des derniers développements, sans effort manuel.  \n",
        "Il permet un **alignement continu avec l’actualité**, tout en conservant un système robuste et modulaire.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- Accès à une **veille climatique automatisée**, intégrée au moteur IA,\n",
        "- Enrichissement progressif sans réentraînement du modèle,\n",
        "- Réduction du **temps de collecte et de structuration** d’informations externes,\n",
        "- Meilleure **réactivité stratégique** pour les communicants, journalistes, ONG ou collectivités,\n",
        "- Base de connaissances toujours fraîche, sans dépendre d’un expert technique.\n"
      ],
      "metadata": {
        "id": "pZpztvk_qIJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. **Module Streamlit - streamlit_app.py**"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app_fusion.py - Version fusionnée avec optimisations et RSS\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from contextlib import contextmanager\n",
        "from qa_modules import OptimizedQAModule  # Module optimisé\n",
        "\n",
        "# Configuration optimisée de Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"🌍 Climate Analyzer Pro\",\n",
        "    page_icon=\"🌍\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Barre de progression persistante\n",
        "# ---------------------------------------------------------\n",
        "@contextmanager\n",
        "def st_progress(title=\"Progress\", max_value=100):\n",
        "    bar = st.progress(0, text=title)\n",
        "    try:\n",
        "        yield bar\n",
        "    finally:\n",
        "        bar.empty()\n",
        "\n",
        "# Cache optimisé pour le module Q&A\n",
        "@st.cache_resource\n",
        "def get_optimized_qa():\n",
        "    \"\"\"Cache du module Q&A optimisé\"\"\"\n",
        "    return OptimizedQAModule()\n",
        "\n",
        "# Persistance session_state - Initialisation correcte\n",
        "if \"trainer\" not in st.session_state:\n",
        "    st.session_state.trainer = None\n",
        "if \"label_names\" not in st.session_state:\n",
        "    st.session_state.label_names = None\n",
        "if \"test_ds\" not in st.session_state:\n",
        "    st.session_state.test_ds = None\n",
        "if \"training\" not in st.session_state:\n",
        "    st.session_state.training = False\n",
        "if \"raw_train_data\" not in st.session_state:\n",
        "    st.session_state.raw_train_data = None\n",
        "if \"qa_module\" not in st.session_state:\n",
        "    st.session_state.qa_module = get_optimized_qa()  # Utilisation du module optimisé\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from visualization_modules import VisualizationManager\n",
        "\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "\n",
        "        # Fusion des modules Q&A\n",
        "        if st.session_state.qa_module is None:\n",
        "            st.session_state.qa_module = get_optimized_qa()\n",
        "\n",
        "        self.qa_module = st.session_state.qa_module\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.load_saved_model()\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Chargement automatique du modèle si déjà présent\"\"\"\n",
        "        if st.session_state.trainer is None and os.path.exists(\"outputs/final_model/config.json\"):\n",
        "            try:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                num_labels = len(self.data_processor.label_mapping) or 2\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                trainer = self.model_manager.setup_trainer(None, None)\n",
        "                trainer.model = trainer.model.from_pretrained(\"outputs/final_model\")\n",
        "                st.session_state.trainer = trainer\n",
        "                st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "                st.success(\"✅ Modèle chargé depuis le disque.\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"⚠️ Chargement impossible : {e}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Menu principal de l'application\"\"\"\n",
        "        st.title(\"🌍 Climate Sentiment Analyzer Pro\")\n",
        "\n",
        "        # Sidebar optimisée avec statistiques Q&A\n",
        "        with st.sidebar:\n",
        "            st.markdown(\"### 📊 Statistiques Système\")\n",
        "\n",
        "            stats = self.qa_module.get_stats()\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.metric(\"Base de connaissances\", stats[\"knowledge_base_size\"])\n",
        "            with col2:\n",
        "                st.metric(\"Total documents\", stats[\"total_documents\"])\n",
        "\n",
        "            # Contrôles rapides\n",
        "            if st.button(\"🗑️ Vider le cache Q&A\"):\n",
        "                self.qa_module.clear_cache()\n",
        "                st.success(\"Cache vidé!\")\n",
        "                st.rerun()\n",
        "\n",
        "            # Navigation principale\n",
        "            mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"🚀 Pipeline Complet\", \"❓ Q&A Avancée\", \"📚 Gestion des Connaissances\",\n",
        "                 \"📰 Mise à jour RSS\", \"📈 Visualisations\"]\n",
        "            )\n",
        "\n",
        "        # Routage vers les différentes sections\n",
        "        if mode == \"🚀 Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"❓ Q&A Avancée\":\n",
        "            self.run_advanced_qa_interface()\n",
        "        elif mode == \"📚 Gestion des Connaissances\":\n",
        "            self.run_knowledge_management()\n",
        "        elif mode == \"📰 Mise à jour RSS\":\n",
        "            self.run_rss_integration()\n",
        "        elif mode == \"📈 Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Pipeline complet d'entraînement (inchangé mais optimisé)\"\"\"\n",
        "        st.header(\"🚀 Pipeline Complet\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\"Téléchargez votre CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            # SLIDERS avec valeurs par défaut optimisées\n",
        "            sample_size = st.slider(\"Taille échantillon\", 1000, 10000, value=4000)\n",
        "            self.config.epochs = st.slider(\"Epochs\", 1, 5, value=3)\n",
        "\n",
        "            is_training = st.session_state.get(\"training\", False)\n",
        "\n",
        "            if st.button(\n",
        "                \"🚀 Lancer l'entraînement\",\n",
        "                type=\"primary\",\n",
        "                disabled=bool(is_training)\n",
        "            ):\n",
        "                st.session_state.training = True\n",
        "                try:\n",
        "                    self.train_pipeline(df, sample_size)\n",
        "                finally:\n",
        "                    st.session_state.training = False\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame, sample_size: int):\n",
        "        \"\"\"Processus d'entraînement optimisé\"\"\"\n",
        "        try:\n",
        "            # 1/4 — Analyse des données avec cache\n",
        "            with st_progress(\"1/4  Analyse des données …\") as bar:\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "                bar.progress(25)\n",
        "\n",
        "            # Sauvegarder les données brutes\n",
        "            raw_train_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                            for item in train_ds]\n",
        "            st.session_state.raw_train_data = raw_train_data\n",
        "\n",
        "            # 2/4 — Tokenizer\n",
        "            with st_progress(\"2/4  Chargement du tokenizer …\") as bar:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                bar.progress(50)\n",
        "\n",
        "            # 3/4 — Modèle\n",
        "            with st_progress(\"3/4  Initialisation du modèle …\") as bar:\n",
        "                num_labels = len(self.data_processor.label_mapping)\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                bar.progress(75)\n",
        "\n",
        "            # 4/4 — Tokenisation optimisée\n",
        "            def prep(ds):\n",
        "                with st_progress(\"4/4  Tokenisation …\") as bar:\n",
        "                    ds = ds.map(\n",
        "                        self.model_manager.tokenize_function,\n",
        "                        batched=True,\n",
        "                        desc=\"Tokenisation\"\n",
        "                    )\n",
        "                    ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                    keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "                    for col in list(ds.column_names):\n",
        "                        if col not in keep:\n",
        "                            ds = ds.remove_columns(col)\n",
        "                    ds.set_format(type=\"torch\", columns=list(keep))\n",
        "                    bar.progress(100)\n",
        "                    return ds\n",
        "\n",
        "            train_ds_processed, val_ds_processed, test_ds_processed = map(prep, (train_ds, val_ds, test_ds))\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds_processed, val_ds_processed)\n",
        "\n",
        "            with st.spinner(\"Entraînement en cours …\"):\n",
        "                trainer.train()\n",
        "\n",
        "            trainer.save_model(\"outputs/final_model\")\n",
        "            trainer.state.save_to_json(\"outputs/final_model/trainer_state.json\")\n",
        "\n",
        "            # Indexer les données d'entraînement dans le module Q&A optimisé\n",
        "            if st.session_state.raw_train_data:\n",
        "                self.qa_module.fit(st.session_state.raw_train_data)\n",
        "                st.session_state.qa_module = self.qa_module\n",
        "\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "            st.session_state.test_ds = test_ds_processed\n",
        "            st.success(\"🎉 Entraînement terminé !\")\n",
        "            st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"Détail: {traceback.format_exc()}\")\n",
        "            st.session_state.training = False\n",
        "\n",
        "    def run_advanced_qa_interface(self):\n",
        "        \"\"\"Interface Q&A avancée fusionnée avec optimisations\"\"\"\n",
        "        st.header(\"❓ Interface Q&A Avancée\")\n",
        "\n",
        "        # Configuration de recherche avec colonnes optimisées\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            question = st.text_input(\n",
        "                \"Posez votre question sur le climat :\",\n",
        "                placeholder=\"Ex: Quelles sont les causes du réchauffement climatique ?\"\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            search_mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"hybrid\", \"knowledge_only\", \"corpus_only\", \"keywords\"],\n",
        "                format_func=lambda x: {\n",
        "                    \"hybrid\": \"🔀 Hybride\",\n",
        "                    \"knowledge_only\": \"📚 Base de connaissances\",\n",
        "                    \"corpus_only\": \"📊 Corpus d'entraînement\",\n",
        "                    \"keywords\": \"🔍 Mots-clés\"\n",
        "                }[x]\n",
        "            )\n",
        "\n",
        "        # Paramètres avancés dans l'expandeur\n",
        "        with st.expander(\"⚙️ Paramètres de recherche\", expanded=False):\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                top_k = st.slider(\"Nombre de résultats\", 1, 10, value=5)\n",
        "            with col2:\n",
        "                show_details = st.checkbox(\"Afficher les détails\", True)\n",
        "\n",
        "        # Recherche principale avec spinner optimisé\n",
        "        if question:\n",
        "            try:\n",
        "                with st.spinner(\"🔍 Recherche intelligente en cours...\"):\n",
        "                    results = self.qa_module.query_with_fallback(question, top_k, search_mode)\n",
        "\n",
        "                self.display_qa_results(results, show_details, f\"Résultats pour: '{question}'\")\n",
        "\n",
        "                # Analyse des résultats dans un expander\n",
        "                if results and len(results) > 1:\n",
        "                    with st.expander(\"📊 Analyse des résultats\", expanded=False):\n",
        "                        self.visualizer.plot_qa_results_analysis(results, question)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Erreur : {str(e)}\")\n",
        "                if st.checkbox(\"Afficher les détails techniques\"):\n",
        "                    st.exception(e)\n",
        "\n",
        "        # Questions suggérées avec boutons\n",
        "        st.markdown(\"### 💡 Questions suggérées\")\n",
        "        suggested_questions = [\n",
        "            \"Quelles sont les principales causes du réchauffement climatique ?\",\n",
        "            \"Comment les énergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la déforestation sur le climat ?\",\n",
        "            \"Comment réduire les émissions de gaz à effet de serre ?\"\n",
        "        ]\n",
        "\n",
        "        cols = st.columns(2)\n",
        "        for i, suggestion in enumerate(suggested_questions[:4]):\n",
        "            if cols[i % 2].button(\n",
        "                suggestion[:50] + \"...\" if len(suggestion) > 50 else suggestion,\n",
        "                key=f\"suggestion_{i}\",\n",
        "                use_container_width=True\n",
        "            ):\n",
        "                st.session_state[\"question\"] = suggestion\n",
        "                st.rerun()\n",
        "\n",
        "    def display_qa_results(self, results: list, show_details: bool, title: str):\n",
        "        \"\"\"Affichage optimisé des résultats Q&A\"\"\"\n",
        "        if not results:\n",
        "            st.info(\"🔍 Aucun résultat trouvé.\")\n",
        "            return\n",
        "\n",
        "        st.markdown(f\"### {title}\")\n",
        "        st.caption(f\"{len(results)} résultat(s) trouvé(s)\")\n",
        "\n",
        "        for result in results:\n",
        "            # Emoji selon la source\n",
        "            emoji = {\n",
        "                \"knowledge_base\": \"📚\",\n",
        "                \"training_corpus\": \"📊\",\n",
        "                \"keywords\": \"🔍\"\n",
        "            }.get(result.get(\"source\"), \"📄\")\n",
        "\n",
        "            # Couleur selon le score\n",
        "            score = result.get(\"score\", 0)\n",
        "            score_color = \"🟢\" if score > 0.7 else \"🟡\" if score > 0.4 else \"🔴\"\n",
        "\n",
        "            with st.expander(\n",
        "                f\"{emoji} Score: {score_color} {score:.3f} - {result.get('source', 'source').replace('_', ' ').title()}\",\n",
        "                expanded=(result.get(\"rank\", 0) == 1)\n",
        "            ):\n",
        "                st.write(\"**Texte:**\")\n",
        "                st.write(result[\"text\"])\n",
        "\n",
        "                if show_details:\n",
        "                    st.divider()\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        if \"category\" in result:\n",
        "                            st.caption(f\"**Catégorie:** `{result['category']}`\")\n",
        "                        if \"keywords\" in result and result[\"keywords\"]:\n",
        "                            st.caption(\"**Mots-clés:** \" + \" \".join([f\"`{kw}`\" for kw in result[\"keywords\"][:3]]))\n",
        "\n",
        "                    with col2:\n",
        "                        st.caption(f\"**Rang:** {result.get('rank', '?')}\")\n",
        "\n",
        "    def run_knowledge_management(self):\n",
        "        \"\"\"Interface de gestion optimisée de la base de connaissances\"\"\"\n",
        "        st.header(\"📚 Gestion des Connaissances\")\n",
        "\n",
        "        tab1, tab2, tab3 = st.tabs([\"📖 Consulter\", \"➕ Ajouter\", \"📊 Statistiques\"])\n",
        "\n",
        "        with tab1:\n",
        "            self._render_knowledge_browser()\n",
        "\n",
        "        with tab2:\n",
        "            self._render_knowledge_adder()\n",
        "\n",
        "        with tab3:\n",
        "            self._render_knowledge_stats()\n",
        "\n",
        "    def _render_knowledge_browser(self):\n",
        "        \"\"\"Sous-composant pour naviguer dans la base de connaissances\"\"\"\n",
        "        st.subheader(\"📖 Consultation\")\n",
        "\n",
        "        # Filtres avec colonnes\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            categories = self.qa_module.get_categories()\n",
        "            filter_category = st.selectbox(\"Filtrer par\", [\"Toutes\"] + categories)\n",
        "        with col2:\n",
        "            search_text = st.text_input(\"Rechercher\", placeholder=\"Mots-clés...\")\n",
        "\n",
        "        # Filtrage et affichage\n",
        "        knowledge_items = []\n",
        "        for idx, item in enumerate(self.qa_module.knowledge_base):\n",
        "            if filter_category == \"Toutes\" or item[\"category\"] == filter_category:\n",
        "                if not search_text or search_text.lower() in item[\"text\"].lower():\n",
        "                    knowledge_items.append((idx, item))\n",
        "\n",
        "        st.info(f\"📄 {len(knowledge_items)} document(s) trouvé(s)\")\n",
        "\n",
        "        # Affichage paginé pour performance\n",
        "        items_per_page = 5\n",
        "        page = st.number_input(\"Page\", min_value=1, max_value=max(1, len(knowledge_items)//items_per_page + 1), value=1)\n",
        "\n",
        "        start_idx = (page - 1) * items_per_page\n",
        "        end_idx = min(start_idx + items_per_page, len(knowledge_items))\n",
        "\n",
        "        for idx, (orig_idx, item) in enumerate(knowledge_items[start_idx:end_idx], start=1):\n",
        "            with st.expander(f\"📄 {item['category'].upper()} - {item['text'][:80]}...\"):\n",
        "                st.write(item[\"text\"])\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.caption(f\"ID: `{orig_idx}`\")\n",
        "                with col2:\n",
        "                    if item[\"keywords\"]:\n",
        "                        st.caption(\"Keywords: \" + \", \".join(item[\"keywords\"][:3]))\n",
        "\n",
        "    def _render_knowledge_adder(self):\n",
        "        \"\"\"Sous-composant pour ajouter des connaissances\"\"\"\n",
        "        st.subheader(\"➕ Ajouter une connaissance\")\n",
        "\n",
        "        with st.form(\"add_knowledge_form\"):\n",
        "            new_text = st.text_area(\n",
        "                \"Texte\",\n",
        "                placeholder=\"Entrez le texte de la nouvelle connaissance...\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                categories = self.qa_module.get_categories()\n",
        "                category_choice = st.selectbox(\"Catégorie\", [\"Nouvelle\"] + categories)\n",
        "\n",
        "                if category_choice == \"Nouvelle\":\n",
        "                    new_category = st.text_input(\"Nouvelle catégorie\", placeholder=\"technologies\")\n",
        "                    final_category = new_category\n",
        "                else:\n",
        "                    final_category = category_choice\n",
        "\n",
        "            with col2:\n",
        "                keywords = st.text_input(\n",
        "                    \"Mots-clés\",\n",
        "                    placeholder=\"tech, innovation, futur\"\n",
        "                )\n",
        "\n",
        "            submitted = st.form_submit_button(\"✅ Ajouter\", use_container_width=True)\n",
        "\n",
        "            if submitted and new_text and final_category:\n",
        "                keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
        "\n",
        "                if self.qa_module.add_knowledge(new_text, final_category, keyword_list):\n",
        "                    st.success(\"✅ Ajouté avec succès!\")\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"❌ Erreur lors de l'ajout\")\n",
        "\n",
        "    def _render_knowledge_stats(self):\n",
        "        \"\"\"Sous-composant pour les statistiques\"\"\"\n",
        "        st.subheader(\"📊 Statistiques\")\n",
        "        stats = self.qa_module.get_stats()\n",
        "\n",
        "        # Métriques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Total\", stats[\"total_documents\"])\n",
        "        col2.metric(\"KB\", stats[\"knowledge_base_size\"])\n",
        "        col3.metric(\"Corpus\", stats[\"training_corpus_size\"])\n",
        "        col4.metric(\"Catégories\", len(stats[\"categories\"]))\n",
        "\n",
        "        # Graphiques si des données existent\n",
        "        if self.qa_module.knowledge_base:\n",
        "            # Distribution par catégorie\n",
        "            category_counts = {}\n",
        "            for item in self.qa_module.knowledge_base:\n",
        "                cat = item[\"category\"]\n",
        "                category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "            df_categories = pd.DataFrame([\n",
        "                {\"Catégorie\": cat, \"Nombre\": count}\n",
        "                for cat, count in category_counts.items()\n",
        "            ])\n",
        "\n",
        "            st.bar_chart(df_categories.set_index(\"Catégorie\"))\n",
        "\n",
        "    def run_rss_integration(self):\n",
        "        \"\"\"Section RSS dans l'interface Streamlit\"\"\"\n",
        "        st.header(\"📰 Mise à jour RSS Automatique\")\n",
        "\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.info(\"📡 Flux RSS configurés:\")\n",
        "            for url in [\n",
        "                \"Carbon Brief\",\n",
        "                \"NASA Climate\",\n",
        "                \"UNFCCC News\"\n",
        "            ]:\n",
        "                st.write(f\"• {url}\")\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"🔄 Mise à jour manuelle\", type=\"primary\"):\n",
        "                with st.spinner(\"Mise à jour en cours...\"):\n",
        "                    try:\n",
        "                        from update_kb_rss import manual_rss_update\n",
        "                        added = manual_rss_update(self.qa_module)\n",
        "                        if added > 0:\n",
        "                            st.success(f\"✅ {added} articles ajoutés\")\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.info(\"Aucun nouvel article trouvé\")\n",
        "                    except ImportError:\n",
        "                        st.error(\"❌ Module update_kb_rss non trouvé\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"❌ Erreur lors de la mise à jour : {str(e)}\")\n",
        "\n",
        "        # Paramètres avancés\n",
        "        with st.expander(\"⚙️ Paramètres RSS\"):\n",
        "            st.write(\"Mise à jour automatique activée\")\n",
        "            st.write(\"Fréquence: Quotidienne à 09:00\")\n",
        "            st.write(\"Sources: Carbon Brief, NASA Climate, UNFCCC\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface des visualisations (inchangée mais avec optimisations)\"\"\"\n",
        "        st.header(\"📈 Visualisations\")\n",
        "\n",
        "        viz = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"Distribution des classes\", \"Matrice de confusion\", \"Courbes d'entraînement\",\n",
        "             \"📊 Métriques BLEU/ROUGE\", \"🔍 Évaluation Q&A\", \"📚 Analyse Knowledge Base\"]\n",
        "        )\n",
        "\n",
        "        test_ds = st.session_state.get(\"test_ds\")\n",
        "        label_names = st.session_state.get(\"label_names\")\n",
        "\n",
        "        try:\n",
        "            if viz == \"Distribution des classes\" and test_ds:\n",
        "                self.visualizer.plot_class_distribution(test_ds[\"labels\"], label_names)\n",
        "            elif viz == \"Matrice de confusion\" and test_ds and st.session_state.trainer:\n",
        "                self.visualizer.show_confusion_matrix(st.session_state.trainer, test_ds, label_names)\n",
        "            elif viz == \"Courbes d'entraînement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/final_model\")\n",
        "            elif viz == \"📊 Métriques BLEU/ROUGE\":\n",
        "                self.run_bleu_rouge_analysis()\n",
        "            elif viz == \"🔍 Évaluation Q&A\":\n",
        "                self.run_qa_evaluation()\n",
        "            elif viz == \"📚 Analyse Knowledge Base\":\n",
        "                self.run_knowledge_base_analysis()\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur : {e}\")\n",
        "            if st.checkbox(\"Détails techniques\"):\n",
        "                st.exception(e)\n",
        "\n",
        "    def run_knowledge_base_analysis(self):\n",
        "        \"\"\"Analyse optimisée de la base de connaissances\"\"\"\n",
        "        st.subheader(\"📚 Analyse de la Base de Connaissances\")\n",
        "\n",
        "        if not self.qa_module.knowledge_base:\n",
        "            st.info(\"La base de connaissances est vide.\")\n",
        "            return\n",
        "\n",
        "        # Mots-clés fréquents\n",
        "        all_keywords = [kw for item in self.qa_module.knowledge_base for kw in item[\"keywords\"]]\n",
        "\n",
        "        if all_keywords:\n",
        "            from collections import Counter\n",
        "            top_keywords = Counter(all_keywords).most_common(10)\n",
        "            df_keywords = pd.DataFrame(top_keywords, columns=[\"Mot-clé\", \"Fréquence\"])\n",
        "            st.bar_chart(df_keywords.set_index(\"Mot-clé\"))\n",
        "\n",
        "        # Longueurs de texte\n",
        "        lengths = [len(item[\"text\"]) for item in self.qa_module.knowledge_base]\n",
        "\n",
        "        if lengths:\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Min\", f\"{min(lengths)}\")\n",
        "            col2.metric(\"Moyenne\", f\"{np.mean(lengths):.0f}\")\n",
        "            col3.metric(\"Max\", f\"{max(lengths)}\")\n",
        "\n",
        "    def run_bleu_rouge_analysis(self):\n",
        "        \"\"\"Analyse BLEU/ROUGE (conservée avec optimisations)\"\"\"\n",
        "        st.subheader(\"📊 Analyse des métriques BLEU et ROUGE\")\n",
        "\n",
        "        # Code inchangé mais avec gestion d'erreurs améliorée\n",
        "        try:\n",
        "            # Section test personnalisé\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                reference_text = st.text_area(\n",
        "                    \"Texte de référence:\",\n",
        "                    \"Le réchauffement climatique est un phénomène global causé par les activités humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                candidate_text = st.text_area(\n",
        "                    \"Texte candidat:\",\n",
        "                    \"Le changement climatique est un problème mondial dû aux actions humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            if st.button(\"Calculer les scores\", use_container_width=True):\n",
        "                bleu_score = self.visualizer.calculate_bleu_score(reference_text, candidate_text)\n",
        "                rouge_scores = self.visualizer.calculate_rouge_score(reference_text, candidate_text)\n",
        "\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                col1.metric(\"BLEU\", f\"{bleu_score:.4f}\")\n",
        "                col2.metric(\"ROUGE-1\", f\"{rouge_scores['rouge-1']:.4f}\")\n",
        "                col3.metric(\"ROUGE-L\", f\"{rouge_scores['rouge-l']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur dans l'analyse BLEU/ROUGE : {e}\")\n",
        "\n",
        "    def run_qa_evaluation(self):\n",
        "        \"\"\"Évaluation Q&A avec optimisations\"\"\"\n",
        "        st.subheader(\"🔍 Évaluation du système Q&A\")\n",
        "\n",
        "        # Questions de test avec suggestions\n",
        "        default_questions = [\n",
        "            \"Quelles sont les principales causes du réchauffement climatique ?\",\n",
        "            \"Comment les énergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la déforestation sur le climat ?\"\n",
        "        ]\n",
        "\n",
        "        if st.checkbox(\"Utiliser questions par défaut\", value=True):\n",
        "            test_questions = default_questions\n",
        "        else:\n",
        "            test_questions = st.text_area(\n",
        "                \"Entrez vos questions (une par ligne):\",\n",
        "                height=100\n",
        "            ).split(\"\\n\")\n",
        "\n",
        "        if test_questions and st.button(\"🚀 Lancer l'évaluation\", type=\"primary\"):\n",
        "            with st.spinner(\"Évaluation en cours...\"):\n",
        "                # Génération des références\n",
        "                reference_answers = []\n",
        "                for question in test_questions:\n",
        "                    kb_results = self.qa_module.query_knowledge_base(question, top_k=1)\n",
        "                    ref = kb_results[0]['text'] if kb_results else \"Référence manquante\"\n",
        "                    reference_answers.append(ref)\n",
        "\n",
        "                self.visualizer.evaluate_qa_performance(\n",
        "                    self.qa_module,\n",
        "                    test_questions,\n",
        "                    reference_answers\n",
        "                )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = ClimateAnalyzerApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f65f66-0c69-4d76-a0a6-428b1ee74243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface utilisateur Streamlit – `streamlit_app.py`\n",
        "\n",
        "Cette application constitue l’interface centrale du projet *Climate Sentiment Analyzer Pro*, permettant d’interagir avec le moteur IA via une interface web optimisée et modulaire, en exploitant l’ensemble des composants développés (prétraitement, entraînement, Q&A, connaissances, RSS, visualisations).\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L’objectif est de **rendre accessible** à tous les utilisateurs – même non techniques – un système avancé de **classification et d’analyse sémantique** des discours sur le climat, via une interface unifiée et interactive.\n",
        "\n",
        "La démarche repose sur une **architecture modulaire** :\n",
        "- Pipeline complet d'entraînement de modèle NLP LoRA,\n",
        "- Interface Q&A sémantique avec moteur hybride,\n",
        "- Système de gestion de la base de connaissances,\n",
        "- Mises à jour RSS en temps réel,\n",
        "- Tableau de bord de visualisation et d’évaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Concevoir une interface fluide malgré la complexité algorithmique,\n",
        "- Intégrer des modules hétérogènes (HuggingFace, FAISS, Langchain, RSS, visualisations) dans une seule app,\n",
        "- Assurer des **temps de réponse rapides** même sur des corpus volumineux,\n",
        "- Offrir à l’utilisateur une **expérience guidée**, pédagogique et personnalisable,\n",
        "- Permettre l’enrichissement continu du système sans re-déploiement.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément | Justification |\n",
        "|---------|----------------|\n",
        "| `Streamlit` | Rapidité de prototypage, accessibilité, interaction instantanée |\n",
        "| Structure multi-onglets | Organisation claire pour chaque fonctionnalité (pipeline, Q&A, RSS…) |\n",
        "| `session_state` | Persistance des objets lourds (modèle, tokenizer, corpus) entre les vues |\n",
        "| Cache Streamlit + FAISS | Optimisation mémoire et performance des recherches |\n",
        "| Intégration Q&A + BLEU/ROUGE | Mesure de la qualité et compréhension du système en temps réel |\n",
        "| Mises à jour RSS intégrées | Ouverture vers des flux externes en production |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "L’application permet :\n",
        "- D’entraîner un modèle de classification de sentiment à partir de **données personnalisées**,\n",
        "- D’interroger en langage naturel la **base de connaissances enrichie** par les flux d’actualité et l’entraînement supervisé,\n",
        "- De visualiser la qualité des résultats, les performances du modèle, la distribution des classes et des catégories sémantiques.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module donne **une dimension opérationnelle et interactive** au projet.  \n",
        "Il transforme les algorithmes développés en **outil métier activable**, facilitant l’expérimentation, l’interprétation des résultats et l’itération continue.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- **Accessibilité no-code** pour les analystes, journalistes ou décideurs,\n",
        "- Personnalisation et déploiement rapide d’un outil d’analyse de discours climatique,\n",
        "- Surveillance **en temps réel** des tendances via l’analyse de corpus ou l'intégration RSS,\n",
        "- Renforcement de la **transparence et traçabilité** des résultats grâce aux visualisations et métriques intégrées,\n",
        "- Base idéale pour un **outil SaaS éducatif ou décisionnel** autour des enjeux climatiques et des controverses sémantiques.\n"
      ],
      "metadata": {
        "id": "4FShVUIxqfe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. **Script d'Installation - setup_pipeline.py**"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rouge-score\",\n",
        "        \"nltk\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "            print(f\"✅ {package} installé\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Erreur avec {package}: {e}\")\n",
        "\n",
        "    # Téléchargement des ressources NLTK\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"✅ Ressources NLTK prêtes\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea889f0-d816-468b-ea64-2d9402d3f33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script de Setup — `setup_pipeline.py`\n",
        "\n",
        "Ce module automatise l'installation des **dépendances logicielles critiques** pour exécuter le projet *Climate Sentiment Analyzer Pro* dans un environnement propre et reproductible.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et démarche\n",
        "\n",
        "L'objectif est de garantir à l'utilisateur ou au développeur une configuration complète, stable et immédiatement exploitable pour faire fonctionner tous les modules du pipeline : NLP, IA, visualisation, interface Streamlit, etc.\n",
        "\n",
        "La démarche consiste à :\n",
        "- Lister toutes les bibliothèques nécessaires avec leurs versions recommandées,\n",
        "- Automatiser leur installation via `pip`,\n",
        "- Télécharger les ressources NLTK indispensables à la génération et l'évaluation de texte.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- **Compatibilité croisée** entre bibliothèques NLP modernes (transformers, PEFT, sentence-transformers, etc.),\n",
        "- **Stabilité des versions** pour éviter les conflits lors de l'entraînement ou du déploiement,\n",
        "- **Simplicité de prise en main** même pour des utilisateurs non techniques (ex. : étudiants, journalistes, etc.),\n",
        "- Préparer le terrain pour un **environnement exécutable à distance (Cloud, Colab, Docker)**.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Élément technique         | Raison du choix |\n",
        "|--------------------------|-----------------|\n",
        "| `subprocess` + `pip`     | Compatible avec tous les OS, installation en ligne de commande Python |\n",
        "| Versions ≥ spécifiques   | Sécurité et compatibilité avec les API utilisées dans le projet |\n",
        "| `nltk.download('punkt')` | Nécessaire pour la tokenisation et les scores ROUGE/BLEU |\n",
        "| `faiss-cpu`              | Permet la recherche vectorielle rapide même sans GPU |\n",
        "| `streamlit`              | L’interface utilisateur repose intégralement dessus |\n",
        "\n",
        "---\n",
        "\n",
        "### Interprétation du résultat\n",
        "\n",
        "Une fois le script exécuté :\n",
        "- Toutes les bibliothèques requises sont installées,\n",
        "- L’environnement Python est prêt à exécuter les notebooks, l’interface Streamlit, l’entraînement et les visualisations,\n",
        "- Plus besoin d’installation manuelle : gain de temps et fiabilité du démarrage.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce script agit comme une **porte d’entrée technique unique**, qui garantit un démarrage sans erreur, reproductible, quel que soit l’environnement. Il est essentiel à la diffusion, la collaboration ou le déploiement du projet.\n",
        "\n",
        "---\n",
        "\n",
        "### Bénéfices métier\n",
        "\n",
        "- **Expérience simplifiée** pour les utilisateurs finaux ou partenaires projet,\n",
        "- **Réduction des erreurs de configuration** en environnement collaboratif (hackathon, cloud, notebook partagé),\n",
        "- **Gain de temps** pour la mise en production ou la démonstration,\n",
        "- Base compatible pour créer un **setup.sh, requirements.txt, Dockerfile ou environnement cloud** clé-en-main.\n"
      ],
      "metadata": {
        "id": "zL5ZKNrOq1nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7ba942-cd7d-43ab-8a21-69c9d931fc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "✅ transformers>=4.36.0 installé\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "✅ datasets>=2.16.0 installé\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "✅ torch>=2.1.0 installé\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "✅ peft>=0.7.0 installé\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "✅ sentence-transformers>=2.2.0 installé\n",
            "Collecting faiss-cpu>=1.7.0\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n",
            "✅ faiss-cpu>=1.7.0 installé\n",
            "Collecting streamlit>=1.29.0\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit>=1.29.0)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.29.0)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n",
            "✅ streamlit>=1.29.0 installé\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "✅ plotly>=5.17.0 installé\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "✅ scikit-learn>=1.3.0 installé\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "✅ matplotlib>=3.7.0 installé\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "✅ seaborn>=0.12.0 installé\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "✅ pandas>=1.5.0 installé\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "✅ numpy>=1.24.0 installé\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=258fe989022c508ed2615c65ec1c03f553b9d1ca31057bf5b3f080efdb039833\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "✅ rouge-score installé\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "✅ nltk installé\n",
            "✅ Ressources NLTK prêtes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-MEnnZWsOuL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db23883f-a4c1-44cb-8f7c-f7863204d98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Y3wskx_fZLEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f71fe0a-b8ad-4b06-9a69-e3f11c4a8e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3WFMzkb6p9",
        "outputId": "a0b85b0e-5b7e-4707-aee7-8291e82d459b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. **Déploiement local d’une application Streamlit via ngrok**"
      ],
      "metadata": {
        "id": "-DI_t3NCrt-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Lancement Streamlit + ngrok (version corrigée)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1️⃣ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2️⃣ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3️⃣ Attendre et créer le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🚀 Interface Streamlit disponible à :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IFyKJzjISjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de20c919-e1a3-484f-f5d6-3382c82e3990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "🚀 Interface Streamlit disponible à :\n",
            "NgrokTunnel: \"https://60ad9aba5de0.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Déploiement local d’une application Streamlit via ngrok\n",
        "\n",
        "#### Intention et démarche\n",
        "Ce script permet de **lancer une application Streamlit localement** et de la rendre accessible à distance grâce à un **tunnel sécurisé ngrok**. C’est une solution idéale pour tester, démontrer ou partager un prototype d’application web (comme ClimateAnalyzerPro) **sans configuration serveur complexe**.\n",
        "\n",
        "#### Enjeux et challenges\n",
        "- Streamlit, par défaut, n’est accessible que localement (`localhost:8501`).\n",
        "- En environnement cloud ou notebook distant (ex: Google Colab), il n’est **pas possible d’exposer directement un port local**.\n",
        "- Il est donc nécessaire de créer un **pont réseau externe** via un tunnel sécurisé (ici : **ngrok**).\n",
        "- Le challenge est de s’assurer que l’interface est fonctionnelle, stable, et bien mappée sur l’URL générée.\n",
        "\n",
        "#### Justification des choix techniques\n",
        "- **`pyngrok`** permet de créer automatiquement un tunnel HTTP sur le port 8501 utilisé par Streamlit.\n",
        "- L’application Streamlit est lancée en arrière-plan (`subprocess.Popen`) avec redirection des logs pour plus de clarté.\n",
        "- Un **temps d’attente** est inséré (`time.sleep(5)`) pour garantir que l’interface soit bien en ligne avant de créer le lien public.\n",
        "\n",
        "#### Interprétation du résultat\n",
        "Une fois le script exécuté :\n",
        "- Ngrok authentifie l’utilisateur via son token personnel.\n",
        "- L’application Streamlit est lancée à l’adresse locale `0.0.0.0:8501`.\n",
        "- Le lien **public et accessible depuis n’importe quel navigateur** est affiché, par exemple : `https://xxxx.ngrok.io`.\n",
        "\n",
        "#### Conclusion\n",
        "Ce mécanisme permet à toute personne, même hors réseau local, de visualiser l’application sans installation préalable. C’est un **gain de temps majeur pour les démonstrations, les retours utilisateurs, ou les tests multi-device**.\n",
        "\n",
        "#### Bénéfices métier\n",
        "- Permet de partager rapidement un prototype de dashboard IA sans infrastructure cloud.\n",
        "- Facilite les **revues d’équipe, tests clients ou présentations à distance**.\n",
        "- Idéal pour un usage en **hackathon, projet étudiant ou preuve de concept en entreprise**.\n",
        "\n"
      ],
      "metadata": {
        "id": "wqr5FcMdrjjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion globale du projet**\n",
        "Ce projet démontre avec succès l’intégration complète d’un pipeline IA de classification de sentiments sur les discussions climatiques, enrichi par un moteur Q&A sémantique et une base de connaissances dynamique, régulièrement mise à jour via des flux RSS spécialisés.\n",
        "\n",
        "Nous avons conçu une application Streamlit robuste, modulaire et pédagogique, capable d’analyser, classifier, interroger et enrichir la connaissance climatique à partir de données ouvertes et d’entrées utilisateurs.\n",
        "\n",
        "---\n",
        "\n",
        "**Bénéfices métier**\n",
        "Décision stratégique informée : en comprenant le ressenti public sur des sujets climatiques, les ONG, décideurs et communicants peuvent ajuster leurs messages et priorités.\n",
        "\n",
        "Outil pédagogique interactif : idéal pour les enseignants, journalistes ou citoyens souhaitant explorer les grands enjeux climatiques de manière interactive.\n",
        "\n",
        "Veille automatisée : grâce à l'intégration RSS, l'application reste continuellement à jour sans effort humain.\n",
        "\n",
        "Base de connaissances enrichissable : les experts peuvent compléter ou corriger la base à la volée, sans redéployer le système.\n",
        "\n",
        "---\n",
        "\n",
        "**Et maintenant…**\n",
        "Et si ce moteur pouvait demain s’adapter à d’autres causes sociétales — santé, éducation, inclusion — pour cartographier en temps réel les émotions, les craintes et les espoirs de nos sociétés ?"
      ],
      "metadata": {
        "id": "sM5-OJPMsVhs"
      }
    }
  ]
}