{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **HACKATHON 2 : LLM fine-tun√© pour l‚ÄôAnalyse de Sentiment autour du climat et les R√©ponses Contextuelles**"
      ],
      "metadata": {
        "id": "gCaW_SHQlaLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans un monde satur√© de messages sur le climat, les opinions s‚Äôexpriment partout : r√©seaux sociaux, forums, blogs, commentaires‚Ä¶\n",
        "Mais comment savoir ce que les gens ressentent vraiment, ce qu‚Äôils demandent, et comment r√©agir intelligemment ?\n",
        "\n",
        "Ce projet propose un pipeline IA complet pour comprendre en profondeur les discours climatiques en ligne, gr√¢ce √† :\n",
        "\n",
        "* L‚Äôanalyse de sentiment fine (positif, neutre, n√©gatif)\n",
        "\n",
        "* La d√©tection d‚Äôintention (plainte, question, engagement‚Ä¶)\n",
        "\n",
        "Et la g√©n√©ration de r√©ponses adapt√©es au ton et au contexte.\n",
        "\n",
        "Objectif m√©tier : permettre aux acteurs de la communication, de la RSE ou de la veille soci√©tale d‚Äôextraire du sens et d‚Äôagir rapidement face aux signaux faibles.\n",
        "\n"
      ],
      "metadata": {
        "id": "BJOefjiulBEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. **Module Core - core_modules.py**"
      ],
      "metadata": {
        "id": "t4ZVvmadrQ96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py - Configuration optimis√©e\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e et optimis√©e\"\"\"\n",
        "    model_name: str = \"distilbert-base-uncased\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 2e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    output_dir: str = \"outputs/final_model\"\n",
        "\n",
        "    # Configuration Q&A\n",
        "    qa_model: str = \"all-MiniLM-L6-v2\"\n",
        "    similarity_threshold: float = 0.3\n",
        "    max_results: int = 5"
      ],
      "metadata": {
        "id": "1l6Yz_5prPWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d66356-abbd-4742-8ff0-5eb3c7157f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing core_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Centrale ‚Äì `core_modules.py`\n",
        "\n",
        "Ce module d√©finit une classe `ClimateConfig` qui regroupe tous les param√®tres n√©cessaires au fonctionnement du pipeline IA. Gr√¢ce √† l'utilisation de `@dataclass`, cette configuration est propre, facilement modifiable, et rend le code plus modulaire.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et D√©marche\n",
        "- Centraliser tous les param√®tres dans un seul fichier pour √©viter leur dispersion dans le code\n",
        "- Faciliter les ajustements lors des tests (mod√®les, batch size, seuils, etc.)\n",
        "- Garantir la reproductibilit√© et la coh√©rence des r√©sultats\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et Challenges\n",
        "- Organiser des param√®tres tr√®s vari√©s (mod√®le, LoRA, Q&A)\n",
        "- Assurer la flexibilit√© pour diff√©rents cas d‚Äôusage\n",
        "- Permettre un fine-tuning l√©ger et efficace\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des Choix Techniques\n",
        "\n",
        "| Param√®tre | R√¥le | Justification |\n",
        "|----------|------|----------------|\n",
        "| `model_name` | Mod√®le de base | `distilbert-base-uncased` : l√©ger et efficace |\n",
        "| `max_length` | Longueur max. des s√©quences | 128 tokens suffisent pour tweets/commentaires |\n",
        "| `batch_size` | Taille des lots | 16 = √©quilibre m√©moire/performance |\n",
        "| `epochs` | Nb d‚Äôit√©rations d‚Äôentra√Ænement | 3 = limite le surapprentissage |\n",
        "| `learning_rate` | Taux d‚Äôapprentissage | 2e-5 = valeur courante pour transformers |\n",
        "| `lora_r` & `lora_alpha` | Param√®tres PEFT | Optimisation fine avec LoRA |\n",
        "| `qa_model` | Mod√®le de similarit√© s√©mantique | `all-MiniLM-L6-v2` : rapide et robuste |\n",
        "| `similarity_threshold` | Seuil de Q&A | 0.3 = bon √©quilibre entre pertinence et bruit |\n",
        "| `max_results` | Nb max de r√©ponses sugg√©r√©es | 5 = clair pour l'utilisateur |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation\n",
        "Cette configuration structure l'ensemble du pipeline : classification de sentiments, d√©tection d‚Äôintentions, g√©n√©ration de r√©ponse et Q&A. Elle garantit coh√©rence, reproductibilit√© et efficacit√©.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices M√©tier\n",
        "- Param√©trage centralis√©, facile √† ajuster\n",
        "- Gain de temps en exp√©rimentation\n",
        "- R√©utilisable pour d‚Äôautres contextes (RH, politique, produits‚Ä¶)\n",
        "- Compatible avec Streamlit pour un d√©ploiement rapide\n"
      ],
      "metadata": {
        "id": "ESY88Eo3m2dN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. **Module Data Processing - data_modules.py**"
      ],
      "metadata": {
        "id": "UQxAuDYSrbsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import Tuple\n",
        "\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "        self.reverse_label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        text_keywords = ['text', 'content', 'message', 'comment', 'body', 'description', 'self_text']\n",
        "        label_keywords = ['label', 'sentiment', 'category', 'class', 'target', 'comment_sentiment']\n",
        "        text_col = next((c for c in df.columns if any(k in str(c).lower() for k in text_keywords)), None)\n",
        "        label_col = next((c for c in df.columns if any(k in str(c).lower() for k in label_keywords)), None)\n",
        "        if not text_col:\n",
        "            text_col = df.select_dtypes(include=['object']).columns[0]\n",
        "        if not label_col:\n",
        "            label_col = df.columns[-1]\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        if pd.isna(text) or str(text).strip().lower() in ['nan', 'none', '', 'null']:\n",
        "            return None\n",
        "        text = str(text).strip()\n",
        "        text = re.sub(r'&gt;|&lt;|&amp;', lambda m: {'&gt;': '>', '&lt;': '<', '&amp;': '&'}[m.group()], text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip() if text.strip() else None\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "        df_clean = df[[self.text_col, self.label_col]].copy()\n",
        "        df_clean.columns = ['text', 'label']\n",
        "        df_clean['text'] = df_clean['text'].apply(self.clean_text)\n",
        "        df_clean['label'] = df_clean['label'].astype(str)\n",
        "        df_clean = df_clean.dropna().reset_index(drop=True)\n",
        "        df_clean = df_clean[df_clean['text'].str.len() >= 10]\n",
        "\n",
        "        if len(df_clean) > sample_size:\n",
        "            df_clean = df_clean.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        unique_labels = sorted(df_clean['label'].unique())\n",
        "        self.label_mapping = {str(l): i for i, l in enumerate(unique_labels)}\n",
        "        df_clean['label_id'] = df_clean['label'].map(self.label_mapping)\n",
        "\n",
        "        # Nettoyage final NaN\n",
        "        df_clean = df_clean.dropna(subset=['label_id'])\n",
        "        df_clean['label_id'] = df_clean['label_id'].astype(int)\n",
        "\n",
        "        if df_clean.empty:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage.\")\n",
        "\n",
        "        train_df, temp = train_test_split(df_clean, test_size=0.4, stratify=df_clean['label_id'], random_state=42)\n",
        "        val_df, test_df = train_test_split(temp, test_size=0.5, stratify=temp['label_id'], random_state=42)\n",
        "\n",
        "        return (\n",
        "            Dataset.from_pandas(train_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(val_df[['text', 'label_id']]),\n",
        "            Dataset.from_pandas(test_df[['text', 'label_id']])\n",
        "        )"
      ],
      "metadata": {
        "id": "PtjC1wkcrbil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ca7507-c2bc-43a3-b0bf-6b4078afd0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Pr√©paration des Donn√©es ‚Äì `data_modules.py`\n",
        "\n",
        "Ce module d√©finit la classe `DataProcessor`, responsable de la **d√©tection automatique des colonnes**, du **nettoyage du texte**, et de la **pr√©paration des datasets** pour l'entra√Ænement, la validation et le test.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et D√©marche\n",
        "- Faciliter l'int√©gration de **n'importe quel fichier de donn√©es textuelles** sans devoir sp√©cifier manuellement les colonnes.\n",
        "- Nettoyer le texte pour enlever les parasites (liens, caract√®res HTML, espaces multiples).\n",
        "- Cr√©er des splits √©quilibr√©s pour un entra√Ænement robuste (train / val / test).\n",
        "- Mapper automatiquement les labels en identifiants num√©riques.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et Challenges\n",
        "- G√©rer des fichiers CSV aux **colonnes nomm√©es diff√©remment** selon les sources (`message`, `body`, `comment`, etc.).\n",
        "- Nettoyer les textes tout en conservant leur **contenu s√©mantique pertinent**.\n",
        "- √âviter les **donn√©es bruit√©es ou vides** qui peuvent fausser l‚Äôapprentissage.\n",
        "- Pr√©server un **√©quilibre des classes** pour garantir la qualit√© du mod√®le.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des Choix Techniques\n",
        "\n",
        "| Fonction | R√¥le | Pourquoi ce choix |\n",
        "|----------|------|-------------------|\n",
        "| `detect_columns()` | D√©tection automatique texte/label | Rend le code adaptable √† n‚Äôimporte quelle source |\n",
        "| `clean_text()` | Nettoyage cibl√© | Supprime les liens, HTML, espaces, et filtres vides |\n",
        "| `sample_size=8000` | Limite d‚Äô√©chantillon | Contr√¥le le co√ªt de calcul pour les tests |\n",
        "| `train_test_split()` | Stratification par label | Assure un √©quilibre de classes dans chaque split |\n",
        "| `Dataset.from_pandas()` | Conversion au bon format | Compatible avec les Transformers de Hugging Face |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du R√©sultat\n",
        "\n",
        "√Ä partir d‚Äôun fichier brut, on obtient :\n",
        "- Trois sous-ensembles (`train`, `val`, `test`) propres et √©quilibr√©s\n",
        "- Des textes nettoy√©s, pr√™ts √† √™tre tokenis√©s\n",
        "- Des labels convertis en entiers (`label_id`) pour la classification\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module permet de transformer automatiquement n'importe quel fichier brut en **donn√©es exploitables par un mod√®le NLP**, avec une qualit√© de pr√©paration conforme aux standards du Machine Learning.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices M√©tier\n",
        "- Gain de temps : aucune configuration manuelle n√©cessaire\n",
        "- Robustesse : nettoyage automatique + gestion des cas extr√™mes (donn√©es vides, d√©s√©quilibre)\n",
        "- R√©utilisabilit√© : fonctionne sur diff√©rents jeux de donn√©es avec tr√®s peu d‚Äôadaptations\n",
        "- S√©curit√© : contr√¥le int√©gr√© des donn√©es valides, avec gestion des erreurs\n"
      ],
      "metadata": {
        "id": "KTigalatnifr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. **Module Mod√®le - model_modules.py**"
      ],
      "metadata": {
        "id": "GUNsO18Xrh5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_modules.py\n",
        "import os\n",
        "import logging\n",
        "import warnings\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        return self.tokenizer\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            num_labels=num_labels,\n",
        "            torch_dtype=torch.float32,\n",
        "            problem_type=\"single_label_classification\"\n",
        "        )\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=self.config.lora_r,\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_lin\", \"v_lin\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "        self.peft_model = get_peft_model(base_model, lora_config)\n",
        "        return self.peft_model\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        preds = np.argmax(predictions, axis=1)\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels, preds),\n",
        "            \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "            \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "        }\n",
        "\n",
        "    def setup_training_args(self):\n",
        "        os.makedirs(self.config.output_dir, exist_ok=True)\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            save_steps=500,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_full_eval=False,\n",
        "            bf16_full_eval=False,\n",
        "            save_total_limit=2,\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        return Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=self.setup_training_args(),\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),\n",
        "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        )"
      ],
      "metadata": {
        "id": "PCUmVHjPrhv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfebadb-018c-41b8-d989-1a65a9789030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Mod√©lisation ‚Äì `model_modules.py`\n",
        "\n",
        "Ce module contient la classe `ModelManager`, charg√©e de configurer, entra√Æner et √©valuer un mod√®le de classification de texte √† l‚Äôaide de Transformers et de la m√©thode de fine-tuning l√©g√®re LoRA.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est de proposer un cadre unifi√© et automatis√© pour :\n",
        "- Initialiser un tokenizer compatible avec le mod√®le choisi,\n",
        "- Charger un mod√®le pr√©entra√Æn√© pour la classification de s√©quences,\n",
        "- Appliquer un fine-tuning efficace avec **LoRA (Low-Rank Adaptation)**,\n",
        "- D√©finir une strat√©gie d'entra√Ænement robuste et √©valuable,\n",
        "- Suivre les performances sur des m√©triques cl√©s via un `Trainer`.\n",
        "\n",
        "Ce module structure toutes les √©tapes critiques de mod√©lisation de mani√®re modulaire et r√©utilisable.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Adapter un grand mod√®le NLP aux contraintes d‚Äôentra√Ænement locales (temps, m√©moire GPU),\n",
        "- Permettre un fine-tuning **l√©ger mais performant**, √©vitant de r√©entra√Æner tous les poids,\n",
        "- Assurer une bonne gestion du padding, du truncation, et des tokens sp√©ciaux pour garantir la stabilit√©,\n",
        "- Obtenir un mod√®le **g√©n√©ralisable** malgr√© un jeu de donn√©es souvent limit√©,\n",
        "- Surveiller les performances de fa√ßon continue avec un `Trainer` configur√© intelligemment.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment | Justification |\n",
        "|--------|---------------|\n",
        "| `AutoTokenizer` et `AutoModelForSequenceClassification` | Compatibilit√© large avec les mod√®les Transformers |\n",
        "| `LoRA (PEFT)` | R√©duction significative du co√ªt d'entra√Ænement tout en conservant de bonnes performances |\n",
        "| `Trainer` de Hugging Face | Simplifie le processus d'entra√Ænement, de validation et de sauvegarde |\n",
        "| `EarlyStoppingCallback` | √âvite le surapprentissage en arr√™tant t√¥t l'entra√Ænement si les performances stagnent |\n",
        "| `compute_metrics()` | Utilise des m√©triques standard (accuracy, f1, precision, recall) pour une √©valuation √©quilibr√©e |\n",
        "| `DataCollatorWithPadding` | G√®re automatiquement le padding dynamique pour les batchs |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "Une fois ex√©cut√©, ce module produit :\n",
        "- Un mod√®le entra√Æn√©, optimis√© avec LoRA, pr√™t pour l‚Äôinf√©rence,\n",
        "- Des scores de performance clairs sur le jeu de validation (f1, pr√©cision, recall),\n",
        "- Un mod√®le sauvegard√© dans un r√©pertoire d√©di√©, r√©utilisable pour d‚Äôautres pr√©dictions.\n",
        "\n",
        "Les m√©triques permettent de comparer diff√©rents mod√®les ou configurations de mani√®re objective.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module encapsule l‚Äôensemble de la logique de mod√©lisation dans une structure coh√©rente, reproductible et modulaire. Il permet un entra√Ænement efficace m√™me avec des ressources limit√©es, tout en maintenant des standards √©lev√©s de performance.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- R√©duction des co√ªts d'entra√Ænement gr√¢ce √† LoRA,\n",
        "- Mod√®le r√©entra√Ænable facilement sur d'autres th√©matiques ou jeux de donn√©es,\n",
        "- Code structur√© facilitant la maintenance, la reproductibilit√© et l‚Äôadaptation √† d‚Äôautres projets,\n",
        "- R√©sultats exploitables rapidement pour des applications concr√®tes : analyse d‚Äôopinion, mod√©ration, veille.\n"
      ],
      "metadata": {
        "id": "P1vYclLAoL1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. **visualization_modules.py**"
      ],
      "metadata": {
        "id": "qYJhFun3Mws5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- NLTK / BLEU / ROUGE ---\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# T√©l√©chargement silencieux des ressources NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "plt.style.use('default')\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestionnaire de visualisations pour Climate Analyzer.\"\"\"\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Outils internes BLEU / ROUGE\n",
        "    # --------------------------------------------------\n",
        "    _smoothie = SmoothingFunction().method4\n",
        "    _rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def _bleu(ref: str, hyp: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        ref_tok = nltk.word_tokenize(ref.lower())\n",
        "        hyp_tok = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu([ref_tok], hyp_tok, smoothing_function=VisualizationManager._smoothie)\n",
        "\n",
        "    @staticmethod\n",
        "    def _rouge_score(ref: str, hyp: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        scores = VisualizationManager._rouge_scorer.score(ref.lower(), hyp.lower())\n",
        "        return {'rouge-1': scores['rouge1'].fmeasure,\n",
        "                'rouge-l': scores['rougeL'].fmeasure}\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 1) Courbes d‚Äôentra√Ænement\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str = \"outputs/final_model\"):\n",
        "        try:\n",
        "            log_file = os.path.join(log_dir, \"trainer_state.json\")\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"üìÑ Aucun log d'entra√Ænement trouv√©.\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"üìâ Aucune donn√©e d'historique trouv√©e.\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_acc, eval_f1 = [], [], [], [], []\n",
        "\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_acc.append(entry.get('eval_accuracy', 0))\n",
        "                    eval_f1.append(entry.get('eval_f1_weighted', 0))\n",
        "                elif 'train_loss' in entry:\n",
        "                    train_loss.append(entry.get('train_loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(\"üìà √âvolution de l'entra√Ænement\", fontsize=16)\n",
        "\n",
        "            if train_loss and eval_loss:\n",
        "                train_steps = np.linspace(0, max(epochs) if epochs else 1, len(train_loss))\n",
        "                axes[0, 0].plot(train_steps, train_loss, 'b-', label='Train Loss', alpha=0.7)\n",
        "                axes[0, 0].plot(epochs[:len(eval_loss)], eval_loss, 'r-o', label='Eval Loss', markersize=4)\n",
        "                axes[0, 0].set_title('Loss Evolution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc:\n",
        "                axes[0, 1].plot(epochs[:len(eval_acc)], eval_acc, 'g-o', label='Accuracy', markersize=4)\n",
        "                axes[0, 1].set_title('Accuracy Evolution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_f1:\n",
        "                axes[1, 0].plot(epochs[:len(eval_f1)], eval_f1, 'm-o', label='F1-Weighted', markersize=4)\n",
        "                axes[1, 0].set_title('F1-Score Evolution')\n",
        "                axes[1, 0].legend()\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "            if eval_acc and eval_f1:\n",
        "                final_metrics = ['Accuracy', 'F1-Score']\n",
        "                final_values = [eval_acc[-1], eval_f1[-1]]\n",
        "                bars = axes[1, 1].bar(final_metrics, final_values, color=['green', 'purple'], alpha=0.7)\n",
        "                axes[1, 1].set_title('Final Metrics')\n",
        "                axes[1, 1].set_ylim(0, 1)\n",
        "                for bar, value in zip(bars, final_values):\n",
        "                    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                                   f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 2) Matrice de confusion\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names: List[str]):\n",
        "        try:\n",
        "            predictions_output = trainer.predict(test_dataset)\n",
        "            predictions = predictions_output.predictions.argmax(axis=1)\n",
        "            true_labels = predictions_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax1)\n",
        "            ax1.set_title(\"Matrice de confusion\")\n",
        "            ax1.set_xlabel(\"Pr√©dictions\")\n",
        "            ax1.set_ylabel(\"Vraies valeurs\")\n",
        "\n",
        "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                       xticklabels=label_names, yticklabels=label_names, ax=ax2)\n",
        "            ax2.set_title(\"Matrice de confusion (normalis√©e)\")\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            report = classification_report(true_labels, predictions,\n",
        "                                         target_names=label_names,\n",
        "                                         output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.subheader(\"üìä Rapport de classification\")\n",
        "            st.dataframe(report_df.round(3))\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage de la matrice de confusion : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 3) Distribution des classes\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names: List[str] = None, title: str = \"Distribution des classes\"):\n",
        "        try:\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist()\n",
        "            labels = [int(x) for x in labels]\n",
        "            label_counts = Counter(labels)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            if label_names:\n",
        "                x_labels = [label_names[i] if i < len(label_names) else f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "            else:\n",
        "                x_labels = [f\"Classe {i}\" for i in sorted(label_counts.keys())]\n",
        "                counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
        "\n",
        "            bars = ax.bar(range(len(x_labels)), counts, color=plt.cm.Set3(np.linspace(0, 1, len(x_labels))))\n",
        "            ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "            ax.set_xlabel(\"Classes\")\n",
        "            ax.set_ylabel(\"Nombre d'√©chantillons\")\n",
        "            ax.set_xticks(range(len(x_labels)))\n",
        "            ax.set_xticklabels(x_labels, rotation=45 if max(map(len, x_labels)) > 10 else 0)\n",
        "\n",
        "            for bar, count in zip(bars, counts):\n",
        "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,\n",
        "                       str(count), ha='center', va='bottom')\n",
        "            total = sum(counts)\n",
        "            ax.text(0.02, 0.98, f\"Total: {total}\\nClasses: {len(x_labels)}\",\n",
        "                   transform=ax.transAxes, va='top', ha='left',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.7))\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage de la distribution : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 4) Analyse des r√©sultats Q&A\n",
        "    # --------------------------------------------------\n",
        "    @staticmethod\n",
        "    def plot_qa_results_analysis(qa_results: List[Dict], question: str):\n",
        "        if not qa_results:\n",
        "            st.info(\"Aucun r√©sultat √† analyser\")\n",
        "            return\n",
        "        try:\n",
        "            scores = [r['score'] for r in qa_results]\n",
        "            ranks = [r['rank'] for r in qa_results]\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            fig.suptitle(f\"Analyse des r√©sultats pour: '{question[:50]}...'\", fontsize=14)\n",
        "\n",
        "            axes[0, 0].hist(scores, bins=min(10, len(scores)), alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[0, 0].set_title(\"Distribution des scores de similarit√©\")\n",
        "            axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', label=f'Moyenne: {np.mean(scores):.3f}')\n",
        "            axes[0, 0].legend()\n",
        "\n",
        "            axes[0, 1].bar(ranks, scores, color='lightcoral', alpha=0.7)\n",
        "            axes[0, 1].set_title(\"Scores par rang\")\n",
        "            axes[0, 1].set_xlabel(\"Rang\")\n",
        "\n",
        "            text_lengths = [len(r['text']) for r in qa_results]\n",
        "            axes[1, 0].scatter(text_lengths, scores, alpha=0.6, color='green')\n",
        "            axes[1, 0].set_title(\"Score vs Longueur du texte\")\n",
        "            axes[1, 0].set_xlabel(\"Longueur du texte\")\n",
        "\n",
        "            top_scores = scores[:min(5, len(scores))]\n",
        "            top_ranks = ranks[:min(5, len(ranks))]\n",
        "            axes[1, 1].barh(range(len(top_scores)), top_scores, color='purple', alpha=0.7)\n",
        "            axes[1, 1].set_title(\"Top 5 des scores\")\n",
        "            axes[1, 1].set_yticks(range(len(top_scores)))\n",
        "            axes[1, 1].set_yticklabels([f\"Rang {r}\" for r in top_ranks])\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.subheader(\"üìà Statistiques d√©taill√©es\")\n",
        "            stats_df = pd.DataFrame({\n",
        "                \"M√©trique\": [\"Score moyen\", \"Score m√©dian\", \"Score max\", \"Score min\", \"√âcart-type\"],\n",
        "                \"Valeur\": [np.mean(scores), np.median(scores), np.max(scores), np.min(scores), np.std(scores)]\n",
        "            })\n",
        "            st.dataframe(stats_df.round(4))\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'analyse des r√©sultats Q&A : {e}\")\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # 5) M√©thodes BLEU / ROUGE manquantes\n",
        "    # --------------------------------------------------\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calcule le BLEU score entre deux textes.\"\"\"\n",
        "        return self._bleu(reference, candidate)\n",
        "\n",
        "    def calculate_rouge_score(self, reference: str, candidate: str) -> dict:\n",
        "        \"\"\"Calcule les scores ROUGE entre deux textes.\"\"\"\n",
        "        return self._rouge_score(reference, candidate)\n",
        "\n",
        "    def visualize_bleu_rouge_scores(self, qa_results, references):\n",
        "        \"\"\"Visualisation BLEU & ROUGE pour chaque paire (ref, r√©sultat).\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for ref, res in zip(references, qa_results):\n",
        "            bleus.append(self._bleu(ref, res['text']))\n",
        "            r1s.append(self._rouge_score(ref, res['text'])['rouge-1'])\n",
        "            rls.append(self._rouge_score(ref, res['text'])['rouge-l'])\n",
        "\n",
        "        x = list(range(1, len(bleus)+1))\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.bar([i-0.2 for i in x], bleus, 0.4, label='BLEU')\n",
        "        plt.bar([i+0.2 for i in x], r1s, 0.4, label='ROUGE-1')\n",
        "        plt.xlabel('Rang')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('BLEU & ROUGE vs r√©f√©rences')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        st.pyplot(plt.gcf())\n",
        "\n",
        "    def evaluate_qa_performance(self, qa_module, questions, references):\n",
        "        \"\"\"√âvaluation compl√®te Q-A avec scores BLEU/ROUGE.\"\"\"\n",
        "        bleus, r1s, rls = [], [], []\n",
        "        for q, ref in zip(questions, references):\n",
        "            res = qa_module.query_with_fallback(q, top_k=1)\n",
        "            if res:\n",
        "                cand = res[0]['text']\n",
        "                bleus.append(self._bleu(ref, cand))\n",
        "                r1s.append(self._rouge_score(ref, cand)['rouge-1'])\n",
        "                rls.append(self._rouge_score(ref, cand)['rouge-l'])\n",
        "\n",
        "        st.write(\"### üìä Global Q-A metrics\")\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        col1.metric(\"Avg BLEU\", f\"{np.mean(bleus):.4f}\")\n",
        "        col2.metric(\"Avg ROUGE-1\", f\"{np.mean(r1s):.4f}\")\n",
        "        col3.metric(\"Avg ROUGE-L\", f\"{np.mean(rls):.4f}\")"
      ],
      "metadata": {
        "id": "oMZt-WwIMwez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "351ea5ad-0d81-4ca3-b979-242c4c8bf88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing visualization_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de Visualisation ‚Äì `visualization_modules.py`\n",
        "\n",
        "Ce module contient la classe `VisualizationManager`, qui regroupe l‚Äôensemble des outils n√©cessaires pour analyser, visualiser et interpr√©ter les r√©sultats du pipeline IA (entra√Ænement, classification, Q&A).\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est de rendre les r√©sultats **visibles, interpr√©tables et exploitables** √† toutes les √©tapes du pipeline :\n",
        "- Visualisation des courbes d'entra√Ænement pour suivre l'apprentissage du mod√®le,\n",
        "- Analyse fine de la classification via matrices de confusion et rapports d√©taill√©s,\n",
        "- √âvaluation des performances des r√©ponses g√©n√©r√©es (Q&A) avec des scores BLEU et ROUGE,\n",
        "- Affichage int√©gr√© dans Streamlit pour une interface interactive tout-en-un.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Faciliter la compr√©hension des performances sans expertise en machine learning,\n",
        "- Offrir des visualisations pertinentes et adapt√©es aux cas d‚Äôusage texte (classification, Q&A),\n",
        "- Rendre l‚Äôanalyse compatible avec un **d√©ploiement Streamlit** pour usage m√©tier,\n",
        "- G√©rer la diversit√© des types de r√©sultats (courbes, matrices, histogrammes, rapports).\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| Composant | Justification |\n",
        "|-----------|----------------|\n",
        "| `matplotlib`, `seaborn` | Biblioth√®ques standards et flexibles pour des graphiques clairs |\n",
        "| `Streamlit` | Int√©gration simple d‚Äô√©l√©ments visuels dans une application web interactive |\n",
        "| `classification_report` et `confusion_matrix` | Outils classiques pour √©valuer la qualit√© de la classification |\n",
        "| `BLEU` / `ROUGE` (NLTK, rouge_score) | R√©f√©rences en NLP pour comparer textes g√©n√©r√©s et attendus |\n",
        "| `Counter`, `json`, `os` | Pour l‚Äôanalyse dynamique de r√©sultats (logs, distributions) |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "Ce module permet de :\n",
        "- Visualiser la dynamique de l‚Äôapprentissage (perte, pr√©cision, f1),\n",
        "- Comprendre les erreurs du mod√®le gr√¢ce √† la matrice de confusion,\n",
        "- Analyser les scores des r√©sultats de questions-r√©ponses selon plusieurs angles,\n",
        "- Quantifier la qualit√© des r√©ponses g√©n√©r√©es en les comparant √† des r√©f√©rences humaines (BLEU/ROUGE),\n",
        "- Explorer la distribution des classes dans les donn√©es.\n",
        "\n",
        "Les graphiques facilitent la **prise de d√©cision** pour ajuster le mod√®le ou identifier ses limites.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module joue un r√¥le cl√© pour **rendre l‚ÄôIA explicable**, accessible et op√©rationnelle dans un cadre m√©tier. Il transforme des sorties num√©riques en **indicateurs visuels** utiles pour la validation, l‚Äôam√©lioration continue et la communication autour du projet.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- Permet aux √©quipes non techniques (RSE, communication, marketing) de **visualiser facilement les r√©sultats**,\n",
        "- Identifie rapidement les classes mal pr√©dites ou les biais du mod√®le,\n",
        "- Facilite l‚Äô√©valuation qualitative et quantitative des r√©ponses g√©n√©r√©es dans des contextes sensibles,\n",
        "- Contribue √† une meilleure adoption des solutions IA gr√¢ce √† une interpr√©tabilit√© renforc√©e.\n"
      ],
      "metadata": {
        "id": "rllSG2_FovTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. **qa_modules.py**"
      ],
      "metadata": {
        "id": "1HSXVYNkNc-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py - Version corrig√©e et optimis√©e\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "from functools import lru_cache\n",
        "import hashlib\n",
        "import os\n",
        "\n",
        "class OptimizedQAModule:\n",
        "    \"\"\"Version optimis√©e du module Q&A avec caching et performance am√©lior√©e\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        try:\n",
        "            self.model = SentenceTransformer(model_name, device='cpu')\n",
        "            self.similarity_threshold = 0.3\n",
        "\n",
        "            # Structures optimis√©es\n",
        "            self.corpus_texts: List[str] = []\n",
        "            self.corpus_labels: List[int] = []\n",
        "            self.corpus_embeddings: Optional[np.ndarray] = None\n",
        "            self.corpus_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Base de connaissances optimis√©e\n",
        "            self.knowledge_base: List[Dict[str, Any]] = []\n",
        "            self.kb_embeddings: Optional[np.ndarray] = None\n",
        "            self.kb_index: Optional[faiss.Index] = None\n",
        "\n",
        "            # Cache pour les requ√™tes fr√©quentes\n",
        "            self._query_cache: Dict[str, List[Dict]] = {}\n",
        "\n",
        "            self._setup_knowledge_base()\n",
        "            logging.info(\"‚úÖ Module Q&A initialis√© avec succ√®s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur initialisation Q&A: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _setup_knowledge_base(self):\n",
        "        \"\"\"Initialisation optimis√©e de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            {\n",
        "                \"text\": \"Le r√©chauffement climatique est principalement caus√© par les √©missions de CO2 humaines.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"CO2\", \"√©missions\", \"humaines\", \"causes\"],\n",
        "                \"weight\": 1.0\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Les √©nergies renouvelables (solaire, √©olien, hydro) r√©duisent drastiquement les √©missions.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"renouvelables\", \"solaire\", \"√©olien\", \"hydro\"],\n",
        "                \"weight\": 1.2\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"La d√©forestation est responsable de 15% des √©missions mondiales de CO2.\",\n",
        "                \"category\": \"causes\",\n",
        "                \"keywords\": [\"d√©forestation\", \"for√™ts\", \"15%\"],\n",
        "                \"weight\": 0.9\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Le transport repr√©sente 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "                \"category\": \"secteurs\",\n",
        "                \"keywords\": [\"transport\", \"24%\", \"v√©hicules\"],\n",
        "                \"weight\": 1.1\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"L'isolation thermique peut r√©duire la consommation √©nerg√©tique jusqu'√† 50%.\",\n",
        "                \"category\": \"solutions\",\n",
        "                \"keywords\": [\"isolation\", \"thermique\", \"50%\"],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "        ]\n",
        "        self._rebuild_kb_index()\n",
        "\n",
        "    def _rebuild_kb_index(self):\n",
        "        \"\"\"Reconstruction optimis√©e de l'index FAISS\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            texts = [item[\"text\"] for item in self.knowledge_base]\n",
        "            self.kb_embeddings = self.model.encode(\n",
        "                texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "\n",
        "            # Index FAISS optimis√©\n",
        "            d = self.kb_embeddings.shape[1]\n",
        "            self.kb_index = faiss.IndexFlatIP(d)\n",
        "            self.kb_index.add(self.kb_embeddings.astype(np.float32))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur reconstruction index: {e}\")\n",
        "\n",
        "    def fit(self, dataset: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Indexation optimis√©e du corpus d'entra√Ænement\"\"\"\n",
        "        try:\n",
        "            if not dataset:\n",
        "                logging.warning(\"‚ö†Ô∏è Dataset vide, rien √† indexer\")\n",
        "                return False\n",
        "\n",
        "            self.corpus_texts = [d[\"text\"] for d in dataset]\n",
        "            self.corpus_labels = [d.get(\"label_id\", 0) for d in dataset]\n",
        "\n",
        "            # Encodage optimis√© avec batch processing\n",
        "            batch_size = 32\n",
        "            embeddings = []\n",
        "\n",
        "            for i in range(0, len(self.corpus_texts), batch_size):\n",
        "                batch = self.corpus_texts[i:i+batch_size]\n",
        "                batch_embeddings = self.model.encode(\n",
        "                    batch,\n",
        "                    normalize_embeddings=True,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_numpy=True\n",
        "                )\n",
        "                embeddings.append(batch_embeddings)\n",
        "\n",
        "            self.corpus_embeddings = np.vstack(embeddings)\n",
        "\n",
        "            # Index FAISS\n",
        "            d = self.corpus_embeddings.shape[1]\n",
        "            self.corpus_index = faiss.IndexFlatIP(d)\n",
        "            self.corpus_index.add(self.corpus_embeddings.astype(np.float32))\n",
        "\n",
        "            # Mise √† jour du cache\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"‚úÖ {len(dataset)} documents index√©s avec succ√®s\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur indexation: {e}\")\n",
        "            return False\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def _get_query_embedding(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Cache des embeddings de requ√™tes fr√©quentes\"\"\"\n",
        "        return self.model.encode([query], normalize_embeddings=True)[0]\n",
        "\n",
        "    def _search_index(self, query: str, index: faiss.Index, texts: List[str],\n",
        "                     source: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans un index FAISS\"\"\"\n",
        "        if index is None or not texts:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_embedding = self._get_query_embedding(query)\n",
        "            scores, indices = index.search(\n",
        "                query_embedding.reshape(1, -1).astype(np.float32),\n",
        "                min(top_k, len(texts))\n",
        "            )\n",
        "\n",
        "            results = []\n",
        "            for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), 1):\n",
        "                if idx < len(texts) and score > self.similarity_threshold:\n",
        "                    result = {\n",
        "                        \"text\": texts[idx],\n",
        "                        \"score\": float(score),\n",
        "                        \"rank\": rank,\n",
        "                        \"source\": source\n",
        "                    }\n",
        "\n",
        "                    # Ajout des m√©tadonn√©es si disponible\n",
        "                    if source == \"knowledge_base\" and idx < len(self.knowledge_base):\n",
        "                        result.update({\n",
        "                            \"category\": self.knowledge_base[idx][\"category\"],\n",
        "                            \"keywords\": self.knowledge_base[idx][\"keywords\"]\n",
        "                        })\n",
        "                    elif source == \"training_corpus\" and idx < len(self.corpus_labels):\n",
        "                        result[\"label_id\"] = int(self.corpus_labels[idx])\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche index: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_knowledge_base(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans la base de connaissances\"\"\"\n",
        "        if not self.knowledge_base:\n",
        "            return []\n",
        "        return self._search_index(query, self.kb_index,\n",
        "                                [item[\"text\"] for item in self.knowledge_base],\n",
        "                                \"knowledge_base\", top_k)\n",
        "\n",
        "    def query_corpus(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche optimis√©e dans le corpus d'entra√Ænement\"\"\"\n",
        "        if not self.corpus_texts:\n",
        "            return []\n",
        "        return self._search_index(query, self.corpus_index, self.corpus_texts,\n",
        "                                \"training_corpus\", top_k)\n",
        "\n",
        "    def keyword_search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par mots-cl√©s optimis√©e avec scoring avanc√©\"\"\"\n",
        "        if not query:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query.lower()))\n",
        "            if not query_words:\n",
        "                return []\n",
        "\n",
        "            scored = []\n",
        "\n",
        "            # Recherche dans la base de connaissances\n",
        "            for item in self.knowledge_base:\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', item[\"text\"].lower()))\n",
        "                keyword_words = set(item[\"keywords\"])\n",
        "\n",
        "                # Score combin√© TF-IDF like\n",
        "                text_score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "                keyword_score = len(query_words & keyword_words) / max(len(keyword_words), 1)\n",
        "                combined_score = (text_score * 0.7 + keyword_score * 0.3) * item.get(\"weight\", 1.0)\n",
        "\n",
        "                if combined_score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": item[\"text\"],\n",
        "                        \"category\": item[\"category\"],\n",
        "                        \"keywords\": item[\"keywords\"],\n",
        "                        \"score\": combined_score,\n",
        "                        \"source\": \"knowledge_base\"\n",
        "                    })\n",
        "\n",
        "            # Recherche dans le corpus\n",
        "            for i, text in enumerate(self.corpus_texts):\n",
        "                text_words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "                score = len(query_words & text_words) / max(len(text_words), 1)\n",
        "\n",
        "                if score > 0.1:\n",
        "                    scored.append({\n",
        "                        \"text\": text,\n",
        "                        \"label_id\": int(self.corpus_labels[i]),\n",
        "                        \"score\": score,\n",
        "                        \"source\": \"training_corpus\"\n",
        "                    })\n",
        "\n",
        "            scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "            for i, result in enumerate(scored[:top_k], 1):\n",
        "                result[\"rank\"] = i\n",
        "\n",
        "            return scored[:top_k]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche mots-cl√©s: {e}\")\n",
        "            return []\n",
        "\n",
        "    def query_with_fallback(self, question: str, top_k: int = 5,\n",
        "                          search_mode: str = \"hybrid\") -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche avec fallback optimis√©\"\"\"\n",
        "        # Cl√© de cache\n",
        "        cache_key = f\"{question}_{search_mode}_{top_k}\"\n",
        "        if cache_key in self._query_cache:\n",
        "            return self._query_cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            # S√©lection du mode de recherche\n",
        "            if search_mode == \"knowledge_only\":\n",
        "                results = self.query_knowledge_base(question, top_k)\n",
        "            elif search_mode == \"corpus_only\":\n",
        "                results = self.query_corpus(question, top_k)\n",
        "            elif search_mode == \"keywords\":\n",
        "                results = self.keyword_search(question, top_k)\n",
        "            else:  # hybrid\n",
        "                kb_results = self.query_knowledge_base(question, top_k // 2 + 1)\n",
        "                corpus_results = self.query_corpus(question, top_k // 2 + 1)\n",
        "\n",
        "                # Fusion et d√©duplication\n",
        "                seen_texts = set()\n",
        "                results = []\n",
        "\n",
        "                for item in kb_results + corpus_results:\n",
        "                    if item[\"text\"] not in seen_texts:\n",
        "                        seen_texts.add(item[\"text\"])\n",
        "                        results.append(item)\n",
        "\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Fallback si n√©cessaire\n",
        "            if not results or (results and max(r[\"score\"] for r in results) < self.similarity_threshold):\n",
        "                fallback = self.keyword_search(question, top_k)\n",
        "                seen = {r[\"text\"] for r in results}\n",
        "                for item in fallback:\n",
        "                    if item[\"text\"] not in seen:\n",
        "                        results.append(item)\n",
        "                results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "                results = results[:top_k]\n",
        "\n",
        "            # Mise en cache\n",
        "            self._query_cache[cache_key] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche avec fallback: {e}\")\n",
        "            return []\n",
        "\n",
        "    def search_by_category(self, category: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Recherche par cat√©gorie optimis√©e\"\"\"\n",
        "        try:\n",
        "            results = [\n",
        "                {\n",
        "                    \"text\": item[\"text\"],\n",
        "                    \"category\": item[\"category\"],\n",
        "                    \"keywords\": item[\"keywords\"],\n",
        "                    \"source\": \"knowledge_base\",\n",
        "                    \"rank\": i + 1,\n",
        "                    \"score\": 1.0\n",
        "                }\n",
        "                for i, item in enumerate([\n",
        "                    it for it in self.knowledge_base\n",
        "                    if it[\"category\"].lower() == category.lower()\n",
        "                ][:top_k])\n",
        "            ]\n",
        "            return results\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur recherche cat√©gorie: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, text: str, category: str = \"custom\",\n",
        "                     keywords: List[str] = None) -> bool:\n",
        "        \"\"\"Ajout optimis√© de nouvelles connaissances\"\"\"\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return False\n",
        "\n",
        "            # V√©rification des doublons\n",
        "            if any(item[\"text\"].strip() == text.strip() for item in self.knowledge_base):\n",
        "                return False\n",
        "\n",
        "            new_item = {\n",
        "                \"text\": text.strip(),\n",
        "                \"category\": category,\n",
        "                \"keywords\": keywords or [],\n",
        "                \"weight\": 1.0\n",
        "            }\n",
        "\n",
        "            self.knowledge_base.append(new_item)\n",
        "            self._rebuild_kb_index()\n",
        "            self._query_cache.clear()\n",
        "\n",
        "            logging.info(f\"‚úÖ Nouvelle connaissance ajout√©e: {text[:50]}...\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur ajout connaissance: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_categories(self) -> List[str]:\n",
        "        \"\"\"Retourne les cat√©gories disponibles\"\"\"\n",
        "        try:\n",
        "            return list({item[\"category\"] for item in self.knowledge_base})\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Statistiques d√©taill√©es et optimis√©es\"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                \"knowledge_base_size\": len(self.knowledge_base),\n",
        "                \"training_corpus_size\": len(self.corpus_texts),\n",
        "                \"total_documents\": len(self.knowledge_base) + len(self.corpus_texts),\n",
        "                \"categories\": self.get_categories(),\n",
        "                \"avg_kb_length\": np.mean([len(item[\"text\"]) for item in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "                \"avg_corpus_length\": np.mean([len(t) for t in self.corpus_texts]) if self.corpus_texts else 0,\n",
        "                \"cache_size\": len(self._query_cache)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Nettoyage manuel du cache\"\"\"\n",
        "        self._query_cache.clear()\n",
        "        logging.info(\"üóëÔ∏è Cache vid√©\")"
      ],
      "metadata": {
        "id": "CdRC_3s7Ncy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ffb7a4-ab03-44d4-aedd-31d5ff4885ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing qa_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Q&A Optimis√© ‚Äì `qa_modules.py`\n",
        "\n",
        "Ce module d√©finit la classe `OptimizedQAModule`, qui met en ≈ìuvre un moteur de questions-r√©ponses s√©mantique, bas√© sur la recherche vectorielle avec FAISS et l'encodage de texte via Sentence Transformers.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est d'offrir un syst√®me intelligent capable de :\n",
        "- R√©pondre automatiquement √† des questions en langage naturel,\n",
        "- S‚Äôappuyer √† la fois sur un corpus d‚Äôentra√Ænement et une base de connaissances manuelle,\n",
        "- Utiliser des embeddings s√©mantiques pour trouver les textes les plus pertinents,\n",
        "- Fournir des r√©sultats rapides, coh√©rents et justifi√©s, m√™me avec un petit jeu de donn√©es.\n",
        "\n",
        "Le module repose sur des strat√©gies hybrides combinant recherche vectorielle, mots-cl√©s, cat√©gories et cache intelligent.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Trouver une **repr√©sentation vectorielle efficace** du langage naturel,\n",
        "- G√©rer la **performance et la rapidit√©** des requ√™tes avec FAISS,\n",
        "- Construire une base de connaissances **explicable et modifiable**,\n",
        "- R√©pondre de mani√®re robuste m√™me en cas de question in√©dite (gr√¢ce au fallback),\n",
        "- Optimiser les ressources via le **caching** et la gestion des doublons.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment | Justification |\n",
        "|--------|----------------|\n",
        "| `SentenceTransformer` (`MiniLM`) | Mod√®le rapide et pr√©cis pour la similarit√© s√©mantique |\n",
        "| `FAISS` | Moteur de recherche vectorielle ultra-performant, adapt√© aux grands volumes |\n",
        "| `query_with_fallback()` | Combine plusieurs strat√©gies de recherche pour maximiser la robustesse |\n",
        "| `knowledge_base` | Base structur√©e avec cat√©gories, mots-cl√©s, pond√©rations |\n",
        "| `lru_cache` + `query_cache` | Acc√©l√©ration des requ√™tes r√©p√©t√©es, am√©lioration des performances |\n",
        "| `keyword_search()` | Alternative lexicale si la recherche s√©mantique √©choue |\n",
        "| `add_knowledge()` | Permet d‚Äôenrichir dynamiquement la base sans red√©ployer le syst√®me |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "Le module retourne une **liste class√©e de r√©ponses pertinentes**, issues :\n",
        "- Soit de la base de connaissances (texte + m√©tadonn√©es),\n",
        "- Soit du corpus d‚Äôentra√Ænement,\n",
        "- Avec un score de similarit√© et un rang attribu√© √† chaque r√©sultat.\n",
        "\n",
        "Il est possible de filtrer par cat√©gorie, d‚Äôanalyser les performances (via d‚Äôautres modules) ou d‚Äôajouter de nouvelles donn√©es en direct.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module constitue le c≈ìur s√©mantique du syst√®me : il permet de **transformer une simple question en une r√©ponse intelligente**, fond√©e sur les connaissances disponibles. Il est √† la fois **modulaire**, **interpr√©table**, et **personnalisable**.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- Automatisation de la recherche d‚Äôinformation dans des textes non structur√©s,\n",
        "- Gain de temps pour les analystes, communicants ou charg√©s de mission,\n",
        "- R√©ponses contextualis√©es m√™me sans supervision humaine,\n",
        "- Extension facile pour s‚Äôadapter √† de nouveaux domaines (RH, juridique, environnement‚Ä¶),\n",
        "- Outil strat√©gique pour la veille, la m√©diation, ou la communication de crise.\n"
      ],
      "metadata": {
        "id": "Zlfd7s-1pP8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. **Module Knowledge Base - knowledge_modules.py**"
      ],
      "metadata": {
        "id": "b_Ply7wGI-4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ],
      "metadata": {
        "id": "B03nOJeFI_US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2088e44-fc08-4ffe-bc9e-28d6eae12078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing knowledge_modules.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module Base de Connaissances Simple ‚Äì `knowledge_modules.py`\n",
        "\n",
        "Ce module propose une version all√©g√©e de moteur de recherche de connaissances, bas√©e uniquement sur une similarit√© lexicale (sans embeddings), utilisable pour des cas simples ou des environnements sans GPU ni d√©pendances lourdes.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est de fournir une **base de connaissances consultable** rapidement, m√™me dans un contexte sans infrastructure complexe. Cette version ne repose pas sur des mod√®les comme `sentence-transformers`, mais sur des techniques classiques de traitement de texte (nettoyage, tokenisation, score de similarit√© par Jaccard).\n",
        "\n",
        "Elle permet de r√©pondre √† une question en retrouvant les phrases les plus proches lexicalement, selon les mots en commun.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Permettre une **recherche l√©g√®re**, rapide √† ex√©cuter et facile √† interpr√©ter,\n",
        "- G√©rer des documents courts, de type \"faits\" ou \"connaissances environnementales\",\n",
        "- √âviter la complexit√© des mod√®les s√©mantiques tout en conservant une pertinence minimale,\n",
        "- Trouver un compromis acceptable entre **simplicit√©** et **qualit√© des r√©sultats**.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment | Justification |\n",
        "|--------|----------------|\n",
        "| Liste Python de phrases | Structure simple, modifiable √† chaud, id√©ale pour des donn√©es statiques |\n",
        "| Score de Jaccard | Mesure classique des similarit√©s lexicales (intersection / union) |\n",
        "| Seuil √† 0.1 | Permet de filtrer les r√©sultats trop faibles ou bruit√©s |\n",
        "| `find_context()` | Fonction centrale pour extraire les 3 connaissances les plus proches |\n",
        "| `add_knowledge()` | Possibilit√© d‚Äôenrichir dynamiquement la base |\n",
        "| `get_stats()` | Outil utile pour √©valuer la couverture et la densit√© de la base |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "√Ä chaque requ√™te, le module retourne **jusqu'√† 3 phrases** de la base consid√©r√©es comme pertinentes, avec un score implicite bas√© sur les mots en commun avec la question. Ce syst√®me est robuste pour des requ√™tes simples ou guid√©es, moins adapt√© √† des formulations complexes ou implicites.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module constitue une **brique l√©g√®re, interpr√©table et autonome** du syst√®me. Il peut servir de fallback ou de moteur Q&A de premier niveau, utilisable dans des environnements contraints (API rapide, edge computing, notebook p√©dagogique, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- **Facile √† int√©grer** dans une application ou un outil low-code,\n",
        "- **Aucune d√©pendance lourde** : compatible avec des infrastructures minimales,\n",
        "- **Maintenance simple** : la base peut √™tre enrichie ou modifi√©e par des profils non techniques,\n",
        "- Peut servir de moteur de recherche de **FAQ, d√©finitions ou glossaire** dans un contexte m√©tier ou √©ducatif.\n"
      ],
      "metadata": {
        "id": "fHUWrpLKpqtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. **update_kb_rss.py**"
      ],
      "metadata": {
        "id": "duM3V0Q02w8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile update_kb_rss.py\n",
        "import feedparser\n",
        "from qa_modules import OptimizedQAModule\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "import schedule\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Configuration du logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Configuration des URLs RSS valides\n",
        "RSS_FEEDS = [\n",
        "    \"https://www.carbonbrief.org/feed/\",\n",
        "    \"https://climate.nasa.gov/news/rss.xml\",\n",
        "    \"https://unfccc.int/news/rss.xml\"\n",
        "]\n",
        "\n",
        "class RSSUpdater:\n",
        "    def __init__(self, qa_module):\n",
        "        self.qa = qa_module\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=400,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\", \". \", \"? \", \"! \"]\n",
        "        )\n",
        "\n",
        "    def fetch_and_process_feed(self, feed_url, max_entries=5):\n",
        "        \"\"\"R√©cup√®re et traite un flux RSS\"\"\"\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            if feed.bozo:\n",
        "                logging.warning(f\"‚ö†Ô∏è Flux RSS mal format√©: {feed_url}\")\n",
        "                return 0\n",
        "\n",
        "            processed = 0\n",
        "            for entry in feed.entries[:max_entries]:\n",
        "                try:\n",
        "                    # Extraction des informations\n",
        "                    title = entry.title if hasattr(entry, 'title') else \"Sans titre\"\n",
        "                    summary = entry.summary if hasattr(entry, 'summary') else \"\"\n",
        "                    link = entry.link if hasattr(entry, 'link') else \"Lien manquant\"\n",
        "\n",
        "                    # Nettoyage du texte\n",
        "                    content = f\"{title}. {summary}\".strip()\n",
        "                    if len(content) < 50:  # Skip trop courts\n",
        "                        continue\n",
        "\n",
        "                    # Cr√©ation du document\n",
        "                    doc = Document(\n",
        "                        page_content=content,\n",
        "                        metadata={\n",
        "                            \"url\": link,\n",
        "                            \"title\": title,\n",
        "                            \"source\": \"rss_feed\",\n",
        "                            \"date\": datetime.datetime.now().isoformat()\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    # D√©coupage en chunks\n",
        "                    chunks = self.text_splitter.split_documents([doc])\n",
        "\n",
        "                    # Ajout √† la base de connaissances\n",
        "                    for chunk in chunks:\n",
        "                        success = self.qa.add_knowledge(\n",
        "                            text=chunk.page_content,\n",
        "                            category=\"rss_news\",\n",
        "                            keywords=[\"rss\", \"news\", \"climate\", \"update\"]\n",
        "                        )\n",
        "                        if success:\n",
        "                            processed += 1\n",
        "                            logging.info(f\"‚úÖ Ajout√©: {title[:60]}...\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"‚ùå Erreur traitement article: {e}\")\n",
        "                    continue\n",
        "\n",
        "            return processed\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"‚ùå Erreur flux RSS {feed_url}: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def update_all_feeds(self):\n",
        "        \"\"\"Mise √† jour de tous les flux RSS\"\"\"\n",
        "        total_added = 0\n",
        "        logging.info(f\"üîÑ D√©but mise √† jour RSS - {datetime.datetime.now()}\")\n",
        "\n",
        "        for feed_url in RSS_FEEDS:\n",
        "            added = self.fetch_and_process_feed(feed_url)\n",
        "            total_added += added\n",
        "            logging.info(f\"üìä {feed_url}: {added} articles ajout√©s\")\n",
        "\n",
        "        logging.info(f\"‚úÖ Mise √† jour termin√©e - Total: {total_added} nouveaux articles\")\n",
        "        return total_added\n",
        "\n",
        "    def start_scheduler(self):\n",
        "        \"\"\"D√©marre le planificateur RSS\"\"\"\n",
        "        # Planification quotidienne √† 9h\n",
        "        schedule.every().day.at(\"09:00\").do(self.update_all_feeds)\n",
        "\n",
        "        # Test imm√©diat\n",
        "        self.update_all_feeds()\n",
        "\n",
        "        logging.info(\"üìÖ Planificateur RSS d√©marr√© - mise √† jour quotidienne √† 09:00\")\n",
        "\n",
        "        # Boucle d'ex√©cution\n",
        "        while True:\n",
        "            schedule.run_pending()\n",
        "            time.sleep(3600)  # V√©rification toutes les heures\n",
        "\n",
        "# Fonction utilitaire pour Streamlit\n",
        "def init_rss_updater(qa_module):\n",
        "    \"\"\"Initialise le RSS updater pour Streamlit\"\"\"\n",
        "    updater = RSSUpdater(qa_module)\n",
        "    return updater\n",
        "\n",
        "def manual_rss_update(qa_module):\n",
        "    \"\"\"Mise √† jour manuelle via Streamlit\"\"\"\n",
        "    try:\n",
        "        updater = RSSUpdater(qa_module)\n",
        "        return updater.update_all_feeds()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Erreur mise √† jour RSS: {e}\")\n",
        "        return 0"
      ],
      "metadata": {
        "id": "L726nrJs2wq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfae87c-419f-4e0f-b69a-e94483713618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing update_kb_rss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module de mise √† jour RSS ‚Äì `update_kb_rss.py`\n",
        "\n",
        "Ce module permet d‚Äôenrichir dynamiquement la base de connaissances √† partir de flux d‚Äôactualit√©s RSS fiables sur le climat (NASA, UNFCCC, CarbonBrief), en utilisant le moteur Q&A s√©mantique d√©j√† en place.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est de rendre le syst√®me **vivant et actualis√©**, en automatisant l‚Äôajout de contenus r√©cents, cr√©dibles et contextualis√©s dans la base de connaissances.  \n",
        "Le module extrait, nettoie, d√©coupe et indexe chaque article de mani√®re autonome, afin de rendre les nouvelles informations imm√©diatement interrogeables via le module Q&A.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Acc√©der √† des contenus r√©cents **sans API complexe** (RSS = standard l√©ger et universel),\n",
        "- Nettoyer et d√©couper les articles pour produire des **chunks exploitables s√©mantiquement**,\n",
        "- Filtrer les contenus trop courts ou bruit√©s,\n",
        "- **Pr√©server la coh√©rence** s√©mantique des ajouts (cat√©gorisation, mots-cl√©s),\n",
        "- Int√©grer les nouvelles connaissances **sans duplication** ni perte de performances.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment | R√¥le / justification |\n",
        "|---------|----------------------|\n",
        "| `feedparser` | Lecture universelle de flux RSS au format XML |\n",
        "| `langchain.text_splitter` | D√©coupe optimis√©e en chunks de 400 tokens avec recouvrement |\n",
        "| `Document(metadata=...)` | Standardisation des articles sous forme structur√©e |\n",
        "| `add_knowledge()` (Q&A) | Int√©gration imm√©diate dans l‚Äôindex s√©mantique FAISS |\n",
        "| `schedule` | Planification automatis√©e quotidienne (9h) |\n",
        "| `manual_rss_update()` | Fonction de d√©clenchement manuel depuis Streamlit |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "Chaque ex√©cution ajoute **des dizaines de nouveaux documents** (articles, paragraphes) dans la base, index√©s avec score, cat√©gorie, date et provenance.  \n",
        "Lorsqu‚Äôun utilisateur pose une question, ces nouvelles connaissances seront automatiquement propos√©es en priorit√© si elles sont pertinentes.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module transforme le moteur Q&A en **assistant d‚Äôactualit√©s climatiques**, toujours au courant des derniers d√©veloppements, sans effort manuel.  \n",
        "Il permet un **alignement continu avec l‚Äôactualit√©**, tout en conservant un syst√®me robuste et modulaire.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- Acc√®s √† une **veille climatique automatis√©e**, int√©gr√©e au moteur IA,\n",
        "- Enrichissement progressif sans r√©entra√Ænement du mod√®le,\n",
        "- R√©duction du **temps de collecte et de structuration** d‚Äôinformations externes,\n",
        "- Meilleure **r√©activit√© strat√©gique** pour les communicants, journalistes, ONG ou collectivit√©s,\n",
        "- Base de connaissances toujours fra√Æche, sans d√©pendre d‚Äôun expert technique.\n"
      ],
      "metadata": {
        "id": "pZpztvk_qIJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. **Module Streamlit - streamlit_app.py**"
      ],
      "metadata": {
        "id": "8HIRsUgmJLaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "# streamlit_app_fusion.py - Version fusionn√©e avec optimisations et RSS\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from contextlib import contextmanager\n",
        "from qa_modules import OptimizedQAModule  # Module optimis√©\n",
        "\n",
        "# Configuration optimis√©e de Streamlit\n",
        "st.set_page_config(\n",
        "    page_title=\"üåç Climate Analyzer Pro\",\n",
        "    page_icon=\"üåç\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Barre de progression persistante\n",
        "# ---------------------------------------------------------\n",
        "@contextmanager\n",
        "def st_progress(title=\"Progress\", max_value=100):\n",
        "    bar = st.progress(0, text=title)\n",
        "    try:\n",
        "        yield bar\n",
        "    finally:\n",
        "        bar.empty()\n",
        "\n",
        "# Cache optimis√© pour le module Q&A\n",
        "@st.cache_resource\n",
        "def get_optimized_qa():\n",
        "    \"\"\"Cache du module Q&A optimis√©\"\"\"\n",
        "    return OptimizedQAModule()\n",
        "\n",
        "# Persistance session_state - Initialisation correcte\n",
        "if \"trainer\" not in st.session_state:\n",
        "    st.session_state.trainer = None\n",
        "if \"label_names\" not in st.session_state:\n",
        "    st.session_state.label_names = None\n",
        "if \"test_ds\" not in st.session_state:\n",
        "    st.session_state.test_ds = None\n",
        "if \"training\" not in st.session_state:\n",
        "    st.session_state.training = False\n",
        "if \"raw_train_data\" not in st.session_state:\n",
        "    st.session_state.raw_train_data = None\n",
        "if \"qa_module\" not in st.session_state:\n",
        "    st.session_state.qa_module = get_optimized_qa()  # Utilisation du module optimis√©\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from visualization_modules import VisualizationManager\n",
        "\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "\n",
        "        # Fusion des modules Q&A\n",
        "        if st.session_state.qa_module is None:\n",
        "            st.session_state.qa_module = get_optimized_qa()\n",
        "\n",
        "        self.qa_module = st.session_state.qa_module\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.load_saved_model()\n",
        "\n",
        "    def load_saved_model(self):\n",
        "        \"\"\"Chargement automatique du mod√®le si d√©j√† pr√©sent\"\"\"\n",
        "        if st.session_state.trainer is None and os.path.exists(\"outputs/final_model/config.json\"):\n",
        "            try:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                num_labels = len(self.data_processor.label_mapping) or 2\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                trainer = self.model_manager.setup_trainer(None, None)\n",
        "                trainer.model = trainer.model.from_pretrained(\"outputs/final_model\")\n",
        "                st.session_state.trainer = trainer\n",
        "                st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "                st.success(\"‚úÖ Mod√®le charg√© depuis le disque.\")\n",
        "            except Exception as e:\n",
        "                st.warning(f\"‚ö†Ô∏è Chargement impossible : {e}\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Menu principal de l'application\"\"\"\n",
        "        st.title(\"üåç Climate Sentiment Analyzer Pro\")\n",
        "\n",
        "        # Sidebar optimis√©e avec statistiques Q&A\n",
        "        with st.sidebar:\n",
        "            st.markdown(\"### üìä Statistiques Syst√®me\")\n",
        "\n",
        "            stats = self.qa_module.get_stats()\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                st.metric(\"Base de connaissances\", stats[\"knowledge_base_size\"])\n",
        "            with col2:\n",
        "                st.metric(\"Total documents\", stats[\"total_documents\"])\n",
        "\n",
        "            # Contr√¥les rapides\n",
        "            if st.button(\"üóëÔ∏è Vider le cache Q&A\"):\n",
        "                self.qa_module.clear_cache()\n",
        "                st.success(\"Cache vid√©!\")\n",
        "                st.rerun()\n",
        "\n",
        "            # Navigation principale\n",
        "            mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"üöÄ Pipeline Complet\", \"‚ùì Q&A Avanc√©e\", \"üìö Gestion des Connaissances\",\n",
        "                 \"üì∞ Mise √† jour RSS\", \"üìà Visualisations\"]\n",
        "            )\n",
        "\n",
        "        # Routage vers les diff√©rentes sections\n",
        "        if mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"‚ùì Q&A Avanc√©e\":\n",
        "            self.run_advanced_qa_interface()\n",
        "        elif mode == \"üìö Gestion des Connaissances\":\n",
        "            self.run_knowledge_management()\n",
        "        elif mode == \"üì∞ Mise √† jour RSS\":\n",
        "            self.run_rss_integration()\n",
        "        elif mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Pipeline complet d'entra√Ænement (inchang√© mais optimis√©)\"\"\"\n",
        "        st.header(\"üöÄ Pipeline Complet\")\n",
        "\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            # SLIDERS avec valeurs par d√©faut optimis√©es\n",
        "            sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, value=4000)\n",
        "            self.config.epochs = st.slider(\"Epochs\", 1, 5, value=3)\n",
        "\n",
        "            is_training = st.session_state.get(\"training\", False)\n",
        "\n",
        "            if st.button(\n",
        "                \"üöÄ Lancer l'entra√Ænement\",\n",
        "                type=\"primary\",\n",
        "                disabled=bool(is_training)\n",
        "            ):\n",
        "                st.session_state.training = True\n",
        "                try:\n",
        "                    self.train_pipeline(df, sample_size)\n",
        "                finally:\n",
        "                    st.session_state.training = False\n",
        "\n",
        "    def train_pipeline(self, df: pd.DataFrame, sample_size: int):\n",
        "        \"\"\"Processus d'entra√Ænement optimis√©\"\"\"\n",
        "        try:\n",
        "            # 1/4 ‚Äî Analyse des donn√©es avec cache\n",
        "            with st_progress(\"1/4  Analyse des donn√©es ‚Ä¶\") as bar:\n",
        "                train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "                bar.progress(25)\n",
        "\n",
        "            # Sauvegarder les donn√©es brutes\n",
        "            raw_train_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                            for item in train_ds]\n",
        "            st.session_state.raw_train_data = raw_train_data\n",
        "\n",
        "            # 2/4 ‚Äî Tokenizer\n",
        "            with st_progress(\"2/4  Chargement du tokenizer ‚Ä¶\") as bar:\n",
        "                self.model_manager.setup_tokenizer()\n",
        "                bar.progress(50)\n",
        "\n",
        "            # 3/4 ‚Äî Mod√®le\n",
        "            with st_progress(\"3/4  Initialisation du mod√®le ‚Ä¶\") as bar:\n",
        "                num_labels = len(self.data_processor.label_mapping)\n",
        "                self.model_manager.setup_model(num_labels)\n",
        "                bar.progress(75)\n",
        "\n",
        "            # 4/4 ‚Äî Tokenisation optimis√©e\n",
        "            def prep(ds):\n",
        "                with st_progress(\"4/4  Tokenisation ‚Ä¶\") as bar:\n",
        "                    ds = ds.map(\n",
        "                        self.model_manager.tokenize_function,\n",
        "                        batched=True,\n",
        "                        desc=\"Tokenisation\"\n",
        "                    )\n",
        "                    ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                    keep = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
        "                    for col in list(ds.column_names):\n",
        "                        if col not in keep:\n",
        "                            ds = ds.remove_columns(col)\n",
        "                    ds.set_format(type=\"torch\", columns=list(keep))\n",
        "                    bar.progress(100)\n",
        "                    return ds\n",
        "\n",
        "            train_ds_processed, val_ds_processed, test_ds_processed = map(prep, (train_ds, val_ds, test_ds))\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds_processed, val_ds_processed)\n",
        "\n",
        "            with st.spinner(\"Entra√Ænement en cours ‚Ä¶\"):\n",
        "                trainer.train()\n",
        "\n",
        "            trainer.save_model(\"outputs/final_model\")\n",
        "            trainer.state.save_to_json(\"outputs/final_model/trainer_state.json\")\n",
        "\n",
        "            # Indexer les donn√©es d'entra√Ænement dans le module Q&A optimis√©\n",
        "            if st.session_state.raw_train_data:\n",
        "                self.qa_module.fit(st.session_state.raw_train_data)\n",
        "                st.session_state.qa_module = self.qa_module\n",
        "\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.label_names = list(self.data_processor.label_mapping.keys())\n",
        "            st.session_state.test_ds = test_ds_processed\n",
        "            st.success(\"üéâ Entra√Ænement termin√© !\")\n",
        "            st.balloons()\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur : {e}\")\n",
        "            import traceback\n",
        "            st.error(f\"D√©tail: {traceback.format_exc()}\")\n",
        "            st.session_state.training = False\n",
        "\n",
        "    def run_advanced_qa_interface(self):\n",
        "        \"\"\"Interface Q&A avanc√©e fusionn√©e avec optimisations\"\"\"\n",
        "        st.header(\"‚ùì Interface Q&A Avanc√©e\")\n",
        "\n",
        "        # Configuration de recherche avec colonnes optimis√©es\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            question = st.text_input(\n",
        "                \"Posez votre question sur le climat :\",\n",
        "                placeholder=\"Ex: Quelles sont les causes du r√©chauffement climatique ?\"\n",
        "            )\n",
        "\n",
        "        with col2:\n",
        "            search_mode = st.selectbox(\n",
        "                \"Mode\",\n",
        "                [\"hybrid\", \"knowledge_only\", \"corpus_only\", \"keywords\"],\n",
        "                format_func=lambda x: {\n",
        "                    \"hybrid\": \"üîÄ Hybride\",\n",
        "                    \"knowledge_only\": \"üìö Base de connaissances\",\n",
        "                    \"corpus_only\": \"üìä Corpus d'entra√Ænement\",\n",
        "                    \"keywords\": \"üîç Mots-cl√©s\"\n",
        "                }[x]\n",
        "            )\n",
        "\n",
        "        # Param√®tres avanc√©s dans l'expandeur\n",
        "        with st.expander(\"‚öôÔ∏è Param√®tres de recherche\", expanded=False):\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                top_k = st.slider(\"Nombre de r√©sultats\", 1, 10, value=5)\n",
        "            with col2:\n",
        "                show_details = st.checkbox(\"Afficher les d√©tails\", True)\n",
        "\n",
        "        # Recherche principale avec spinner optimis√©\n",
        "        if question:\n",
        "            try:\n",
        "                with st.spinner(\"üîç Recherche intelligente en cours...\"):\n",
        "                    results = self.qa_module.query_with_fallback(question, top_k, search_mode)\n",
        "\n",
        "                self.display_qa_results(results, show_details, f\"R√©sultats pour: '{question}'\")\n",
        "\n",
        "                # Analyse des r√©sultats dans un expander\n",
        "                if results and len(results) > 1:\n",
        "                    with st.expander(\"üìä Analyse des r√©sultats\", expanded=False):\n",
        "                        self.visualizer.plot_qa_results_analysis(results, question)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå Erreur : {str(e)}\")\n",
        "                if st.checkbox(\"Afficher les d√©tails techniques\"):\n",
        "                    st.exception(e)\n",
        "\n",
        "        # Questions sugg√©r√©es avec boutons\n",
        "        st.markdown(\"### üí° Questions sugg√©r√©es\")\n",
        "        suggested_questions = [\n",
        "            \"Quelles sont les principales causes du r√©chauffement climatique ?\",\n",
        "            \"Comment les √©nergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la d√©forestation sur le climat ?\",\n",
        "            \"Comment r√©duire les √©missions de gaz √† effet de serre ?\"\n",
        "        ]\n",
        "\n",
        "        cols = st.columns(2)\n",
        "        for i, suggestion in enumerate(suggested_questions[:4]):\n",
        "            if cols[i % 2].button(\n",
        "                suggestion[:50] + \"...\" if len(suggestion) > 50 else suggestion,\n",
        "                key=f\"suggestion_{i}\",\n",
        "                use_container_width=True\n",
        "            ):\n",
        "                st.session_state[\"question\"] = suggestion\n",
        "                st.rerun()\n",
        "\n",
        "    def display_qa_results(self, results: list, show_details: bool, title: str):\n",
        "        \"\"\"Affichage optimis√© des r√©sultats Q&A\"\"\"\n",
        "        if not results:\n",
        "            st.info(\"üîç Aucun r√©sultat trouv√©.\")\n",
        "            return\n",
        "\n",
        "        st.markdown(f\"### {title}\")\n",
        "        st.caption(f\"{len(results)} r√©sultat(s) trouv√©(s)\")\n",
        "\n",
        "        for result in results:\n",
        "            # Emoji selon la source\n",
        "            emoji = {\n",
        "                \"knowledge_base\": \"üìö\",\n",
        "                \"training_corpus\": \"üìä\",\n",
        "                \"keywords\": \"üîç\"\n",
        "            }.get(result.get(\"source\"), \"üìÑ\")\n",
        "\n",
        "            # Couleur selon le score\n",
        "            score = result.get(\"score\", 0)\n",
        "            score_color = \"üü¢\" if score > 0.7 else \"üü°\" if score > 0.4 else \"üî¥\"\n",
        "\n",
        "            with st.expander(\n",
        "                f\"{emoji} Score: {score_color} {score:.3f} - {result.get('source', 'source').replace('_', ' ').title()}\",\n",
        "                expanded=(result.get(\"rank\", 0) == 1)\n",
        "            ):\n",
        "                st.write(\"**Texte:**\")\n",
        "                st.write(result[\"text\"])\n",
        "\n",
        "                if show_details:\n",
        "                    st.divider()\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        if \"category\" in result:\n",
        "                            st.caption(f\"**Cat√©gorie:** `{result['category']}`\")\n",
        "                        if \"keywords\" in result and result[\"keywords\"]:\n",
        "                            st.caption(\"**Mots-cl√©s:** \" + \" \".join([f\"`{kw}`\" for kw in result[\"keywords\"][:3]]))\n",
        "\n",
        "                    with col2:\n",
        "                        st.caption(f\"**Rang:** {result.get('rank', '?')}\")\n",
        "\n",
        "    def run_knowledge_management(self):\n",
        "        \"\"\"Interface de gestion optimis√©e de la base de connaissances\"\"\"\n",
        "        st.header(\"üìö Gestion des Connaissances\")\n",
        "\n",
        "        tab1, tab2, tab3 = st.tabs([\"üìñ Consulter\", \"‚ûï Ajouter\", \"üìä Statistiques\"])\n",
        "\n",
        "        with tab1:\n",
        "            self._render_knowledge_browser()\n",
        "\n",
        "        with tab2:\n",
        "            self._render_knowledge_adder()\n",
        "\n",
        "        with tab3:\n",
        "            self._render_knowledge_stats()\n",
        "\n",
        "    def _render_knowledge_browser(self):\n",
        "        \"\"\"Sous-composant pour naviguer dans la base de connaissances\"\"\"\n",
        "        st.subheader(\"üìñ Consultation\")\n",
        "\n",
        "        # Filtres avec colonnes\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            categories = self.qa_module.get_categories()\n",
        "            filter_category = st.selectbox(\"Filtrer par\", [\"Toutes\"] + categories)\n",
        "        with col2:\n",
        "            search_text = st.text_input(\"Rechercher\", placeholder=\"Mots-cl√©s...\")\n",
        "\n",
        "        # Filtrage et affichage\n",
        "        knowledge_items = []\n",
        "        for idx, item in enumerate(self.qa_module.knowledge_base):\n",
        "            if filter_category == \"Toutes\" or item[\"category\"] == filter_category:\n",
        "                if not search_text or search_text.lower() in item[\"text\"].lower():\n",
        "                    knowledge_items.append((idx, item))\n",
        "\n",
        "        st.info(f\"üìÑ {len(knowledge_items)} document(s) trouv√©(s)\")\n",
        "\n",
        "        # Affichage pagin√© pour performance\n",
        "        items_per_page = 5\n",
        "        page = st.number_input(\"Page\", min_value=1, max_value=max(1, len(knowledge_items)//items_per_page + 1), value=1)\n",
        "\n",
        "        start_idx = (page - 1) * items_per_page\n",
        "        end_idx = min(start_idx + items_per_page, len(knowledge_items))\n",
        "\n",
        "        for idx, (orig_idx, item) in enumerate(knowledge_items[start_idx:end_idx], start=1):\n",
        "            with st.expander(f\"üìÑ {item['category'].upper()} - {item['text'][:80]}...\"):\n",
        "                st.write(item[\"text\"])\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.caption(f\"ID: `{orig_idx}`\")\n",
        "                with col2:\n",
        "                    if item[\"keywords\"]:\n",
        "                        st.caption(\"Keywords: \" + \", \".join(item[\"keywords\"][:3]))\n",
        "\n",
        "    def _render_knowledge_adder(self):\n",
        "        \"\"\"Sous-composant pour ajouter des connaissances\"\"\"\n",
        "        st.subheader(\"‚ûï Ajouter une connaissance\")\n",
        "\n",
        "        with st.form(\"add_knowledge_form\"):\n",
        "            new_text = st.text_area(\n",
        "                \"Texte\",\n",
        "                placeholder=\"Entrez le texte de la nouvelle connaissance...\",\n",
        "                height=100\n",
        "            )\n",
        "\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                categories = self.qa_module.get_categories()\n",
        "                category_choice = st.selectbox(\"Cat√©gorie\", [\"Nouvelle\"] + categories)\n",
        "\n",
        "                if category_choice == \"Nouvelle\":\n",
        "                    new_category = st.text_input(\"Nouvelle cat√©gorie\", placeholder=\"technologies\")\n",
        "                    final_category = new_category\n",
        "                else:\n",
        "                    final_category = category_choice\n",
        "\n",
        "            with col2:\n",
        "                keywords = st.text_input(\n",
        "                    \"Mots-cl√©s\",\n",
        "                    placeholder=\"tech, innovation, futur\"\n",
        "                )\n",
        "\n",
        "            submitted = st.form_submit_button(\"‚úÖ Ajouter\", use_container_width=True)\n",
        "\n",
        "            if submitted and new_text and final_category:\n",
        "                keyword_list = [kw.strip() for kw in keywords.split(\",\") if kw.strip()]\n",
        "\n",
        "                if self.qa_module.add_knowledge(new_text, final_category, keyword_list):\n",
        "                    st.success(\"‚úÖ Ajout√© avec succ√®s!\")\n",
        "                    st.rerun()\n",
        "                else:\n",
        "                    st.error(\"‚ùå Erreur lors de l'ajout\")\n",
        "\n",
        "    def _render_knowledge_stats(self):\n",
        "        \"\"\"Sous-composant pour les statistiques\"\"\"\n",
        "        st.subheader(\"üìä Statistiques\")\n",
        "        stats = self.qa_module.get_stats()\n",
        "\n",
        "        # M√©triques principales\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Total\", stats[\"total_documents\"])\n",
        "        col2.metric(\"KB\", stats[\"knowledge_base_size\"])\n",
        "        col3.metric(\"Corpus\", stats[\"training_corpus_size\"])\n",
        "        col4.metric(\"Cat√©gories\", len(stats[\"categories\"]))\n",
        "\n",
        "        # Graphiques si des donn√©es existent\n",
        "        if self.qa_module.knowledge_base:\n",
        "            # Distribution par cat√©gorie\n",
        "            category_counts = {}\n",
        "            for item in self.qa_module.knowledge_base:\n",
        "                cat = item[\"category\"]\n",
        "                category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "            df_categories = pd.DataFrame([\n",
        "                {\"Cat√©gorie\": cat, \"Nombre\": count}\n",
        "                for cat, count in category_counts.items()\n",
        "            ])\n",
        "\n",
        "            st.bar_chart(df_categories.set_index(\"Cat√©gorie\"))\n",
        "\n",
        "    def run_rss_integration(self):\n",
        "        \"\"\"Section RSS dans l'interface Streamlit\"\"\"\n",
        "        st.header(\"üì∞ Mise √† jour RSS Automatique\")\n",
        "\n",
        "        col1, col2 = st.columns([3, 1])\n",
        "\n",
        "        with col1:\n",
        "            st.info(\"üì° Flux RSS configur√©s:\")\n",
        "            for url in [\n",
        "                \"Carbon Brief\",\n",
        "                \"NASA Climate\",\n",
        "                \"UNFCCC News\"\n",
        "            ]:\n",
        "                st.write(f\"‚Ä¢ {url}\")\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"üîÑ Mise √† jour manuelle\", type=\"primary\"):\n",
        "                with st.spinner(\"Mise √† jour en cours...\"):\n",
        "                    try:\n",
        "                        from update_kb_rss import manual_rss_update\n",
        "                        added = manual_rss_update(self.qa_module)\n",
        "                        if added > 0:\n",
        "                            st.success(f\"‚úÖ {added} articles ajout√©s\")\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.info(\"Aucun nouvel article trouv√©\")\n",
        "                    except ImportError:\n",
        "                        st.error(\"‚ùå Module update_kb_rss non trouv√©\")\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"‚ùå Erreur lors de la mise √† jour : {str(e)}\")\n",
        "\n",
        "        # Param√®tres avanc√©s\n",
        "        with st.expander(\"‚öôÔ∏è Param√®tres RSS\"):\n",
        "            st.write(\"Mise √† jour automatique activ√©e\")\n",
        "            st.write(\"Fr√©quence: Quotidienne √† 09:00\")\n",
        "            st.write(\"Sources: Carbon Brief, NASA Climate, UNFCCC\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        \"\"\"Interface des visualisations (inchang√©e mais avec optimisations)\"\"\"\n",
        "        st.header(\"üìà Visualisations\")\n",
        "\n",
        "        viz = st.selectbox(\n",
        "            \"Choisir une visualisation\",\n",
        "            [\"Distribution des classes\", \"Matrice de confusion\", \"Courbes d'entra√Ænement\",\n",
        "             \"üìä M√©triques BLEU/ROUGE\", \"üîç √âvaluation Q&A\", \"üìö Analyse Knowledge Base\"]\n",
        "        )\n",
        "\n",
        "        test_ds = st.session_state.get(\"test_ds\")\n",
        "        label_names = st.session_state.get(\"label_names\")\n",
        "\n",
        "        try:\n",
        "            if viz == \"Distribution des classes\" and test_ds:\n",
        "                self.visualizer.plot_class_distribution(test_ds[\"labels\"], label_names)\n",
        "            elif viz == \"Matrice de confusion\" and test_ds and st.session_state.trainer:\n",
        "                self.visualizer.show_confusion_matrix(st.session_state.trainer, test_ds, label_names)\n",
        "            elif viz == \"Courbes d'entra√Ænement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/final_model\")\n",
        "            elif viz == \"üìä M√©triques BLEU/ROUGE\":\n",
        "                self.run_bleu_rouge_analysis()\n",
        "            elif viz == \"üîç √âvaluation Q&A\":\n",
        "                self.run_qa_evaluation()\n",
        "            elif viz == \"üìö Analyse Knowledge Base\":\n",
        "                self.run_knowledge_base_analysis()\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur : {e}\")\n",
        "            if st.checkbox(\"D√©tails techniques\"):\n",
        "                st.exception(e)\n",
        "\n",
        "    def run_knowledge_base_analysis(self):\n",
        "        \"\"\"Analyse optimis√©e de la base de connaissances\"\"\"\n",
        "        st.subheader(\"üìö Analyse de la Base de Connaissances\")\n",
        "\n",
        "        if not self.qa_module.knowledge_base:\n",
        "            st.info(\"La base de connaissances est vide.\")\n",
        "            return\n",
        "\n",
        "        # Mots-cl√©s fr√©quents\n",
        "        all_keywords = [kw for item in self.qa_module.knowledge_base for kw in item[\"keywords\"]]\n",
        "\n",
        "        if all_keywords:\n",
        "            from collections import Counter\n",
        "            top_keywords = Counter(all_keywords).most_common(10)\n",
        "            df_keywords = pd.DataFrame(top_keywords, columns=[\"Mot-cl√©\", \"Fr√©quence\"])\n",
        "            st.bar_chart(df_keywords.set_index(\"Mot-cl√©\"))\n",
        "\n",
        "        # Longueurs de texte\n",
        "        lengths = [len(item[\"text\"]) for item in self.qa_module.knowledge_base]\n",
        "\n",
        "        if lengths:\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Min\", f\"{min(lengths)}\")\n",
        "            col2.metric(\"Moyenne\", f\"{np.mean(lengths):.0f}\")\n",
        "            col3.metric(\"Max\", f\"{max(lengths)}\")\n",
        "\n",
        "    def run_bleu_rouge_analysis(self):\n",
        "        \"\"\"Analyse BLEU/ROUGE (conserv√©e avec optimisations)\"\"\"\n",
        "        st.subheader(\"üìä Analyse des m√©triques BLEU et ROUGE\")\n",
        "\n",
        "        # Code inchang√© mais avec gestion d'erreurs am√©lior√©e\n",
        "        try:\n",
        "            # Section test personnalis√©\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                reference_text = st.text_area(\n",
        "                    \"Texte de r√©f√©rence:\",\n",
        "                    \"Le r√©chauffement climatique est un ph√©nom√®ne global caus√© par les activit√©s humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            with col2:\n",
        "                candidate_text = st.text_area(\n",
        "                    \"Texte candidat:\",\n",
        "                    \"Le changement climatique est un probl√®me mondial d√ª aux actions humaines.\",\n",
        "                    height=100\n",
        "                )\n",
        "\n",
        "            if st.button(\"Calculer les scores\", use_container_width=True):\n",
        "                bleu_score = self.visualizer.calculate_bleu_score(reference_text, candidate_text)\n",
        "                rouge_scores = self.visualizer.calculate_rouge_score(reference_text, candidate_text)\n",
        "\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                col1.metric(\"BLEU\", f\"{bleu_score:.4f}\")\n",
        "                col2.metric(\"ROUGE-1\", f\"{rouge_scores['rouge-1']:.4f}\")\n",
        "                col3.metric(\"ROUGE-L\", f\"{rouge_scores['rouge-l']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur dans l'analyse BLEU/ROUGE : {e}\")\n",
        "\n",
        "    def run_qa_evaluation(self):\n",
        "        \"\"\"√âvaluation Q&A avec optimisations\"\"\"\n",
        "        st.subheader(\"üîç √âvaluation du syst√®me Q&A\")\n",
        "\n",
        "        # Questions de test avec suggestions\n",
        "        default_questions = [\n",
        "            \"Quelles sont les principales causes du r√©chauffement climatique ?\",\n",
        "            \"Comment les √©nergies renouvelables peuvent-elles aider ?\",\n",
        "            \"Quel est l'impact de la d√©forestation sur le climat ?\"\n",
        "        ]\n",
        "\n",
        "        if st.checkbox(\"Utiliser questions par d√©faut\", value=True):\n",
        "            test_questions = default_questions\n",
        "        else:\n",
        "            test_questions = st.text_area(\n",
        "                \"Entrez vos questions (une par ligne):\",\n",
        "                height=100\n",
        "            ).split(\"\\n\")\n",
        "\n",
        "        if test_questions and st.button(\"üöÄ Lancer l'√©valuation\", type=\"primary\"):\n",
        "            with st.spinner(\"√âvaluation en cours...\"):\n",
        "                # G√©n√©ration des r√©f√©rences\n",
        "                reference_answers = []\n",
        "                for question in test_questions:\n",
        "                    kb_results = self.qa_module.query_knowledge_base(question, top_k=1)\n",
        "                    ref = kb_results[0]['text'] if kb_results else \"R√©f√©rence manquante\"\n",
        "                    reference_answers.append(ref)\n",
        "\n",
        "                self.visualizer.evaluate_qa_performance(\n",
        "                    self.qa_module,\n",
        "                    test_questions,\n",
        "                    reference_answers\n",
        "                )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = ClimateAnalyzerApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "RAHSrmUDJLRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f65f66-0c69-4d76-a0a6-428b1ee74243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface utilisateur Streamlit ‚Äì `streamlit_app.py`\n",
        "\n",
        "Cette application constitue l‚Äôinterface centrale du projet *Climate Sentiment Analyzer Pro*, permettant d‚Äôinteragir avec le moteur IA via une interface web optimis√©e et modulaire, en exploitant l‚Äôensemble des composants d√©velopp√©s (pr√©traitement, entra√Ænement, Q&A, connaissances, RSS, visualisations).\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L‚Äôobjectif est de **rendre accessible** √† tous les utilisateurs ‚Äì m√™me non techniques ‚Äì un syst√®me avanc√© de **classification et d‚Äôanalyse s√©mantique** des discours sur le climat, via une interface unifi√©e et interactive.\n",
        "\n",
        "La d√©marche repose sur une **architecture modulaire** :\n",
        "- Pipeline complet d'entra√Ænement de mod√®le NLP LoRA,\n",
        "- Interface Q&A s√©mantique avec moteur hybride,\n",
        "- Syst√®me de gestion de la base de connaissances,\n",
        "- Mises √† jour RSS en temps r√©el,\n",
        "- Tableau de bord de visualisation et d‚Äô√©valuation.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- Concevoir une interface fluide malgr√© la complexit√© algorithmique,\n",
        "- Int√©grer des modules h√©t√©rog√®nes (HuggingFace, FAISS, Langchain, RSS, visualisations) dans une seule app,\n",
        "- Assurer des **temps de r√©ponse rapides** m√™me sur des corpus volumineux,\n",
        "- Offrir √† l‚Äôutilisateur une **exp√©rience guid√©e**, p√©dagogique et personnalisable,\n",
        "- Permettre l‚Äôenrichissement continu du syst√®me sans re-d√©ploiement.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment | Justification |\n",
        "|---------|----------------|\n",
        "| `Streamlit` | Rapidit√© de prototypage, accessibilit√©, interaction instantan√©e |\n",
        "| Structure multi-onglets | Organisation claire pour chaque fonctionnalit√© (pipeline, Q&A, RSS‚Ä¶) |\n",
        "| `session_state` | Persistance des objets lourds (mod√®le, tokenizer, corpus) entre les vues |\n",
        "| Cache Streamlit + FAISS | Optimisation m√©moire et performance des recherches |\n",
        "| Int√©gration Q&A + BLEU/ROUGE | Mesure de la qualit√© et compr√©hension du syst√®me en temps r√©el |\n",
        "| Mises √† jour RSS int√©gr√©es | Ouverture vers des flux externes en production |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "L‚Äôapplication permet :\n",
        "- D‚Äôentra√Æner un mod√®le de classification de sentiment √† partir de **donn√©es personnalis√©es**,\n",
        "- D‚Äôinterroger en langage naturel la **base de connaissances enrichie** par les flux d‚Äôactualit√© et l‚Äôentra√Ænement supervis√©,\n",
        "- De visualiser la qualit√© des r√©sultats, les performances du mod√®le, la distribution des classes et des cat√©gories s√©mantiques.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce module donne **une dimension op√©rationnelle et interactive** au projet.  \n",
        "Il transforme les algorithmes d√©velopp√©s en **outil m√©tier activable**, facilitant l‚Äôexp√©rimentation, l‚Äôinterpr√©tation des r√©sultats et l‚Äôit√©ration continue.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- **Accessibilit√© no-code** pour les analystes, journalistes ou d√©cideurs,\n",
        "- Personnalisation et d√©ploiement rapide d‚Äôun outil d‚Äôanalyse de discours climatique,\n",
        "- Surveillance **en temps r√©el** des tendances via l‚Äôanalyse de corpus ou l'int√©gration RSS,\n",
        "- Renforcement de la **transparence et tra√ßabilit√©** des r√©sultats gr√¢ce aux visualisations et m√©triques int√©gr√©es,\n",
        "- Base id√©ale pour un **outil SaaS √©ducatif ou d√©cisionnel** autour des enjeux climatiques et des controverses s√©mantiques.\n"
      ],
      "metadata": {
        "id": "4FShVUIxqfe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. **Script d'Installation - setup_pipeline.py**"
      ],
      "metadata": {
        "id": "iwpyOye9JTab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\",\n",
        "        \"rouge-score\",\n",
        "        \"nltk\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        cmd = [sys.executable, \"-m\", \"pip\", \"install\", package]\n",
        "        try:\n",
        "            subprocess.check_call(cmd)\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "\n",
        "    # T√©l√©chargement des ressources NLTK\n",
        "    import nltk\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"‚úÖ Ressources NLTK pr√™tes\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ],
      "metadata": {
        "id": "dUJ_36HSJTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea889f0-d816-468b-ea64-2d9402d3f33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing setup_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script de Setup ‚Äî `setup_pipeline.py`\n",
        "\n",
        "Ce module automatise l'installation des **d√©pendances logicielles critiques** pour ex√©cuter le projet *Climate Sentiment Analyzer Pro* dans un environnement propre et reproductible.\n",
        "\n",
        "---\n",
        "\n",
        "### Intention et d√©marche\n",
        "\n",
        "L'objectif est de garantir √† l'utilisateur ou au d√©veloppeur une configuration compl√®te, stable et imm√©diatement exploitable pour faire fonctionner tous les modules du pipeline : NLP, IA, visualisation, interface Streamlit, etc.\n",
        "\n",
        "La d√©marche consiste √† :\n",
        "- Lister toutes les biblioth√®ques n√©cessaires avec leurs versions recommand√©es,\n",
        "- Automatiser leur installation via `pip`,\n",
        "- T√©l√©charger les ressources NLTK indispensables √† la g√©n√©ration et l'√©valuation de texte.\n",
        "\n",
        "---\n",
        "\n",
        "### Enjeux et challenges\n",
        "\n",
        "- **Compatibilit√© crois√©e** entre biblioth√®ques NLP modernes (transformers, PEFT, sentence-transformers, etc.),\n",
        "- **Stabilit√© des versions** pour √©viter les conflits lors de l'entra√Ænement ou du d√©ploiement,\n",
        "- **Simplicit√© de prise en main** m√™me pour des utilisateurs non techniques (ex. : √©tudiants, journalistes, etc.),\n",
        "- Pr√©parer le terrain pour un **environnement ex√©cutable √† distance (Cloud, Colab, Docker)**.\n",
        "\n",
        "---\n",
        "\n",
        "### Justification des choix\n",
        "\n",
        "| √âl√©ment technique         | Raison du choix |\n",
        "|--------------------------|-----------------|\n",
        "| `subprocess` + `pip`     | Compatible avec tous les OS, installation en ligne de commande Python |\n",
        "| Versions ‚â• sp√©cifiques   | S√©curit√© et compatibilit√© avec les API utilis√©es dans le projet |\n",
        "| `nltk.download('punkt')` | N√©cessaire pour la tokenisation et les scores ROUGE/BLEU |\n",
        "| `faiss-cpu`              | Permet la recherche vectorielle rapide m√™me sans GPU |\n",
        "| `streamlit`              | L‚Äôinterface utilisateur repose int√©gralement dessus |\n",
        "\n",
        "---\n",
        "\n",
        "### Interpr√©tation du r√©sultat\n",
        "\n",
        "Une fois le script ex√©cut√© :\n",
        "- Toutes les biblioth√®ques requises sont install√©es,\n",
        "- L‚Äôenvironnement Python est pr√™t √† ex√©cuter les notebooks, l‚Äôinterface Streamlit, l‚Äôentra√Ænement et les visualisations,\n",
        "- Plus besoin d‚Äôinstallation manuelle : gain de temps et fiabilit√© du d√©marrage.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ce script agit comme une **porte d‚Äôentr√©e technique unique**, qui garantit un d√©marrage sans erreur, reproductible, quel que soit l‚Äôenvironnement. Il est essentiel √† la diffusion, la collaboration ou le d√©ploiement du projet.\n",
        "\n",
        "---\n",
        "\n",
        "### B√©n√©fices m√©tier\n",
        "\n",
        "- **Exp√©rience simplifi√©e** pour les utilisateurs finaux ou partenaires projet,\n",
        "- **R√©duction des erreurs de configuration** en environnement collaboratif (hackathon, cloud, notebook partag√©),\n",
        "- **Gain de temps** pour la mise en production ou la d√©monstration,\n",
        "- Base compatible pour cr√©er un **setup.sh, requirements.txt, Dockerfile ou environnement cloud** cl√©-en-main.\n"
      ],
      "metadata": {
        "id": "zL5ZKNrOq1nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_pipeline.py"
      ],
      "metadata": {
        "id": "GM5RW4GHLxwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7ba942-cd7d-43ab-8a21-69c9d931fc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "‚úÖ transformers>=4.36.0 install√©\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.34.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "‚úÖ datasets>=2.16.0 install√©\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "‚úÖ torch>=2.1.0 install√©\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "‚úÖ peft>=0.7.0 install√©\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "‚úÖ sentence-transformers>=2.2.0 install√©\n",
            "Collecting faiss-cpu>=1.7.0\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n",
            "‚úÖ faiss-cpu>=1.7.0 install√©\n",
            "Collecting streamlit>=1.29.0\n",
            "  Downloading streamlit-1.47.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit>=1.29.0)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.29.0)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "Downloading streamlit-1.47.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.47.1 watchdog-6.0.0\n",
            "‚úÖ streamlit>=1.29.0 install√©\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "‚úÖ plotly>=5.17.0 install√©\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "‚úÖ scikit-learn>=1.3.0 install√©\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "‚úÖ matplotlib>=3.7.0 install√©\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "‚úÖ seaborn>=0.12.0 install√©\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "‚úÖ pandas>=1.5.0 install√©\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "‚úÖ numpy>=1.24.0 install√©\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=258fe989022c508ed2615c65ec1c03f553b9d1ca31057bf5b3f080efdb039833\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "‚úÖ rouge-score install√©\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "‚úÖ nltk install√©\n",
            "‚úÖ Ressources NLTK pr√™tes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "-MEnnZWsOuL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db23883f-a4c1-44cb-8f7c-f7863204d98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "Y3wskx_fZLEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f71fe0a-b8ad-4b06-9a69-e3f11c4a8e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K3WFMzkb6p9",
        "outputId": "a0b85b0e-5b7e-4707-aee7-8291e82d459b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. **D√©ploiement local d‚Äôune application Streamlit via ngrok**"
      ],
      "metadata": {
        "id": "-DI_t3NCrt-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Lancement Streamlit + ngrok (version corrig√©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre et cr√©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ],
      "metadata": {
        "id": "IFyKJzjISjWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de20c919-e1a3-484f-f5d6-3382c82e3990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://60ad9aba5de0.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D√©ploiement local d‚Äôune application Streamlit via ngrok\n",
        "\n",
        "#### Intention et d√©marche\n",
        "Ce script permet de **lancer une application Streamlit localement** et de la rendre accessible √† distance gr√¢ce √† un **tunnel s√©curis√© ngrok**. C‚Äôest une solution id√©ale pour tester, d√©montrer ou partager un prototype d‚Äôapplication web (comme ClimateAnalyzerPro) **sans configuration serveur complexe**.\n",
        "\n",
        "#### Enjeux et challenges\n",
        "- Streamlit, par d√©faut, n‚Äôest accessible que localement (`localhost:8501`).\n",
        "- En environnement cloud ou notebook distant (ex: Google Colab), il n‚Äôest **pas possible d‚Äôexposer directement un port local**.\n",
        "- Il est donc n√©cessaire de cr√©er un **pont r√©seau externe** via un tunnel s√©curis√© (ici : **ngrok**).\n",
        "- Le challenge est de s‚Äôassurer que l‚Äôinterface est fonctionnelle, stable, et bien mapp√©e sur l‚ÄôURL g√©n√©r√©e.\n",
        "\n",
        "#### Justification des choix techniques\n",
        "- **`pyngrok`** permet de cr√©er automatiquement un tunnel HTTP sur le port 8501 utilis√© par Streamlit.\n",
        "- L‚Äôapplication Streamlit est lanc√©e en arri√®re-plan (`subprocess.Popen`) avec redirection des logs pour plus de clart√©.\n",
        "- Un **temps d‚Äôattente** est ins√©r√© (`time.sleep(5)`) pour garantir que l‚Äôinterface soit bien en ligne avant de cr√©er le lien public.\n",
        "\n",
        "#### Interpr√©tation du r√©sultat\n",
        "Une fois le script ex√©cut√© :\n",
        "- Ngrok authentifie l‚Äôutilisateur via son token personnel.\n",
        "- L‚Äôapplication Streamlit est lanc√©e √† l‚Äôadresse locale `0.0.0.0:8501`.\n",
        "- Le lien **public et accessible depuis n‚Äôimporte quel navigateur** est affich√©, par exemple : `https://xxxx.ngrok.io`.\n",
        "\n",
        "#### Conclusion\n",
        "Ce m√©canisme permet √† toute personne, m√™me hors r√©seau local, de visualiser l‚Äôapplication sans installation pr√©alable. C‚Äôest un **gain de temps majeur pour les d√©monstrations, les retours utilisateurs, ou les tests multi-device**.\n",
        "\n",
        "#### B√©n√©fices m√©tier\n",
        "- Permet de partager rapidement un prototype de dashboard IA sans infrastructure cloud.\n",
        "- Facilite les **revues d‚Äô√©quipe, tests clients ou pr√©sentations √† distance**.\n",
        "- Id√©al pour un usage en **hackathon, projet √©tudiant ou preuve de concept en entreprise**.\n",
        "\n"
      ],
      "metadata": {
        "id": "wqr5FcMdrjjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion globale du projet**\n",
        "Ce projet d√©montre avec succ√®s l‚Äôint√©gration compl√®te d‚Äôun pipeline IA de classification de sentiments sur les discussions climatiques, enrichi par un moteur Q&A s√©mantique et une base de connaissances dynamique, r√©guli√®rement mise √† jour via des flux RSS sp√©cialis√©s.\n",
        "\n",
        "Nous avons con√ßu une application Streamlit robuste, modulaire et p√©dagogique, capable d‚Äôanalyser, classifier, interroger et enrichir la connaissance climatique √† partir de donn√©es ouvertes et d‚Äôentr√©es utilisateurs.\n",
        "\n",
        "---\n",
        "\n",
        "**B√©n√©fices m√©tier**\n",
        "D√©cision strat√©gique inform√©e : en comprenant le ressenti public sur des sujets climatiques, les ONG, d√©cideurs et communicants peuvent ajuster leurs messages et priorit√©s.\n",
        "\n",
        "Outil p√©dagogique interactif : id√©al pour les enseignants, journalistes ou citoyens souhaitant explorer les grands enjeux climatiques de mani√®re interactive.\n",
        "\n",
        "Veille automatis√©e : gr√¢ce √† l'int√©gration RSS, l'application reste continuellement √† jour sans effort humain.\n",
        "\n",
        "Base de connaissances enrichissable : les experts peuvent compl√©ter ou corriger la base √† la vol√©e, sans red√©ployer le syst√®me.\n",
        "\n",
        "---\n",
        "\n",
        "**Et maintenant‚Ä¶**\n",
        "Et si ce moteur pouvait demain s‚Äôadapter √† d‚Äôautres causes soci√©tales ‚Äî sant√©, √©ducation, inclusion ‚Äî pour cartographier en temps r√©el les √©motions, les craintes et les espoirs de nos soci√©t√©s ?"
      ],
      "metadata": {
        "id": "sM5-OJPMsVhs"
      }
    }
  ]
}