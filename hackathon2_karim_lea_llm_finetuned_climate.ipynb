{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnNQNqXaS458"
      },
      "source": [
        "# **Fine-Tuned LLM for Sentiment Analysis and Contextual Responses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lby4w31eZ5u-"
      },
      "source": [
        "Dans un monde saturé de données, où les entreprises cherchent à automatiser leurs interactions tout en maintenant un haut niveau de personnalisation, l’Intelligence Artificielle générative ouvre des perspectives inédites. Mais comment créer un assistant conversationnel qui ne se contente pas de répondre, mais qui comprend vraiment ce que ressent l’utilisateur et s’adapte à son contexte ?\n",
        "\n",
        "C’est tout l’enjeu de notre projet : construire un assistant IA intelligent capable à la fois de détecter le ton émotionnel d’un message (positif, négatif, neutre) et de générer des réponses enrichies par un moteur de recherche contextuel, le tout avec des modèles légers et optimisés grâce à LoRA, une méthode de fine-tuning économe en ressources.\n",
        "\n",
        "Notre mission : rendre l’IA utile, pertinente et accessible, même avec des moyens limités. Ce projet démontre qu’on peut concilier efficacité, sobriété technologique et intelligence conversationnelle, pour répondre à des enjeux métiers bien réels : service client, RH, assistance juridique, e-commerce… les cas d’usage sont nombreux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ZVvmadrQ96"
      },
      "source": [
        "# 1.**Module Core - core_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpXUvNPOTPe-"
      },
      "source": [
        "Ce module core_modules.py constitue un composant fondamental de l’architecture du projet. Il centralise à la fois les paramètres de configuration du système et la structure de sortie des prédictions, permettant ainsi une meilleure organisation, une lisibilité accrue, et une évolutivité maîtrisée du code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-8Ke1NTPOW"
      },
      "source": [
        "### **Intention et démarche**\n",
        "\n",
        "Objectif du module :\n",
        "Structurer proprement les résultats via la classe PredictionResult.\n",
        "\n",
        "Externaliser la configuration dans une classe ClimateConfig pour éviter les variables magiques dispersées dans le code.\n",
        "\n",
        "Pourquoi c’est important :\n",
        "Lorsqu'on travaille sur un projet complexe avec fine-tuning de LLMs, classification, génération, et retrieval, il devient essentiel de standardiser les flux de données et de modulariser les paramètres.\n",
        "\n",
        "Cela permet aussi d’assurer une compatibilité fluide entre les modules d’évaluation, d’interface et de génération."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l6Yz_5prPWS",
        "outputId": "2805c27e-6818-44cb-c144-749c049535d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les résultats de prédiction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralisée\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHzrWB4Tjuw"
      },
      "source": [
        "`ClimateConfig`Une classe de configuration centralisée :\n",
        "\n",
        "Contient les hyperparamètres du modèle, du fine-tuning LoRA, et la détection automatique de device (GPU/CPU).\n",
        "\n",
        "Grâce à la méthode to_dict(), cette configuration peut être journalisée, sauvegardée, ou utilisée dynamiquement dans d’autres modules.\n",
        "\n",
        "**Avantage** : on peut tester plusieurs variantes de configuration sans toucher au cœur du code. C’est essentiel pour l’expérimentation en machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnuHfgjHTkyM"
      },
      "source": [
        "**Enjeu technique et challenge :**\n",
        "Le challenge ici est de maintenir un code propre, modulaire et traçable, dans un projet mêlant fine-tuning PEFT (LoRA), embedding, retrieval, inférence générative, et interface utilisateur.\n",
        "\n",
        "Une mauvaise gestion des paramètres ou un manque de structuration des résultats rendraient le projet instable, peu réutilisable, et difficile à évaluer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jup6I0MKT0sJ"
      },
      "source": [
        "**Interprétation du résultat :**\n",
        "Ce module ne retourne pas un résultat au sens fonctionnel immédiat, mais il formalise et encapsule deux éléments essentiels :\n",
        "\n",
        "\n",
        "\n",
        "*   La standardisation de la sortie du modèle (PredictionResult)\n",
        "*   La gouvernance centralisée des paramètres (ClimateConfig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Cela permet de :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Faciliter l’orchestration du pipeline global\n",
        "*   Tracer les expérimentations\n",
        "*   Intégrer facilement des logs, métriques, et dashboards\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9yVcO8T5vB"
      },
      "source": [
        "**Conclusion :** Ce module pose les bases de la robustesse du projet IA. Il reflète une bonne pratique d’industrialisation des projets LLM : abstraction des paramètres, traçabilité, et standardisation des résultats. Dans un contexte professionnel, cette structuration permet :\n",
        "\n",
        "\n",
        "*   Une collaboration fluide entre data scientists et développeurs front-end\n",
        "*   Une réutilisabilité du modèle dans d'autres projets\n",
        "*   Une mise en production simplifiée\n",
        "\n",
        "\n",
        "En résumé, ce fichier est invisible pour l’utilisateur final, mais essentiel à la stabilité, l’évolution, et la fiabilité du système."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_bu1PGeUEYq"
      },
      "source": [
        "# 2. **Module Data Processing - data_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWFjD4keP89"
      },
      "source": [
        "Ce module data_modules.py est le pôle central de prétraitement des données dans le pipeline du projet de fine-tuning LLM avec LoRA et génération contextuelle. Il a été conçu avec une logique résiliente, automatisée et hautement réutilisable, indispensable pour manipuler des datasets hétérogènes dans un contexte de hackathon ou d’expérimentation rapide.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlfrNA8_eZrW"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyki8ArTeWD1"
      },
      "source": [
        "\n",
        "**Objectifs principaux :**\n",
        "\n",
        "*   Détecter automatiquement les colonnes texte et label dans n’importe quel DataFrame.\n",
        "*   Nettoyer et préparer les données de manière robuste, quels que soient leur format ou leur qualité initiale.\n",
        "*   Générer un triplet train/val/test proprement structuré, stratifié si possible.\n",
        "*   Produire un format compatible avec les modèles Hugging Face (datasets.Dataset).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6w7f53esmi"
      },
      "source": [
        "**Pourquoi cette approche ?**\n",
        "Parce que dans un hackathon ou une expérimentation IA rapide :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Les données ne sont pas toujours bien structurées.\n",
        "*   Le temps est limité pour ajuster manuellement les colonnes ou le nettoyage.\n",
        "*   Une flexibilité et automatisation maximale est nécessaire.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtjC1wkcrbil",
        "outputId": "4aaab23a-c67f-4882-b0c3-80418bad7fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralisée du traitement des données avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"Détection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\"🔍 Détection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\"📋 Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-clés\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # Vérifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caractères\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: première colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouvé, prendre la dernière colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\"✅ Colonnes détectées: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs problématiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les chaînes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Préparation des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\"📊 Préparation des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # Détection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\"❌ Impossible de détecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes nécessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"❌ Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\"📋 Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des données\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"🧹 Après suppression des NaN: {len(df_work)} lignes (supprimé: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de manière sécurisée)\n",
        "        try:\n",
        "            # Vérifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\"📝 Après filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # Vérification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\"❌ Aucune donnée valide après nettoyage!\")\n",
        "\n",
        "        # 5. Échantillonnage si nécessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\"🎯 Échantillonnage à {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\"🏷️ Labels uniques trouvés: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # Vérification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\"⚠️ Problème de mapping des labels détecté\")\n",
        "            print(f\"Labels non mappés: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\"📊 Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifiés\n",
        "        try:\n",
        "            # Vérifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\"✅ Stratification activée\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\"⚠️ Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # Deuxième split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\"📊 Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\"✅ Datasets créés avec succès\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur lors de la création des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de données\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\"❌ DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\"❌ DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\"✅ DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_o8Cgbvfjua"
      },
      "source": [
        "**Enjeux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Fiabilité sur données inconnues : il faut que le module fonctionne sur n'importe quel jeu de données texte+label.\n",
        "*   Robustesse aux erreurs utilisateurs : fichiers mal formatés, labels textuels, colonnes manquantes ou bruitées.\n",
        "*   Préparation optimisée pour le fine-tuning LLMs : nettoyage, encodage des labels, et split cohérent sont critiques pour éviter du surapprentissage ou des échecs d'entraînement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "\n",
        "*   Détecter automatiquement les bonnes colonnes sans\n",
        "heuristique trop rigide.\n",
        "*   Nettoyer du texte avec des formats incohérents, des valeurs manquantes, des types mixtes.\n",
        "*   Gérer des jeux de données déséquilibrés ou avec trop peu d'exemples par classe.\n",
        "*   Assurer une compatibilité fluide avec la bibliothèque datasets de Hugging Face.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m1vP8V4fngl"
      },
      "source": [
        "Ce module produit trois objets de type Dataset : train_dataset, val_dataset, test_dataset – directement utilisables avec les tokenizers et les Trainer de Hugging Face.\n",
        "\n",
        "Il génère aussi :\n",
        "\n",
        "Un mapping label → id compatible avec les modèles de classification.\n",
        "\n",
        "Des messages de log détaillés pour comprendre chaque étape et corriger si besoin.\n",
        "\n",
        "Un fallback automatique si certaines opérations échouent (e.g., split sans stratification).\n",
        "\n",
        "Grâce à cette logique :\n",
        "\n",
        "Le modèle ne se base jamais sur des données corrompues ou mal étiquetées.\n",
        "\n",
        "L’entraînement est plus stable, reproductible et transparent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1taFg01fvpa"
      },
      "source": [
        "**Conclusion :**\n",
        "Ce module est un véritable pont entre le monde brut des données réelles et les exigences rigoureuses du machine learning moderne. Il incarne une bonne pratique fondamentale en IA : rendre le traitement des données automatique, sécurisé et traçable.\n",
        "\n",
        "**Bénéfices métiers :**\n",
        "Permet à une équipe data ou produit de tester rapidement plusieurs sources de données sans ajustement manuel.\n",
        "\n",
        "Réduit considérablement le risque d'erreur humaine lors de l’exploration et la préparation des données.\n",
        "\n",
        "Accélère la mise en production ou le prototypage d’outils IA centrés sur le langage naturel.\n",
        "\n",
        "En somme, data_modules.py est un accélérateur de projet IA, une brique essentielle pour garantir que \"garbage in ≠ garbage out\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45WnDHBEURFf"
      },
      "source": [
        "# 3. **Module Modèle - model_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vCHuPWg1CT"
      },
      "source": [
        "Le fichier model_modules.py représente la brique stratégique du projet IA, dédiée à la configuration, l’adaptation et l’entraînement du modèle de classification via une approche parameter-efficient (fine-tuning avec LoRA). Il encapsule l'ensemble des décisions techniques critiques liées au modèle, au tokenizer, aux paramètres d'entraînement, et à la logique d'évaluation, dans une classe modulaire, claire et facilement maintenable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6ZXCsv1hCRt"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V0DBnLmg-8s"
      },
      "source": [
        "\n",
        "**Objectifs du module ModelManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Centraliser la configuration du tokenizer et du modèle Hugging Face.\n",
        "*   Appliquer LoRA (Low-Rank Adaptation) pour une fine-tuning efficace sur GPU ou CPU.\n",
        "*   Paramétrer finement l’entraînement, en prenant en compte la gestion mémoire, les métriques pertinentes, et les stratégies de logging/saving.\n",
        "*   Instancier Trainer de Hugging Face de manière optimisée pour une expérience de fine-tuning fluide et reproductible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Pourquoi cette structuration ?**\n",
        "\n",
        "\n",
        "\n",
        "*   Favorise la réutilisabilité (nouveau modèle ou dataset = mêmes méthodes).\n",
        "*   Répond aux contraintes de performance (GPU limité, petit batch).\n",
        "*   Simplifie la traçabilité des expériences (logs + évaluation centralisée).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCUmVHjPrhv5",
        "outputId": "b5784870-963c-4fcc-a200-35fea3367882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py (VERSION ULTRA-STABLE - FP32 ONLY)\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# Désactiver les warnings\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Tokenizer sécurisé.\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token or '[PAD]'\n",
        "            print(f\"✅ Tokenizer chargé : {self.config.model_name}\")\n",
        "            return self.tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur tokenizer : {e}\")\n",
        "            raise\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Modèle en FP32 uniquement pour éviter l'erreur FP16.\"\"\"\n",
        "        if num_labels < 2:\n",
        "            raise ValueError(\"❌ num_labels doit être ≥ 2\")\n",
        "\n",
        "        try:\n",
        "            print(f\"🔧 Chargement modèle FP32 pour {num_labels} classes...\")\n",
        "\n",
        "            # 🔒 Forcer FP32\n",
        "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.config.model_name,\n",
        "                num_labels=num_labels,\n",
        "                torch_dtype=torch.float32,  # 🔒 FP32 uniquement\n",
        "                device_map=None,  # Pas de device_map pour éviter les conflits\n",
        "                problem_type=\"single_label_classification\",\n",
        "            )\n",
        "\n",
        "            target_modules = self.get_target_modules()\n",
        "\n",
        "            lora_config = LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=self.config.lora_r,\n",
        "                lora_alpha=self.config.lora_alpha,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=target_modules,\n",
        "                bias=\"none\",\n",
        "            )\n",
        "\n",
        "            self.peft_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            # Affichage des paramètres\n",
        "            trainable = sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.peft_model.parameters())\n",
        "            print(f\"📊 Paramètres : {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
        "\n",
        "            return self.peft_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur modèle : {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_target_modules(self):\n",
        "        \"\"\"Modules cibles LoRA.\"\"\"\n",
        "        name = self.config.model_name.lower()\n",
        "        if \"distilbert\" in name:\n",
        "            return [\"q_lin\", \"v_lin\"]\n",
        "        elif \"bert\" in name or \"roberta\" in name:\n",
        "            return [\"query\", \"value\"]\n",
        "        return [\"query\", \"value\", \"dense\"]\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation.\"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"outputs/runs\"):\n",
        "        \"\"\"TrainingArguments 100% FP32.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        logging_dir = os.path.join(output_dir, \"logs\")\n",
        "        os.makedirs(logging_dir, exist_ok=True)\n",
        "\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            logging_dir=logging_dir,\n",
        "\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "\n",
        "            # 🔒 Désactivation complète de la précision mixte\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_backend=None,\n",
        "            half_precision_backend=None,\n",
        "\n",
        "            gradient_checkpointing=True,\n",
        "            dataloader_num_workers=2,\n",
        "\n",
        "            save_total_limit=2,\n",
        "            save_steps=500,\n",
        "\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            push_to_hub=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Trainer sécurisé.\"\"\"\n",
        "        try:\n",
        "            training_args = self.setup_training_args()\n",
        "            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "\n",
        "            def compute_metrics(eval_pred):\n",
        "                predictions, labels = eval_pred\n",
        "                preds = np.argmax(predictions, axis=1)\n",
        "                return {\n",
        "                    \"accuracy\": accuracy_score(labels, preds),\n",
        "                    \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "                    \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                }\n",
        "\n",
        "            self.trainer = Trainer(\n",
        "                model=self.peft_model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                tokenizer=self.tokenizer,\n",
        "                compute_metrics=compute_metrics,\n",
        "                data_collator=data_collator,\n",
        "                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "            )\n",
        "\n",
        "            print(\"✅ Trainer configuré\")\n",
        "            return self.trainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erreur trainer : {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFmi29tHhjpH"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9i0oZiBhe7k"
      },
      "source": [
        "\n",
        "**Enjeux clés :**\n",
        "\n",
        "\n",
        "*   Adapter un LLM à une tâche de classification sans explosion mémoire (grâce à LoRA).\n",
        "*   Assurer la robustesse cross-model : prendre en charge DistilBERT, BERT, etc.\n",
        "*   Faciliter le scaling des expérimentations : simple changement de config = nouveau test reproductible.\n",
        "*   Optimiser la performance métier via des métriques équilibrées (f1, accuracy...).\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "*   Cibler correctement les couches internes à adapter avec LoRA (ex. q_lin, v_lin, ou query, value).\n",
        "*   Paramétrer TrainingArguments de façon équilibrée pour s’adapter à du low compute budget.\n",
        "*   Synchroniser tokenizer, dataset, et modèle, sans bugs de dimension ou padding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Jiy7o6h9v9"
      },
      "source": [
        "**Grâce à ce module :**\n",
        "\n",
        "Le modèle est LoRA-ready, avec gel des poids de base et focus sur l’adaptation via matrices faibles rangs.\n",
        "\n",
        "Le tokenizer est automatiquement configuré, avec pad_token géré pour éviter les erreurs silencieuses.\n",
        "\n",
        "Les métriques clés (accuracy, F1, precision, recall) sont calculées systématiquement.\n",
        "\n",
        "L’utilisateur peut entraîner sur GPU/CPU, avec logging optimisé pour analyse fine.\n",
        "\n",
        "Cette architecture modulaire permet de tester facilement plusieurs modèles (distilbert, roberta, albert) sans réécriture de code, et offre un point d'entrée unique pour tout le pipeline de classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGqSAlpHiJ2c"
      },
      "source": [
        "**Conclusion :**\n",
        "model_modules.py est une colonne vertébrale technique du projet. Il incarne l’exigence de :\n",
        "\n",
        "\n",
        "\n",
        "*   Performance (LoRA),\n",
        "*   Lisibilité (code modulaire),\n",
        "*   Robustesse (fallback CPU, configuration dynamique),\n",
        "*   Évaluation métier (métriques pertinentes dès l’entraînement).\n",
        "\n",
        "\n",
        "Il démontre comment une bonne ingénierie modèle permet à une équipe IA de construire des prototypes puissants et scalables, tout en gardant la possibilité de passer rapidement en production ou en démonstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LikcKVbdielx"
      },
      "source": [
        "**Bénéfice métier :**\n",
        "Ce module simplifie le travail des équipes data et produit en réduisant drastiquement le coût de fine-tuning tout en maintenant des performances élevées. Il est donc un levier stratégique pour démocratiser l’adaptation de LLMs sur des cas spécifiques (analyse de sentiment, classification thématique, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riAeZv2PUka_"
      },
      "source": [
        "# 4. **Visualization_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TDCzMFujkX7"
      },
      "source": [
        "Le module visualization_modules.py est une composante clé pour l’analyse, l’interprétation et la communication des résultats du projet de fine-tuning d’un LLM. Il centralise les outils de visualisation de la performance (courbes d’entraînement, précision, matrice de confusion, etc.) pour permettre à l’équipe data — mais aussi produit ou métier — de comprendre comment le modèle apprend et où il se trompe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxc1VLWjuua"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuDE7Sq7jrvz"
      },
      "source": [
        "\n",
        "**Objectifs du VisualizationManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre les courbes d’entraînement lisibles et interactives via Streamlit.\n",
        "*   Offrir des analyses post-training, comme la matrice de confusion et le rapport de classification.\n",
        "*   Automatiser le suivi des performances, sans besoin de coder chaque fois les visualisations.\n",
        "*   \n",
        "Faciliter la compréhension métier des résultats via des graphiques clairs et structurés.\n",
        "\n",
        "\n",
        "**Pourquoi ce module est crucial ?**\n",
        "\n",
        "Dans un projet IA, le succès ne se mesure pas uniquement par des chiffres. Les visualisations :\n",
        "*   révèlent les biais d'apprentissage (ex. overfitting),\n",
        "*   permettent de communiquer efficacement entre profils techniques et non techniques,\n",
        "*   facilitent la prise de décision (changement de modèle,ajustement hyperparamètres...).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMZt-WwIMwez",
        "outputId": "20ad39c0-e925-424e-d407-12a365a1919b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestion des visualisations d'entraînement et d'évaluation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str):\n",
        "        \"\"\"Affiche les courbes d'entraînement (garantit logs)\"\"\"\n",
        "        try:\n",
        "            import json\n",
        "            log_file = f\"{log_dir}/trainer_state.json\"\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"📄 Fichier de logs non trouvé\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"📉 Aucune donnée d'entraînement trouvée\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_accuracy = [], [], [], []\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
        "                elif 'loss' in entry:\n",
        "                    train_loss.append(entry.get('loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "            fig.suptitle('📈 Évolution de l\\'entraînement', fontsize=16)\n",
        "\n",
        "            # Loss\n",
        "            if train_loss:\n",
        "                axes[0].plot(range(len(train_loss)), train_loss, 'b-', label='Train Loss', marker='o')\n",
        "            if eval_loss:\n",
        "                axes[0].plot(epochs[:len(eval_loss)], eval_loss, 'r-', label='Eval Loss', marker='s')\n",
        "            axes[0].set_title('Perte (Loss)')\n",
        "            axes[0].set_xlabel('Epoch')\n",
        "            axes[0].set_ylabel('Loss')\n",
        "            axes[0].legend()\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Accuracy\n",
        "            if eval_accuracy:\n",
        "                axes[1].plot(epochs[:len(eval_accuracy)], eval_accuracy, 'g-', label='Accuracy', marker='^')\n",
        "            axes[1].set_title('Précision')\n",
        "            axes[1].set_xlabel('Epoch')\n",
        "            axes[1].set_ylabel('Accuracy')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names):\n",
        "        \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "        try:\n",
        "            from sklearn.metrics import confusion_matrix, classification_report\n",
        "            preds_output = trainer.predict(test_dataset)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(labels, preds)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=label_names, yticklabels=label_names, ax=ax)\n",
        "            ax.set_title('Matrice de Confusion')\n",
        "            ax.set_xlabel('Prédictions')\n",
        "            ax.set_ylabel('Vraies étiquettes')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Rapport de classification\n",
        "            st.subheader(\"📊 Rapport de Classification\")\n",
        "            report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.dataframe(report_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur matrice : {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names=None, title=\"Distribution des classes\"):\n",
        "        \"\"\"Histogramme des classes\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.countplot(x=labels, ax=ax)\n",
        "        if label_names:\n",
        "            ax.set_xticklabels(label_names)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(\"Classes\")\n",
        "        ax.set_ylabel(\"Nombre d'exemples\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_f1_per_class(labels_true, labels_pred, label_names):\n",
        "        \"\"\"Barplot F1-score par classe\"\"\"\n",
        "        scores = f1_score(labels_true, labels_pred, average=None)\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.barplot(x=label_names, y=scores, ax=ax)\n",
        "        ax.set_title(\"F1-score par classe\")\n",
        "        ax.set_ylabel(\"F1-score\")\n",
        "        st.pyplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ycovcgRkfPG"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SufjM84RkdQT"
      },
      "source": [
        "\n",
        "**Enjeux principaux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre accessibles les logs de Hugging Face à travers une UI conviviale.\n",
        "*   Permettre une analyse rapide et fiable des prédictions du modèle sur le jeu de test.\n",
        "*   Créer des visualisations robustes qui ne plantent pas si un fichier est absent ou mal formé.\n",
        "\n",
        "\n",
        "**Difficultés techniques :**\n",
        "\n",
        "\n",
        "*   Lecture et structuration des logs (trainer_state.json) : il faut interpréter les étapes d'entraînement parfois désy\n",
        "\n",
        "\n",
        "*   Synchronisation des métriques (loss, accuracy, learning\n",
        "rate) sur plusieurs epochs.\n",
        "\n",
        "*   Adaptation de matplotlib et seaborn à l’environnement Streamlit.\n",
        "*   Affichage conditionnel (fallbacks, gestion d’erreurs utilisateur ou absence de logs).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTMpu1rlJMR"
      },
      "source": [
        "**Grâce à ce module :**\n",
        "\n",
        "Les courbes de loss et d’accuracy montrent si le modèle converge correctement ou pas.\n",
        "\n",
        "Le learning rate peut être analysé pour détecter un mauvais taux d’apprentissage.\n",
        "\n",
        "La matrice de confusion permet d’identifier les classes les plus confondues.\n",
        "\n",
        "Le rapport de classification offre une vue granulée sur la précision, le rappel et le F1-score de chaque classe.\n",
        "\n",
        "Ces sorties visuelles sont essentielles pour diagnostiquer les faiblesses du modèle (ex. biais vers la classe majoritaire, difficulté à détecter certaines émotions/sentiments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QsD2dF2lTSW"
      },
      "source": [
        "**Conclusion :**\n",
        "[visualization_modules.py] agit comme un miroir du comportement du modèle. C’est un pont entre l’apprentissage automatique et la compréhension humaine.\n",
        "\n",
        "Il permet :\n",
        "\n",
        "\n",
        "*   un monitoring transparent de l'entraînement,\n",
        "*   une analyse fine des erreurs,\n",
        "*   et une prise de décision éclairée sur les prochaines étapes (plus de données, changement de modèle, etc.).\n",
        "\n",
        "\n",
        "\n",
        "En rendant les résultats lisibles et interactifs, ce module transforme un projet IA technique en outil intelligible et valorisable, aussi bien pour des data scientists que pour des décideurs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1n1zZTClo0P"
      },
      "source": [
        "**Intérêt métier :**\n",
        "Ce module permet aux parties prenantes non techniques (produit, marketing, direction) de comprendre la valeur et les limites du modèle, favorisant ainsi l’adoption, la confiance et les décisions stratégiques basées sur la donnée.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1N0-XlXUKD"
      },
      "source": [
        "# 5. **qa_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COTLVwUdm7ga"
      },
      "source": [
        "Le module qa_modules.py constitue le pilier de la composante de recherche et de génération de réponses contextuelles du projet. Il met en œuvre une recherche sémantique intelligente, permettant de récupérer les documents les plus pertinents à une requête utilisateur. C’est une brique essentielle pour assurer que l’agent IA ne réponde pas de manière générique, mais bien en s’appuyant sur le contexte pertinent extrait des données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCZRxJpSCQ2u"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3XsARR6CNVK"
      },
      "source": [
        "\n",
        "**L'objectif de ce module est double :**\n",
        "\n",
        "- Indexer un corpus de textes avec un modèle d’embeddings (SentenceTransformer) de manière à pouvoir retrouver rapidement les passages les plus proches sémantiquement d’une question ou d’un texte.\n",
        "\n",
        "- Fournir une interface simple pour interroger ce corpus, récupérer les résultats les plus pertinents, et les utiliser pour alimenter des prompts dans une étape de génération (RAG – Retrieval-Augmented Generation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HCljPBZCU5B"
      },
      "source": [
        "**Étapes clés de la démarche :**\n",
        "\n",
        "\n",
        "\n",
        "*   Chargement d’un modèle léger (all-MiniLM-L6-v2) pour encoder les textes.\n",
        "*   Indexation des documents via fit(), produisant un tableau d’embeddings.\n",
        "*   Interrogation via query(), comparant un embedding de la question aux documents via cosine_similarity.\n",
        "*   Extraction des meilleurs résultats (top_k) avec leurs labels et scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdRC_3s7Ncy6",
        "outputId": "2c10926e-2f9c-4bb9-a666-c499a18aacaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import streamlit as st\n",
        "\n",
        "class QAModule:\n",
        "    \"\"\"Module de recherche et Q&A basé sur sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"Initialisation avec modèle d'embedding\"\"\"\n",
        "        try:\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            self.corpus_embeddings = None\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "            self.model_name = model_name\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur chargement modèle Q&A: {e}\")\n",
        "            # Fallback\n",
        "            self.encoder = None\n",
        "            self.model_name = \"fallback\"\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Indexe le dataset pour la recherche\"\"\"\n",
        "        if self.encoder is None:\n",
        "            st.warning(\"Module Q&A non disponible\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.corpus_texts = [item['text'] for item in dataset]\n",
        "            self.labels = [item['label_id'] for item in dataset]\n",
        "\n",
        "            with st.spinner(\"📊 Indexation des données pour la recherche...\"):\n",
        "                self.corpus_embeddings = self.encoder.encode(\n",
        "                    self.corpus_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            st.success(f\"✅ {len(self.corpus_texts)} éléments indexés\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur indexation Q&A: {e}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Recherche les textes les plus similaires à la question\"\"\"\n",
        "        if self.encoder is None or self.corpus_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            question_embedding = self.encoder.encode([question], convert_to_tensor=False)\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Top K indices\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for idx in top_indices:\n",
        "                results.append({\n",
        "                    \"text\": self.corpus_texts[idx],\n",
        "                    \"label_id\": int(self.labels[idx]),\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"rank\": len(results) + 1\n",
        "                })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur recherche Q&A: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Statistiques du module Q&A\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"indexed_items\": len(self.corpus_texts),\n",
        "            \"embedding_dim\": len(self.corpus_embeddings[0]) if self.corpus_embeddings else 0\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqcriP0C-co"
      },
      "source": [
        "**Enjeux et Challenges**\n",
        "**Enjeux :**\n",
        "- Obtenir des résultats contextuels de qualité pour améliorer la génération (réduction des hallucinations).\n",
        "\n",
        "- Permettre un accès rapide à l’information à partir d’un corpus non structuré.\n",
        "\n",
        "- Intégrer un mécanisme simple mais robuste de RAG, sans passer par des solutions coûteuses ou complexes (ex : Pinecone).\n",
        "\n",
        "**Challenges :**\n",
        "- La qualité des embeddings : il faut un bon équilibre entre légèreté du modèle (pour la vitesse) et pertinence sémantique.\n",
        "\n",
        "- La gestion des grands corpus : encode() peut devenir lent ou gourmand si le dataset est volumineux.\n",
        "\n",
        "- La gestion des erreurs : si l’encodage échoue, tout le système de génération contextuelle est compromis.\n",
        "\n",
        "- Rendre le tout accessible via Streamlit sans alourdir l’UX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQoQCPSDRDA"
      },
      "source": [
        "**L'utilisation de ce module permet de :**\n",
        "\n",
        "- Identifier les documents les plus pertinents pour enrichir une requête utilisateur.\n",
        "\n",
        "- Générer une base contextuelle solide pour la génération de texte contrôlée et informée.\n",
        "\n",
        "- Alimenter des tableaux de bord ou des fonctions de réponse intelligente dans l’app (comme un chatbot enrichi ou un moteur d’aide).\n",
        "\n",
        "La méthode query() renvoie une liste classée de passages textuels, avec leur score de similarité, leur étiquette, et leur rang. Cette approche est bien plus efficace que de se baser uniquement sur des mots-clés ou des règles fixes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1MBtYWuDc9v"
      },
      "source": [
        "**Conclusion :**\n",
        "`qa_modules.py` incarne l’intelligence contextuelle du projet :\n",
        "il permet à un modèle de génération de ne pas inventer des faits, mais de s’appuyer sur des passages vérifiés, proches sémantiquement de la question.\n",
        "\n",
        "- Il est simple, modulaire et efficace :\n",
        "\n",
        "- Il ne dépend pas d’un back-end lourd.\n",
        "\n",
        "- Il est facilement extensible (autres modèles d’embeddings, base vectorielle plus avancée…).\n",
        "\n",
        "- Il augmente drastiquement la pertinence métier des réponses générées par le modèle LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGaqFer3EHQQ"
      },
      "source": [
        "**Intérêt métier :**\n",
        "Ce module est particulièrement utile dans des cas d’usage tels que :\n",
        "\n",
        "- FAQ automatisées intelligentes\n",
        "\n",
        "- hatbots d’assistance documentée\n",
        "\n",
        "- Recherche d’information rapide pour agents internes\n",
        "\n",
        "- Systèmes de RAG (Retrieval-Augmented Generation) en production\n",
        "\n",
        "En clair, ce module transforme un modèle de génération standard en assistant intelligent, documenté et adapté au contexte utilisateur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHtCva_kXd6d"
      },
      "source": [
        "# 6. **Module Knowledge Base - knowledge_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Emsl3kTEyJj"
      },
      "source": [
        "Le module `knowledge_modules.py` constitue une base de connaissances embarquée — une alternative légère aux méthodes classiques de retrieval par embeddings. Il est fondé sur des techniques simples de recherche textuelle par similarité lexicale, permettant d’enrichir les réponses générées par un modèle LLM avec des faits clés issus d’un corpus prédéfini.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVCI7IpE-f7"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cytQ25E7JE"
      },
      "source": [
        "\n",
        "**Ce module vise à :**\n",
        "\n",
        "- Fournir une base de connaissances interprétable, personnalisable et rapide à intégrer, sans dépendance à des bibliothèques externes lourdes.\n",
        "\n",
        "- Récupérer les faits les plus pertinents par rapport à une requête utilisateur, à l’aide d’un score de similarité Jaccard basé sur les mots communs.\n",
        "\n",
        "- Proposer un fallback utile dans des environnements à faibles ressources (CPU only) ou sans accès Internet.\n",
        "\n",
        "**Démarche :**\n",
        "- Initialisation manuelle d’une base de 10 faits scientifiques sur le climat (dans setup_knowledge_base()).\n",
        "\n",
        "- Analyse d’une requête utilisateur (via find_context()) par tokenisation simple + scoring Jaccard.\n",
        "\n",
        "- Tri des résultats et renvoi des top_k plus pertinents si leur score dépasse un seuil minimal.\n",
        "\n",
        "- Ajout dynamique possible de nouveaux faits (via add_knowledge()).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B03nOJeFI_US",
        "outputId": "161d8f92-8ee9-43ec-e909-8859513a9751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le réchauffement climatique est principalement causé par les émissions de gaz à effet de serre d'origine humaine.\",\n",
        "            \"Les énergies renouvelables comme le solaire et l'éolien sont essentielles pour décarboner notre économie.\",\n",
        "            \"La déforestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports représente environ 24% des émissions mondiales de gaz à effet de serre.\",\n",
        "            \"L'amélioration de l'efficacité énergétique des bâtiments peut réduire jusqu'à 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et régénératrice peut séquestrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les océans absorbent 25% du CO2 atmosphérique mais s'acidifient, menaçant les écosystèmes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises à réduire leurs émissions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'atténuation des émissions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralité carbone.\"\n",
        "        ]\n",
        "        print(\"✅ Base de connaissances initialisée avec recherche par mots-clés\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarité textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarité basé sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score décroissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"✅ Nouvelle connaissance ajoutée: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1erCbaJtFkoQ"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEsFefgFFhKa"
      },
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- Répondre rapidement à des questions fréquentes à partir d’un corpus contrôlé.\n",
        "\n",
        "- Permettre à un LLM de générer du texte documenté et précis, même sans pipeline de RAG complet.\n",
        "\n",
        "- Éviter les hallucinations en injectant des faits fiables dans les prompts.\n",
        "\n",
        "**Challenges :**\n",
        "- La limitation du matching lexical : sans embeddings, la pertinence sémantique est réduite.\n",
        "\n",
        "- L’approche ne gère pas la polysémie ou la synonymie, ce qui peut faire rater des résultats importants.\n",
        "\n",
        "- La base doit être structurée à la main, ce qui peut devenir contraignant à grande échelle.\n",
        "\n",
        "- Le seuil de pertinence (score > 0.1) est empirique et nécessite un ajustement selon les cas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoNYOARjF4M3"
      },
      "source": [
        "Ce module renvoie une liste triée des documents les plus proches d’une requête, ce qui peut être utilisé pour :\n",
        "\n",
        "- Construire un prompt enrichi dans un agent conversationnel.\n",
        "\n",
        "- Justifier une réponse automatique (avec la source en annexe).\n",
        "\n",
        "- Alimenter un système de recommandation ou un moteur de suggestion.\n",
        "\n",
        "Il fournit également des statistiques sur la base, comme le nombre total de faits et la longueur moyenne des documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwKlVPsGD6F"
      },
      "source": [
        "**Conclusion :**\n",
        "- `knowledge_modules.py` illustre un compromis ingénieux entre simplicité, interprétabilité et efficacité :\n",
        "\n",
        "- Il évite le recours à des modèles lourds (pas de FAISS, pas de vector store).\n",
        "\n",
        "- Il est parfaitement adapté pour des MVP, des environnements déconnectés, ou des prototypes embarqués.\n",
        "\n",
        "- Il offre une base extensible via l’ajout dynamique de connaissances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDyCNTKZGYo7"
      },
      "source": [
        "**Interprétation métier :**\n",
        "Pour un utilisateur métier (ex. : responsable RSE, chargé de mission climat), ce module permet :\n",
        "\n",
        "- De construire un référentiel de faits vérifiables, facilement enrichissable.\n",
        "\n",
        "- De garantir que les modèles génératifs s’appuient sur des informations validées, donc cohérentes avec la politique de l’organisation.\n",
        "\n",
        "- De servir de fallback contextuel si la recherche par embeddings est indisponible.\n",
        "\n",
        "En somme, ce module agit comme un filet de sécurité cognitif pour l’IA générative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELT7z04LX4d5"
      },
      "source": [
        "# 7. **Module Streamlit - streamlit_app.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9v1uCM0G-oq"
      },
      "source": [
        "Le fichier streamlit_app.py est l'orchestrateur final du projet Climate Sentiment Analyzer, une application interactive développée avec Streamlit qui permet de visualiser, entraîner, tester et interpréter un modèle de classification de sentiments appliqué à des textes sur le climat, tout en intégrant des modules de génération de contexte et de question/réponse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRoy3D5KHVBu"
      },
      "source": [
        "**Intention et Démarche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G1Q-RMjHSzi"
      },
      "source": [
        "\n",
        "L’objectif de ce module est d’intégrer de manière fluide tous les composants précédemment développés (prétraitement, entraînement, visualisation, prédiction, Q&A, etc.) dans une interface utilisateur accessible, sans compétences techniques requises.\n",
        "\n",
        "**Les étapes-clés de la démarche :**\n",
        "1. Structuration en classes :\n",
        "\n",
        "- `ClimateAnalyzerApp` : gère l’interface utilisateur, les interactions et les affichages.\n",
        "\n",
        "- `PipelineOrchestrator` : sert de point d’entrée principal du script.\n",
        "\n",
        "2. Navigation par onglets (via selectbox) :\n",
        "\n",
        "- Pipeline Complet\n",
        "\n",
        "- Traitement des données\n",
        "\n",
        "- Gestion du modèle\n",
        "\n",
        "- Analyse de texte\n",
        "\n",
        "- Q&A intelligent\n",
        "\n",
        "- Visualisations\n",
        "\n",
        "3. Personnalisation UX/UI :\n",
        "\n",
        "- CSS intégré pour améliorer l’apparence (gradient, cards, layout responsive).\n",
        "\n",
        "- Affichage dynamique des étapes, erreurs, barres de progression.\n",
        "\n",
        "4. Optimisation de l’expérience :\n",
        "\n",
        "- Upload de CSV pour lancer le pipeline.\n",
        "\n",
        "- Lancement à la demande de l’entraînement.\n",
        "\n",
        "- Visualisation des courbes d’apprentissage.\n",
        "\n",
        "- Interface de question-réponse par similarité.\n",
        "\n",
        "- Analyse fine de texte en temps réel avec contexte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAHSrmUDJLRG",
        "outputId": "343054cd-68fc-494b-b5b7-f818b16e5266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from knowledge_modules import KnowledgeBase\n",
        "from visualization_modules import VisualizationManager\n",
        "from qa_modules import QAModule\n",
        "\n",
        "st.set_page_config(page_title=\"🌍 Climate Analyzer – Complet\", page_icon=\"🌍\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>.main-header{background:linear-gradient(135deg,#667eea,#764ba2);padding:2rem;border-radius:15px;color:white;text-align:center}</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.qa_module = QAModule()\n",
        "\n",
        "    def run(self):\n",
        "        # 🔁 Persistance via session_state\n",
        "        for key in [\"trained\", \"trainer\", \"test_ds\", \"label_mapping\"]:\n",
        "            if key not in st.session_state:\n",
        "                st.session_state[key] = None if key != \"trained\" else False\n",
        "\n",
        "        st.markdown('<div class=\"main-header\"><h1>🌍 Climate Sentiment Analyzer</h1><h3>Pipeline Complet</h3></div>', unsafe_allow_html=True)\n",
        "        mode = st.sidebar.selectbox(\"Mode\", [\"🚀 Pipeline Complet\", \"📊 Data Processing\", \"❓ Q&A\", \"📈 Visualisations\"])\n",
        "\n",
        "        if mode == \"🚀 Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"📊 Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif mode == \"❓ Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif mode == \"📈 Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        st.header(\"🚀 Pipeline Complet\")\n",
        "        uploaded_file = st.file_uploader(\"Téléchargez votre fichier CSV\", type=[\"csv\"])\n",
        "\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"✅ Fichier chargé : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            sample_size = st.slider(\"Taille échantillon\", 1000, 10000, 4000)\n",
        "            epochs = st.slider(\"Epochs\", 1, 5, 3)\n",
        "            self.config.epochs = epochs\n",
        "\n",
        "            if st.button(\"🚀 Lancer l'entraînement\", type=\"primary\"):\n",
        "                self.run_real_training(df, sample_size)\n",
        "\n",
        "    def run_real_training(self, df, sample_size):\n",
        "        progress = st.progress(0)\n",
        "        status = st.empty()\n",
        "\n",
        "        try:\n",
        "            status.text(\"📊 Préparation des données...\")\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress.progress(20)\n",
        "\n",
        "            status.text(\"🤖 Configuration du modèle...\")\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            num_labels = len(self.data_processor.label_mapping)\n",
        "            self.model_manager.setup_model(num_labels)\n",
        "            progress.progress(40)\n",
        "\n",
        "            def prepare_dataset(ds):\n",
        "                ds = ds.map(\n",
        "                    self.model_manager.tokenize_function,\n",
        "                    batched=True,\n",
        "                    remove_columns=[\"text\"]\n",
        "                )\n",
        "                ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "                return ds\n",
        "\n",
        "            train_ds = prepare_dataset(train_ds)\n",
        "            val_ds = prepare_dataset(val_ds)\n",
        "            test_ds = prepare_dataset(test_ds)\n",
        "            progress.progress(60)\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds, val_ds)\n",
        "            progress.progress(70)\n",
        "\n",
        "            with st.spinner(\"Entraînement du modèle...\"):\n",
        "                trainer.train()\n",
        "            progress.progress(90)\n",
        "\n",
        "            metrics = trainer.evaluate(test_ds)\n",
        "\n",
        "            qa_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                       for item in self.data_processor.prepare_datasets(df, sample_size)[0]]\n",
        "            self.qa_module.fit(qa_data)\n",
        "\n",
        "            model_path = \"outputs/final_model\"\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "            trainer.save_model(model_path)\n",
        "\n",
        "            # 🔁 Mise à jour session_state\n",
        "            st.session_state.trained = True\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.test_ds = test_ds\n",
        "            st.session_state.label_mapping = self.data_processor.label_mapping\n",
        "\n",
        "            progress.progress(100)\n",
        "            st.success(\"🎉 Entraînement terminé avec succès!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur d'entraînement: {e}\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        st.header(\"📊 Data Processing\")\n",
        "        uploaded_file = st.file_uploader(\"Téléchargez votre fichier CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"✅ Fichier chargé: {df.shape}\")\n",
        "            st.dataframe(df.head())\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Train\", len(train_ds))\n",
        "            col2.metric(\"Validation\", len(val_ds))\n",
        "            col3.metric(\"Test\", len(test_ds))\n",
        "\n",
        "    def run_qa_interface(self):\n",
        "        st.header(\"❓ Interface Q&A\")\n",
        "        if not st.session_state.get(\"trained\", False):\n",
        "            st.warning(\"⚠️ Veuillez d'abord entraîner un modèle.\")\n",
        "            return\n",
        "\n",
        "        question = st.text_input(\"Votre question:\", placeholder=\"Ex: Quelles sont les causes du réchauffement climatique?\")\n",
        "        if question:\n",
        "            results = self.qa_module.query(question, top_k=5)\n",
        "            if results:\n",
        "                for i, result in enumerate(results, 1):\n",
        "                    with st.expander(f\"Résultat {i} - Score: {result['score']:.3f}\"):\n",
        "                        st.write(f\"**Texte:** {result['text']}\")\n",
        "                        st.write(f\"**Label ID:** {result['label_id']}\")\n",
        "            else:\n",
        "                st.warning(\"Aucun résultat trouvé.\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        st.header(\"📈 Visualisations\")\n",
        "        if not st.session_state.get(\"trained\", False):\n",
        "            st.warning(\"⚠️ Aucune donnée d'entraînement disponible.\")\n",
        "            return\n",
        "\n",
        "        viz_option = st.selectbox(\n",
        "            \"Choisir le type de visualisation:\",\n",
        "            [\"Matrice de confusion\", \"Rapport de classification\", \"Distribution des classes\", \"F1-score par classe\", \"Courbes d'entraînement\"]\n",
        "        )\n",
        "\n",
        "        trainer = st.session_state.trainer\n",
        "        test_ds = st.session_state.test_ds\n",
        "        label_mapping = st.session_state.label_mapping\n",
        "        label_names = list(label_mapping.keys())\n",
        "\n",
        "        try:\n",
        "            preds_output = trainer.predict(test_ds)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            if viz_option == \"Matrice de confusion\":\n",
        "                self.visualizer.show_confusion_matrix(trainer, test_ds, label_names)\n",
        "\n",
        "            elif viz_option == \"Rapport de classification\":\n",
        "                from sklearn.metrics import classification_report\n",
        "                report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
        "                st.dataframe(pd.DataFrame(report).transpose())\n",
        "\n",
        "            elif viz_option == \"Distribution des classes\":\n",
        "                self.visualizer.plot_class_distribution([label_mapping[i] for i in labels])\n",
        "\n",
        "            elif viz_option == \"F1-score par classe\":\n",
        "                self.visualizer.plot_f1_per_class(labels, preds, label_names)\n",
        "\n",
        "            elif viz_option == \"Courbes d'entraînement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/runs/logs\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Erreur de visualisation : {e}\")\n",
        "\n",
        "class PipelineOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        self.app.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIuOqTiBH9EQ"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvurmPkhH7Ko"
      },
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- Rendre le projet utilisable par tous : chercheurs, décideurs, étudiants...\n",
        "\n",
        "- Permettre un test rapide du modèle sur des données réelles.\n",
        "\n",
        "- Fournir une expérience fluide et complète du cycle ML : de la data à la prédiction.\n",
        "\n",
        "- Gérer robustement les erreurs, les exceptions et les chemins absents.\n",
        "\n",
        "**Challenges :**\n",
        "- Intégrer des composants hétérogènes (PyTorch, HuggingFace, PEFT, Streamlit, Pandas…).\n",
        "\n",
        "- Garder une structure lisible, modulaire et maintenable malgré la complexité croissante.\n",
        "\n",
        "- Assurer une compatibilité GPU/CPU sans casser le pipeline.\n",
        "\n",
        "- Offrir une expérience utilisateur intuitive, sans sacrifier la puissance technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSszDp0iIHcD"
      },
      "source": [
        "Ce module donne naissance à une web app de NLP complète, dans laquelle on peut :\n",
        "\n",
        "- Charger ses propres données climatiques textuelles.\n",
        "\n",
        "- Lancer un entraînement LoRA allégé.\n",
        "\n",
        "- Observer les courbes d’apprentissage (loss, accuracy, LR).\n",
        "\n",
        "- Obtenir des métriques (f1, precision, recall).\n",
        "\n",
        "- Interroger le modèle pour des prédictions individuelles.\n",
        "\n",
        "- Rechercher des éléments similaires grâce au module Q&A.\n",
        "\n",
        "- Obtenir un contexte sémantique enrichi via une base de connaissances intégrée.\n",
        "\n",
        "Le tout sans toucher à une seule ligne de code Python, ce qui en fait un véritable outil métier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_mTK6LIUCS"
      },
      "source": [
        "**Conclusion :**\n",
        "\n",
        "streamlit_app.py est l'aboutissement de tout le projet : unifier les briques IA, orchestrer les étapes, simplifier l’expérience utilisateur. C’est ce module qui transforme un ensemble de scripts en produit prêt à l’emploi, démontrant toute la puissance d’un prototype Low-Code + IA pour des enjeux climatiques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrc4-MnIbMG"
      },
      "source": [
        "**Interprétation Métiers**\n",
        "Pour un utilisateur métier (chercheur, communicant, analyste RSE) :\n",
        "\n",
        "- Cette interface devient un laboratoire numérique interactif : il peut expérimenter avec des jeux de données, tester ses hypothèses, comprendre la tonalité de certains discours.\n",
        "\n",
        "- Elle offre un moyen rapide de valider la perception des messages liés au climat.\n",
        "\n",
        "- Le module de Q&A permet une exploration sémantique fine, utile pour identifier les représentations collectives, tensions, ou idées dominantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2IYPH2YF6K"
      },
      "source": [
        "# 8. **Script d'Installation - setup_pipeline.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKJOPexFJTP2"
      },
      "source": [
        "Ce module est conçu pour automatiser l'installation de toutes les dépendances nécessaires au bon fonctionnement du projet d'analyse de sentiment climatique basé sur un pipeline IA complet. L'objectif est de garantir que tout utilisateur puisse configurer son environnement sans erreurs ni oublis, avec une commande unique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fysRkseqJbuk"
      },
      "source": [
        "**Démarche technique**\n",
        "Liste des dépendances critiques :\n",
        "- Le fichier contient une liste structurée des bibliothèques indispensables :\n",
        "\n",
        "- transformers, datasets, torch → pour le fine-tuning des LLMs.\n",
        "\n",
        "- peft, sentence-transformers, faiss-cpu → pour le LoRA et la recherche sémantique.\n",
        "\n",
        "- streamlit, plotly, matplotlib, seaborn → pour l’interface utilisateur et les visualisations.\n",
        "\n",
        "- scikit-learn, pandas, numpy → pour les métriques, le traitement de données et les structures fondamentales.\n",
        "\n",
        "Installation dynamique avec subprocess :\n",
        "\n",
        "- Le script utilise subprocess.check_call pour lancer des commandes pip install de façon indépendante, module par module.\n",
        "\n",
        "- Si une erreur survient, elle est capturée, signalée sans arrêter l’installation des autres paquets (try/except).\n",
        "\n",
        "Exécution autonome :\n",
        "\n",
        "- Le if __name__ == \"__main__\" permet de lancer l’installation avec une seule commande :\n",
        "\n",
        "`python setup_pipeline.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUJ_36HSJTQ1",
        "outputId": "45881458-58dd-4d9a-d301-51770925b7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup_pipeline.py\n",
        "# setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation complète des dépendances\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"✅ {package} installé\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"⚠️ Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"✅ Installation complète terminée!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUaKEEjmQEQb"
      },
      "source": [
        "**Enjeux et challenges**\n",
        "- Synchronisation des versions : Il est crucial de figer les versions compatibles pour éviter les conflits ou des comportements instables.\n",
        "\n",
        "- Robustesse multiplateforme : Utiliser subprocess rend le script plus portable que des solutions type requirements.txt, surtout en contexte programmatique ou en notebook.\n",
        "\n",
        "- Expérience utilisateur : Le module évite aux utilisateurs d’avoir à gérer manuellement l’installation, source d’erreurs fréquentes dans les projets IA complexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDuxTb0yQLBN"
      },
      "source": [
        "**Interprétation des résultats**\n",
        "- Chaque dépendance installée avec succès affiche un ✅.\n",
        "\n",
        "- En cas de souci (par exemple, dépendance manquante, connexion, conflit), le message est explicite avec ⚠️.\n",
        "\n",
        "- Une fois le processus terminé, un message de confirmation final s'affiche :\n",
        "✅ Installation complète terminée!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWQqNG7zQU-b"
      },
      "source": [
        "**Conclusion :**\n",
        "\n",
        "Ce module incarne une meilleure pratique de déploiement dans tout projet IA : il centralise, fiabilise et simplifie la mise en place de l’environnement technique. Grâce à ce script, toute personne ou équipe peut répliquer l'environnement de développement en un clic, favorisant la collaboration, la portabilité et la reproductibilité des résultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM5RW4GHLxwP",
        "outputId": "7210f0ff-38b7-4d0b-97ef-9d1c438a85a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.33.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "✅ transformers>=4.36.0 installé\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.33.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "✅ datasets>=2.16.0 installé\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "✅ torch>=2.1.0 installé\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.33.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "✅ peft>=0.7.0 installé\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.33.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "✅ sentence-transformers>=2.2.0 installé\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "✅ faiss-cpu>=1.7.0 installé\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "✅ streamlit>=1.29.0 installé\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "✅ plotly>=5.17.0 installé\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "✅ scikit-learn>=1.3.0 installé\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "✅ matplotlib>=3.7.0 installé\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "✅ seaborn>=0.12.0 installé\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "✅ pandas>=1.5.0 installé\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "✅ numpy>=1.24.0 installé\n",
            "✅ Installation complète terminée!\n"
          ]
        }
      ],
      "source": [
        "!python setup_pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "2f203569-4fc2-472e-8afe-27d486492fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "282cd127-5a68-43f8-e7d2-9a67e3ab1a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZxSn9_Q9kT"
      },
      "source": [
        "# **Streamlit + ngrok**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5BckmEJREOX"
      },
      "source": [
        "Ce script sert à démarrer une application Streamlit en local et à la rendre accessible via un lien public grâce à ngrok, un outil de tunneling très utile en développement collaboratif, démonstration ou test depuis un cloud (comme Colab, GCP ou un serveur distant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkbykT4rRM29"
      },
      "source": [
        "|  Avantage                        |  Explication                                                            |\n",
        "| --------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **Accessible depuis Internet**    | Vous pouvez tester ou faire une démo à distance, même depuis un notebook. |\n",
        "| **Simple et rapide**              | Pas besoin de configurer un serveur web ou de modifier les DNS.           |\n",
        "| **Compatible Colab**              | Fonctionne parfaitement depuis un environnement Google Colab ou serveur.  |\n",
        "| **Pas besoin d'ouvrir des ports** | Idéal en réseau d’entreprise ou cloud privé (ports souvent bloqués).      |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "cc93240d-20ba-41bc-8bc6-57ece074959a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "🚀 Interface Streamlit disponible à :\n",
            "NgrokTunnel: \"https://1961fef2d158.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# 🔧 Lancement Streamlit + ngrok (version corrigée)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1️⃣ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2️⃣ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3️⃣ Attendre et créer le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"🚀 Interface Streamlit disponible à :\")\n",
        "print(public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qoix6etRWpw"
      },
      "source": [
        "Cette démarche est indispensable pour déployer rapidement et temporairement une interface Streamlit sur le web, sans infrastructure complexe. Elle facilite les démos en direct, les tests collaboratifs ou les livraisons rapides de POC (Proof of Concept) IA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1DOx7hrSKCy"
      },
      "source": [
        "# **CONCLUSION GLOBALE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7T_H3XbSOIg"
      },
      "source": [
        "Ce projet propose un pipeline complet et modulaire pour l’analyse de sentiments climatiques à partir de textes. Il intègre plusieurs étapes clés : le prétraitement intelligent des données (avec détection automatique des colonnes), la modélisation optimisée avec LoRA pour fine-tuning allégé, la visualisation des performances, une interface de prédiction et de contexte, ainsi qu’un moteur de questions-réponses sémantiques. Chaque module a été pensé pour être robuste, réutilisable et facilement déployable grâce à Streamlit et ngrok. Les principaux défis rencontrés concernent la gestion des données hétérogènes, la configuration efficace du modèle, et l'interprétabilité des résultats. Grâce à des choix techniques adaptés (PEFT, Streamlit, embeddings), ce projet rend l’intelligence artificielle accessible et interactive, avec des bénéfices immédiats pour l'exploration, la sensibilisation ou l’analyse d'opinion sur des enjeux environnementaux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh-bUUfHSQYy"
      },
      "source": [
        "Et si demain, ces outils devenaient des assistants de décision pour la transition écologique, jusqu’où pourrions-nous aller collectivement ?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
