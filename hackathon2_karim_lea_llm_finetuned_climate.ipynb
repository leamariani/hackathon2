{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnNQNqXaS458"
      },
      "source": [
        "# **Fine-Tuned LLM for Sentiment Analysis and Contextual Responses**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lby4w31eZ5u-"
      },
      "source": [
        "Dans un monde satur√© de donn√©es, o√π les entreprises cherchent √† automatiser leurs interactions tout en maintenant un haut niveau de personnalisation, l‚ÄôIntelligence Artificielle g√©n√©rative ouvre des perspectives in√©dites. Mais comment cr√©er un assistant conversationnel qui ne se contente pas de r√©pondre, mais qui comprend vraiment ce que ressent l‚Äôutilisateur et s‚Äôadapte √† son contexte‚ÄØ?\n",
        "\n",
        "C‚Äôest tout l‚Äôenjeu de notre projet : construire un assistant IA intelligent capable √† la fois de d√©tecter le ton √©motionnel d‚Äôun message (positif, n√©gatif, neutre) et de g√©n√©rer des r√©ponses enrichies par un moteur de recherche contextuel, le tout avec des mod√®les l√©gers et optimis√©s gr√¢ce √† LoRA, une m√©thode de fine-tuning √©conome en ressources.\n",
        "\n",
        "Notre mission : rendre l‚ÄôIA utile, pertinente et accessible, m√™me avec des moyens limit√©s. Ce projet d√©montre qu‚Äôon peut concilier efficacit√©, sobri√©t√© technologique et intelligence conversationnelle, pour r√©pondre √† des enjeux m√©tiers bien r√©els : service client, RH, assistance juridique, e-commerce‚Ä¶ les cas d‚Äôusage sont nombreux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4ZVvmadrQ96"
      },
      "source": [
        "# 1.**Module Core - core_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpXUvNPOTPe-"
      },
      "source": [
        "Ce module core_modules.py constitue un composant fondamental de l‚Äôarchitecture du projet. Il centralise √† la fois les param√®tres de configuration du syst√®me et la structure de sortie des pr√©dictions, permettant ainsi une meilleure organisation, une lisibilit√© accrue, et une √©volutivit√© ma√Ætris√©e du code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY-8Ke1NTPOW"
      },
      "source": [
        "### **Intention et d√©marche**\n",
        "\n",
        "Objectif du module :\n",
        "Structurer proprement les r√©sultats via la classe PredictionResult.\n",
        "\n",
        "Externaliser la configuration dans une classe ClimateConfig pour √©viter les variables magiques dispers√©es dans le code.\n",
        "\n",
        "Pourquoi c‚Äôest important :\n",
        "Lorsqu'on travaille sur un projet complexe avec fine-tuning de LLMs, classification, g√©n√©ration, et retrieval, il devient essentiel de standardiser les flux de donn√©es et de modulariser les param√®tres.\n",
        "\n",
        "Cela permet aussi d‚Äôassurer une compatibilit√© fluide entre les modules d‚Äô√©valuation, d‚Äôinterface et de g√©n√©ration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l6Yz_5prPWS",
        "outputId": "2805c27e-6818-44cb-c144-749c049535d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting core_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core_modules.py\n",
        "# core_modules.py\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import Dict, List, Optional\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "\n",
        "@dataclass\n",
        "class PredictionResult:\n",
        "    \"\"\"Structure pour les r√©sultats de pr√©diction\"\"\"\n",
        "    text: str\n",
        "    predicted_label: str\n",
        "    confidence: float\n",
        "    all_scores: Dict[str, float]\n",
        "    context: Optional[List[str]] = None\n",
        "    processing_time: float = 0.0\n",
        "\n",
        "class ClimateConfig:\n",
        "    \"\"\"Configuration centralis√©e\"\"\"\n",
        "    def __init__(self):\n",
        "        self.model_name = \"distilbert-base-uncased\"\n",
        "        self.max_length = 256\n",
        "        self.batch_size = 16\n",
        "        self.learning_rate = 2e-4\n",
        "        self.epochs = 3\n",
        "        self.lora_r = 16\n",
        "        self.lora_alpha = 32\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return {\n",
        "            'model_name': self.model_name,\n",
        "            'max_length': self.max_length,\n",
        "            'batch_size': self.batch_size,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'epochs': self.epochs,\n",
        "            'device': str(self.device),\n",
        "            'lora_config': {'r': self.lora_r, 'alpha': self.lora_alpha}\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTHzrWB4Tjuw"
      },
      "source": [
        "`ClimateConfig`Une classe de configuration centralis√©e :\n",
        "\n",
        "Contient les hyperparam√®tres du mod√®le, du fine-tuning LoRA, et la d√©tection automatique de device (GPU/CPU).\n",
        "\n",
        "Gr√¢ce √† la m√©thode to_dict(), cette configuration peut √™tre journalis√©e, sauvegard√©e, ou utilis√©e dynamiquement dans d‚Äôautres modules.\n",
        "\n",
        "**Avantage** : on peut tester plusieurs variantes de configuration sans toucher au c≈ìur du code. C‚Äôest essentiel pour l‚Äôexp√©rimentation en machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnuHfgjHTkyM"
      },
      "source": [
        "**Enjeu technique et challenge :**\n",
        "Le challenge ici est de maintenir un code propre, modulaire et tra√ßable, dans un projet m√™lant fine-tuning PEFT (LoRA), embedding, retrieval, inf√©rence g√©n√©rative, et interface utilisateur.\n",
        "\n",
        "Une mauvaise gestion des param√®tres ou un manque de structuration des r√©sultats rendraient le projet instable, peu r√©utilisable, et difficile √† √©valuer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jup6I0MKT0sJ"
      },
      "source": [
        "**Interpr√©tation du r√©sultat :**\n",
        "Ce module ne retourne pas un r√©sultat au sens fonctionnel imm√©diat, mais il formalise et encapsule deux √©l√©ments essentiels :\n",
        "\n",
        "\n",
        "\n",
        "*   La standardisation de la sortie du mod√®le (PredictionResult)\n",
        "*   La gouvernance centralis√©e des param√®tres (ClimateConfig)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Cela permet de :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Faciliter l‚Äôorchestration du pipeline global\n",
        "*   Tracer les exp√©rimentations\n",
        "*   Int√©grer facilement des logs, m√©triques, et dashboards\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9yVcO8T5vB"
      },
      "source": [
        "**Conclusion :** Ce module pose les bases de la robustesse du projet IA. Il refl√®te une bonne pratique d‚Äôindustrialisation des projets LLM : abstraction des param√®tres, tra√ßabilit√©, et standardisation des r√©sultats. Dans un contexte professionnel, cette structuration permet :\n",
        "\n",
        "\n",
        "*   Une collaboration fluide entre data scientists et d√©veloppeurs front-end\n",
        "*   Une r√©utilisabilit√© du mod√®le dans d'autres projets\n",
        "*   Une mise en production simplifi√©e\n",
        "\n",
        "\n",
        "En r√©sum√©, ce fichier est invisible pour l‚Äôutilisateur final, mais essentiel √† la stabilit√©, l‚Äô√©volution, et la fiabilit√© du syst√®me."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_bu1PGeUEYq"
      },
      "source": [
        "# 2. **Module Data Processing - data_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWFjD4keP89"
      },
      "source": [
        "Ce module data_modules.py est le p√¥le central de pr√©traitement des donn√©es dans le pipeline du projet de fine-tuning LLM avec LoRA et g√©n√©ration contextuelle. Il a √©t√© con√ßu avec une logique r√©siliente, automatis√©e et hautement r√©utilisable, indispensable pour manipuler des datasets h√©t√©rog√®nes dans un contexte de hackathon ou d‚Äôexp√©rimentation rapide.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlfrNA8_eZrW"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyki8ArTeWD1"
      },
      "source": [
        "\n",
        "**Objectifs principaux :**\n",
        "\n",
        "*   D√©tecter automatiquement les colonnes texte et label dans n‚Äôimporte quel DataFrame.\n",
        "*   Nettoyer et pr√©parer les donn√©es de mani√®re robuste, quels que soient leur format ou leur qualit√© initiale.\n",
        "*   G√©n√©rer un triplet train/val/test proprement structur√©, stratifi√© si possible.\n",
        "*   Produire un format compatible avec les mod√®les Hugging Face (datasets.Dataset).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6w7f53esmi"
      },
      "source": [
        "**Pourquoi cette approche ?**\n",
        "Parce que dans un hackathon ou une exp√©rimentation IA rapide :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Les donn√©es ne sont pas toujours bien structur√©es.\n",
        "*   Le temps est limit√© pour ajuster manuellement les colonnes ou le nettoyage.\n",
        "*   Une flexibilit√© et automatisation maximale est n√©cessaire.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtjC1wkcrbil",
        "outputId": "4aaab23a-c67f-4882-b0c3-80418bad7fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting data_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile data_modules.py\n",
        "\n",
        "# data_modules.py\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Gestion centralis√©e du traitement des donn√©es avec gestion robuste des erreurs\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_col = None\n",
        "        self.label_col = None\n",
        "        self.label_mapping = {}\n",
        "\n",
        "    def detect_columns(self, df: pd.DataFrame) -> Tuple[str, str]:\n",
        "        \"\"\"D√©tection automatique des colonnes texte et label avec validation\"\"\"\n",
        "        print(f\"üîç D√©tection des colonnes sur {df.shape[0]} lignes et {df.shape[1]} colonnes\")\n",
        "        print(f\"üìã Colonnes disponibles: {list(df.columns)}\")\n",
        "\n",
        "        text_keywords = ['self_text', 'text', 'content', 'message', 'comment', 'body', 'description']\n",
        "        label_keywords = ['comment_sentiment', 'sentiment', 'label', 'category', 'class', 'target']\n",
        "\n",
        "        # Recherche intelligente\n",
        "        text_col = None\n",
        "        label_col = None\n",
        "\n",
        "        # Recherche par mots-cl√©s\n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "\n",
        "            # Recherche colonne texte\n",
        "            if not text_col:\n",
        "                for keyword in text_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        text_col = col\n",
        "                        break\n",
        "\n",
        "            # Recherche colonne label\n",
        "            if not label_col:\n",
        "                for keyword in label_keywords:\n",
        "                    if keyword.lower() in col_lower:\n",
        "                        label_col = col\n",
        "                        break\n",
        "\n",
        "        # Fallback intelligent pour la colonne texte\n",
        "        if not text_col:\n",
        "            string_cols = []\n",
        "            for col in df.columns:\n",
        "                try:\n",
        "                    # V√©rifier si la colonne contient principalement du texte\n",
        "                    sample = df[col].dropna().head(100)\n",
        "                    if len(sample) > 0:\n",
        "                        # Convertir en string et calculer la longueur moyenne\n",
        "                        sample_str = sample.astype(str)\n",
        "                        avg_length = sample_str.str.len().mean()\n",
        "                        if avg_length > 10:  # Textes probablement plus longs que 10 caract√®res\n",
        "                            string_cols.append((col, avg_length))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if string_cols:\n",
        "                # Prendre la colonne avec le texte le plus long en moyenne\n",
        "                text_col = max(string_cols, key=lambda x: x[1])[0]\n",
        "            else:\n",
        "                # Last resort: premi√®re colonne object\n",
        "                object_cols = df.select_dtypes(include=['object']).columns\n",
        "                if len(object_cols) > 0:\n",
        "                    text_col = object_cols[0]\n",
        "\n",
        "        # Fallback pour la colonne label\n",
        "        if not label_col:\n",
        "            # Chercher une colonne avec peu de valeurs uniques (potentiel label)\n",
        "            for col in df.columns:\n",
        "                if col != text_col:\n",
        "                    try:\n",
        "                        unique_count = df[col].nunique()\n",
        "                        total_count = len(df[col].dropna())\n",
        "                        if total_count > 0 and unique_count < min(20, total_count * 0.1):\n",
        "                            label_col = col\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Si toujours pas trouv√©, prendre la derni√®re colonne\n",
        "            if not label_col:\n",
        "                label_col = df.columns[-1]\n",
        "\n",
        "        print(f\"‚úÖ Colonnes d√©tect√©es: Text='{text_col}', Label='{label_col}'\")\n",
        "        return text_col, label_col\n",
        "\n",
        "    def clean_text_column(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Nettoyage robuste d'une colonne texte\"\"\"\n",
        "        try:\n",
        "            # Convertir en string d'abord\n",
        "            cleaned = series.astype(str)\n",
        "\n",
        "            # Remplacer les valeurs probl√©matiques\n",
        "            cleaned = cleaned.replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "            # Supprimer les espaces\n",
        "            cleaned = cleaned.str.strip()\n",
        "\n",
        "            # Remplacer les cha√Ænes vides par NaN\n",
        "            cleaned = cleaned.replace('', pd.NA)\n",
        "\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur nettoyage texte: {e}\")\n",
        "            # Fallback: conversion simple\n",
        "            return series.astype(str)\n",
        "\n",
        "    def prepare_datasets(self, df: pd.DataFrame, sample_size: int = 8000) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Pr√©paration des datasets avec validation robuste\"\"\"\n",
        "\n",
        "        print(f\"üìä Pr√©paration des datasets - Taille originale: {df.shape}\")\n",
        "\n",
        "        # D√©tection des colonnes\n",
        "        self.text_col, self.label_col = self.detect_columns(df)\n",
        "\n",
        "        if not self.text_col or not self.label_col:\n",
        "            raise ValueError(f\"‚ùå Impossible de d√©tecter les colonnes: text='{self.text_col}', label='{self.label_col}'\")\n",
        "\n",
        "        # Extraction et copie des colonnes n√©cessaires\n",
        "        try:\n",
        "            df_work = df[[self.text_col, self.label_col]].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"‚ùå Colonnes manquantes: {e}\")\n",
        "            print(f\"Colonnes disponibles: {list(df.columns)}\")\n",
        "            raise\n",
        "\n",
        "        # Renommer les colonnes\n",
        "        df_work.columns = ['text', 'label']\n",
        "\n",
        "        print(f\"üìã Avant nettoyage: {len(df_work)} lignes\")\n",
        "\n",
        "        # Nettoyage robuste des donn√©es\n",
        "        # 1. Nettoyage de la colonne texte\n",
        "        df_work['text'] = self.clean_text_column(df_work['text'])\n",
        "\n",
        "        # 2. Nettoyage de la colonne label\n",
        "        df_work['label'] = df_work['label'].astype(str).str.strip()\n",
        "        df_work['label'] = df_work['label'].replace(['nan', 'NaN', 'None', 'null', ''], pd.NA)\n",
        "\n",
        "        # 3. Suppression des lignes avec des valeurs manquantes\n",
        "        initial_size = len(df_work)\n",
        "        df_work = df_work.dropna()\n",
        "        print(f\"üßπ Apr√®s suppression des NaN: {len(df_work)} lignes (supprim√©: {initial_size - len(df_work)})\")\n",
        "\n",
        "        # 4. Filtrage des textes trop courts (de mani√®re s√©curis√©e)\n",
        "        try:\n",
        "            # V√©rifier que nous avons bien des strings\n",
        "            df_work['text'] = df_work['text'].astype(str)\n",
        "\n",
        "            # Filtrer les textes trop courts\n",
        "            mask = df_work['text'].str.len() > 5\n",
        "            df_work = df_work[mask]\n",
        "            print(f\"üìù Apr√®s filtrage textes courts: {len(df_work)} lignes\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du filtrage des textes: {e}\")\n",
        "            # Continuer sans filtrage si erreur\n",
        "\n",
        "        # V√©rification finale\n",
        "        if len(df_work) == 0:\n",
        "            raise ValueError(\"‚ùå Aucune donn√©e valide apr√®s nettoyage!\")\n",
        "\n",
        "        # 5. √âchantillonnage si n√©cessaire\n",
        "        if len(df_work) > sample_size:\n",
        "            df_work = df_work.sample(n=sample_size, random_state=42)\n",
        "            print(f\"üéØ √âchantillonnage √† {sample_size} lignes\")\n",
        "\n",
        "        # 6. Mapping des labels\n",
        "        unique_labels = sorted(df_work['label'].unique())\n",
        "        print(f\"üè∑Ô∏è Labels uniques trouv√©s: {unique_labels}\")\n",
        "\n",
        "        self.label_mapping = {str(label): idx for idx, label in enumerate(unique_labels)}\n",
        "        df_work['label_id'] = df_work['label'].astype(str).map(self.label_mapping)\n",
        "\n",
        "        # V√©rification du mapping\n",
        "        if df_work['label_id'].isna().any():\n",
        "            print(\"‚ö†Ô∏è Probl√®me de mapping des labels d√©tect√©\")\n",
        "            print(f\"Labels non mapp√©s: {df_work[df_work['label_id'].isna()]['label'].unique()}\")\n",
        "\n",
        "        print(f\"üìä Mapping des labels: {self.label_mapping}\")\n",
        "\n",
        "        # 7. Splits stratifi√©s\n",
        "        try:\n",
        "            # V√©rifier si on peut faire une stratification\n",
        "            if len(unique_labels) > 1 and all(df_work['label_id'].value_counts() >= 2):\n",
        "                stratify_col = df_work['label_id']\n",
        "                print(\"‚úÖ Stratification activ√©e\")\n",
        "            else:\n",
        "                stratify_col = None\n",
        "                print(\"‚ö†Ô∏è Pas de stratification (pas assez d'exemples par classe)\")\n",
        "\n",
        "            # Premier split: train vs (val + test)\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_work,\n",
        "                test_size=0.4,\n",
        "                random_state=42,\n",
        "                stratify=stratify_col if stratify_col is not None else None\n",
        "            )\n",
        "\n",
        "            # Deuxi√®me split: val vs test\n",
        "            if stratify_col is not None:\n",
        "                temp_stratify = temp_df['label_id']\n",
        "            else:\n",
        "                temp_stratify = None\n",
        "\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=0.5,\n",
        "                random_state=42,\n",
        "                stratify=temp_stratify if temp_stratify is not None else None\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors du split: {e}\")\n",
        "            # Fallback: split simple\n",
        "            train_size = int(0.6 * len(df_work))\n",
        "            val_size = int(0.2 * len(df_work))\n",
        "\n",
        "            train_df = df_work[:train_size]\n",
        "            val_df = df_work[train_size:train_size+val_size]\n",
        "            test_df = df_work[train_size+val_size:]\n",
        "\n",
        "        print(f\"üìä Splits finaux: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # 8. Conversion en Dataset\n",
        "        try:\n",
        "            train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
        "            test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
        "\n",
        "            print(\"‚úÖ Datasets cr√©√©s avec succ√®s\")\n",
        "\n",
        "            return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur lors de la cr√©ation des datasets: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques du processeur de donn√©es\"\"\"\n",
        "        return {\n",
        "            \"text_column\": self.text_col,\n",
        "            \"label_column\": self.label_col,\n",
        "            \"label_mapping\": self.label_mapping,\n",
        "            \"num_labels\": len(self.label_mapping)\n",
        "        }\n",
        "\n",
        "    def validate_dataframe(self, df: pd.DataFrame) -> bool:\n",
        "        \"\"\"Validation d'un DataFrame\"\"\"\n",
        "        try:\n",
        "            if df is None or df.empty:\n",
        "                print(\"‚ùå DataFrame vide ou None\")\n",
        "                return False\n",
        "\n",
        "            if len(df.columns) < 2:\n",
        "                print(\"‚ùå DataFrame doit avoir au moins 2 colonnes\")\n",
        "                return False\n",
        "\n",
        "            print(f\"‚úÖ DataFrame valide: {df.shape}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur validation DataFrame: {e}\")\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_o8Cgbvfjua"
      },
      "source": [
        "**Enjeux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Fiabilit√© sur donn√©es inconnues : il faut que le module fonctionne sur n'importe quel jeu de donn√©es texte+label.\n",
        "*   Robustesse aux erreurs utilisateurs : fichiers mal format√©s, labels textuels, colonnes manquantes ou bruit√©es.\n",
        "*   Pr√©paration optimis√©e pour le fine-tuning LLMs : nettoyage, encodage des labels, et split coh√©rent sont critiques pour √©viter du surapprentissage ou des √©checs d'entra√Ænement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "\n",
        "*   D√©tecter automatiquement les bonnes colonnes sans\n",
        "heuristique trop rigide.\n",
        "*   Nettoyer du texte avec des formats incoh√©rents, des valeurs manquantes, des types mixtes.\n",
        "*   G√©rer des jeux de donn√©es d√©s√©quilibr√©s ou avec trop peu d'exemples par classe.\n",
        "*   Assurer une compatibilit√© fluide avec la biblioth√®que datasets de Hugging Face.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m1vP8V4fngl"
      },
      "source": [
        "Ce module produit trois objets de type Dataset : train_dataset, val_dataset, test_dataset ‚Äì directement utilisables avec les tokenizers et les Trainer de Hugging Face.\n",
        "\n",
        "Il g√©n√®re aussi :\n",
        "\n",
        "Un mapping label ‚Üí id compatible avec les mod√®les de classification.\n",
        "\n",
        "Des messages de log d√©taill√©s pour comprendre chaque √©tape et corriger si besoin.\n",
        "\n",
        "Un fallback automatique si certaines op√©rations √©chouent (e.g., split sans stratification).\n",
        "\n",
        "Gr√¢ce √† cette logique :\n",
        "\n",
        "Le mod√®le ne se base jamais sur des donn√©es corrompues ou mal √©tiquet√©es.\n",
        "\n",
        "L‚Äôentra√Ænement est plus stable, reproductible et transparent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1taFg01fvpa"
      },
      "source": [
        "**Conclusion :**\n",
        "Ce module est un v√©ritable pont entre le monde brut des donn√©es r√©elles et les exigences rigoureuses du machine learning moderne. Il incarne une bonne pratique fondamentale en IA : rendre le traitement des donn√©es automatique, s√©curis√© et tra√ßable.\n",
        "\n",
        "**B√©n√©fices m√©tiers :**\n",
        "Permet √† une √©quipe data ou produit de tester rapidement plusieurs sources de donn√©es sans ajustement manuel.\n",
        "\n",
        "R√©duit consid√©rablement le risque d'erreur humaine lors de l‚Äôexploration et la pr√©paration des donn√©es.\n",
        "\n",
        "Acc√©l√®re la mise en production ou le prototypage d‚Äôoutils IA centr√©s sur le langage naturel.\n",
        "\n",
        "En somme, data_modules.py est un acc√©l√©rateur de projet IA, une brique essentielle pour garantir que \"garbage in ‚â† garbage out\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45WnDHBEURFf"
      },
      "source": [
        "# 3. **Module Mod√®le - model_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0vCHuPWg1CT"
      },
      "source": [
        "Le fichier model_modules.py repr√©sente la brique strat√©gique du projet IA, d√©di√©e √† la configuration, l‚Äôadaptation et l‚Äôentra√Ænement du mod√®le de classification via une approche parameter-efficient (fine-tuning avec LoRA). Il encapsule l'ensemble des d√©cisions techniques critiques li√©es au mod√®le, au tokenizer, aux param√®tres d'entra√Ænement, et √† la logique d'√©valuation, dans une classe modulaire, claire et facilement maintenable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6ZXCsv1hCRt"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V0DBnLmg-8s"
      },
      "source": [
        "\n",
        "**Objectifs du module ModelManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Centraliser la configuration du tokenizer et du mod√®le Hugging Face.\n",
        "*   Appliquer LoRA (Low-Rank Adaptation) pour une fine-tuning efficace sur GPU ou CPU.\n",
        "*   Param√©trer finement l‚Äôentra√Ænement, en prenant en compte la gestion m√©moire, les m√©triques pertinentes, et les strat√©gies de logging/saving.\n",
        "*   Instancier Trainer de Hugging Face de mani√®re optimis√©e pour une exp√©rience de fine-tuning fluide et reproductible.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Pourquoi cette structuration ?**\n",
        "\n",
        "\n",
        "\n",
        "*   Favorise la r√©utilisabilit√© (nouveau mod√®le ou dataset = m√™mes m√©thodes).\n",
        "*   R√©pond aux contraintes de performance (GPU limit√©, petit batch).\n",
        "*   Simplifie la tra√ßabilit√© des exp√©riences (logs + √©valuation centralis√©e).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCUmVHjPrhv5",
        "outputId": "b5784870-963c-4fcc-a200-35fea3367882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting model_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model_modules.py\n",
        "# model_modules.py (VERSION ULTRA-STABLE - FP32 ONLY)\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# D√©sactiver les warnings\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = None\n",
        "        self.peft_model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def setup_tokenizer(self):\n",
        "        \"\"\"Tokenizer s√©curis√©.\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token or '[PAD]'\n",
        "            print(f\"‚úÖ Tokenizer charg√© : {self.config.model_name}\")\n",
        "            return self.tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur tokenizer : {e}\")\n",
        "            raise\n",
        "\n",
        "    def setup_model(self, num_labels: int):\n",
        "        \"\"\"Mod√®le en FP32 uniquement pour √©viter l'erreur FP16.\"\"\"\n",
        "        if num_labels < 2:\n",
        "            raise ValueError(\"‚ùå num_labels doit √™tre ‚â• 2\")\n",
        "\n",
        "        try:\n",
        "            print(f\"üîß Chargement mod√®le FP32 pour {num_labels} classes...\")\n",
        "\n",
        "            # üîí Forcer FP32\n",
        "            base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.config.model_name,\n",
        "                num_labels=num_labels,\n",
        "                torch_dtype=torch.float32,  # üîí FP32 uniquement\n",
        "                device_map=None,  # Pas de device_map pour √©viter les conflits\n",
        "                problem_type=\"single_label_classification\",\n",
        "            )\n",
        "\n",
        "            target_modules = self.get_target_modules()\n",
        "\n",
        "            lora_config = LoraConfig(\n",
        "                task_type=TaskType.SEQ_CLS,\n",
        "                r=self.config.lora_r,\n",
        "                lora_alpha=self.config.lora_alpha,\n",
        "                lora_dropout=0.1,\n",
        "                target_modules=target_modules,\n",
        "                bias=\"none\",\n",
        "            )\n",
        "\n",
        "            self.peft_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "            # Affichage des param√®tres\n",
        "            trainable = sum(p.numel() for p in self.peft_model.parameters() if p.requires_grad)\n",
        "            total = sum(p.numel() for p in self.peft_model.parameters())\n",
        "            print(f\"üìä Param√®tres : {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
        "\n",
        "            return self.peft_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur mod√®le : {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_target_modules(self):\n",
        "        \"\"\"Modules cibles LoRA.\"\"\"\n",
        "        name = self.config.model_name.lower()\n",
        "        if \"distilbert\" in name:\n",
        "            return [\"q_lin\", \"v_lin\"]\n",
        "        elif \"bert\" in name or \"roberta\" in name:\n",
        "            return [\"query\", \"value\"]\n",
        "        return [\"query\", \"value\", \"dense\"]\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenisation.\"\"\"\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.config.max_length,\n",
        "        )\n",
        "\n",
        "    def setup_training_args(self, output_dir=\"outputs/runs\"):\n",
        "        \"\"\"TrainingArguments 100% FP32.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        logging_dir = os.path.join(output_dir, \"logs\")\n",
        "        os.makedirs(logging_dir, exist_ok=True)\n",
        "\n",
        "        return TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=self.config.epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            per_device_eval_batch_size=self.config.batch_size * 2,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            warmup_steps=200,\n",
        "            weight_decay=0.01,\n",
        "\n",
        "            eval_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=50,\n",
        "            logging_dir=logging_dir,\n",
        "\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_accuracy\",\n",
        "            greater_is_better=True,\n",
        "\n",
        "            # üîí D√©sactivation compl√®te de la pr√©cision mixte\n",
        "            fp16=False,\n",
        "            bf16=False,\n",
        "            fp16_backend=None,\n",
        "            half_precision_backend=None,\n",
        "\n",
        "            gradient_checkpointing=True,\n",
        "            dataloader_num_workers=2,\n",
        "\n",
        "            save_total_limit=2,\n",
        "            save_steps=500,\n",
        "\n",
        "            report_to=\"none\",\n",
        "            remove_unused_columns=False,\n",
        "            push_to_hub=False,\n",
        "        )\n",
        "\n",
        "    def setup_trainer(self, train_dataset, val_dataset):\n",
        "        \"\"\"Trainer s√©curis√©.\"\"\"\n",
        "        try:\n",
        "            training_args = self.setup_training_args()\n",
        "            data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "\n",
        "            def compute_metrics(eval_pred):\n",
        "                predictions, labels = eval_pred\n",
        "                preds = np.argmax(predictions, axis=1)\n",
        "                return {\n",
        "                    \"accuracy\": accuracy_score(labels, preds),\n",
        "                    \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "                    \"precision\": precision_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                    \"recall\": recall_score(labels, preds, average=\"weighted\", zero_division=0),\n",
        "                }\n",
        "\n",
        "            self.trainer = Trainer(\n",
        "                model=self.peft_model,\n",
        "                args=training_args,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=val_dataset,\n",
        "                tokenizer=self.tokenizer,\n",
        "                compute_metrics=compute_metrics,\n",
        "                data_collator=data_collator,\n",
        "                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Trainer configur√©\")\n",
        "            return self.trainer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erreur trainer : {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFmi29tHhjpH"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9i0oZiBhe7k"
      },
      "source": [
        "\n",
        "**Enjeux cl√©s :**\n",
        "\n",
        "\n",
        "*   Adapter un LLM √† une t√¢che de classification sans explosion m√©moire (gr√¢ce √† LoRA).\n",
        "*   Assurer la robustesse cross-model : prendre en charge DistilBERT, BERT, etc.\n",
        "*   Faciliter le scaling des exp√©rimentations : simple changement de config = nouveau test reproductible.\n",
        "*   Optimiser la performance m√©tier via des m√©triques √©quilibr√©es (f1, accuracy...).\n",
        "\n",
        "\n",
        "**Challenges techniques :**\n",
        "\n",
        "\n",
        "*   Cibler correctement les couches internes √† adapter avec LoRA (ex. q_lin, v_lin, ou query, value).\n",
        "*   Param√©trer TrainingArguments de fa√ßon √©quilibr√©e pour s‚Äôadapter √† du low compute budget.\n",
        "*   Synchroniser tokenizer, dataset, et mod√®le, sans bugs de dimension ou padding.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Jiy7o6h9v9"
      },
      "source": [
        "**Gr√¢ce √† ce module :**\n",
        "\n",
        "Le mod√®le est LoRA-ready, avec gel des poids de base et focus sur l‚Äôadaptation via matrices faibles rangs.\n",
        "\n",
        "Le tokenizer est automatiquement configur√©, avec pad_token g√©r√© pour √©viter les erreurs silencieuses.\n",
        "\n",
        "Les m√©triques cl√©s (accuracy, F1, precision, recall) sont calcul√©es syst√©matiquement.\n",
        "\n",
        "L‚Äôutilisateur peut entra√Æner sur GPU/CPU, avec logging optimis√© pour analyse fine.\n",
        "\n",
        "Cette architecture modulaire permet de tester facilement plusieurs mod√®les (distilbert, roberta, albert) sans r√©√©criture de code, et offre un point d'entr√©e unique pour tout le pipeline de classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGqSAlpHiJ2c"
      },
      "source": [
        "**Conclusion :**\n",
        "model_modules.py est une colonne vert√©brale technique du projet. Il incarne l‚Äôexigence de :\n",
        "\n",
        "\n",
        "\n",
        "*   Performance (LoRA),\n",
        "*   Lisibilit√© (code modulaire),\n",
        "*   Robustesse (fallback CPU, configuration dynamique),\n",
        "*   √âvaluation m√©tier (m√©triques pertinentes d√®s l‚Äôentra√Ænement).\n",
        "\n",
        "\n",
        "Il d√©montre comment une bonne ing√©nierie mod√®le permet √† une √©quipe IA de construire des prototypes puissants et scalables, tout en gardant la possibilit√© de passer rapidement en production ou en d√©monstration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LikcKVbdielx"
      },
      "source": [
        "**B√©n√©fice m√©tier :**\n",
        "Ce module simplifie le travail des √©quipes data et produit en r√©duisant drastiquement le co√ªt de fine-tuning tout en maintenant des performances √©lev√©es. Il est donc un levier strat√©gique pour d√©mocratiser l‚Äôadaptation de LLMs sur des cas sp√©cifiques (analyse de sentiment, classification th√©matique, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riAeZv2PUka_"
      },
      "source": [
        "# 4. **Visualization_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TDCzMFujkX7"
      },
      "source": [
        "Le module visualization_modules.py est une composante cl√© pour l‚Äôanalyse, l‚Äôinterpr√©tation et la communication des r√©sultats du projet de fine-tuning d‚Äôun LLM. Il centralise les outils de visualisation de la performance (courbes d‚Äôentra√Ænement, pr√©cision, matrice de confusion, etc.) pour permettre √† l‚Äô√©quipe data ‚Äî mais aussi produit ou m√©tier ‚Äî de comprendre comment le mod√®le apprend et o√π il se trompe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxc1VLWjuua"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuDE7Sq7jrvz"
      },
      "source": [
        "\n",
        "**Objectifs du VisualizationManager :**\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre les courbes d‚Äôentra√Ænement lisibles et interactives via Streamlit.\n",
        "*   Offrir des analyses post-training, comme la matrice de confusion et le rapport de classification.\n",
        "*   Automatiser le suivi des performances, sans besoin de coder chaque fois les visualisations.\n",
        "*   \n",
        "Faciliter la compr√©hension m√©tier des r√©sultats via des graphiques clairs et structur√©s.\n",
        "\n",
        "\n",
        "**Pourquoi ce module est crucial ?**\n",
        "\n",
        "Dans un projet IA, le succ√®s ne se mesure pas uniquement par des chiffres. Les visualisations :\n",
        "*   r√©v√®lent les biais d'apprentissage (ex. overfitting),\n",
        "*   permettent de communiquer efficacement entre profils techniques et non techniques,\n",
        "*   facilitent la prise de d√©cision (changement de mod√®le,ajustement hyperparam√®tres...).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMZt-WwIMwez",
        "outputId": "20ad39c0-e925-424e-d407-12a365a1919b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting visualization_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile visualization_modules.py\n",
        "# visualization_modules.py\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import numpy as np\n",
        "\n",
        "class VisualizationManager:\n",
        "    \"\"\"Gestion des visualisations d'entra√Ænement et d'√©valuation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_curves(log_dir: str):\n",
        "        \"\"\"Affiche les courbes d'entra√Ænement (garantit logs)\"\"\"\n",
        "        try:\n",
        "            import json\n",
        "            log_file = f\"{log_dir}/trainer_state.json\"\n",
        "            if not os.path.exists(log_file):\n",
        "                st.warning(\"üìÑ Fichier de logs non trouv√©\")\n",
        "                return\n",
        "\n",
        "            with open(log_file, 'r') as f:\n",
        "                logs = json.load(f)\n",
        "\n",
        "            history = logs.get('log_history', [])\n",
        "            if not history:\n",
        "                st.warning(\"üìâ Aucune donn√©e d'entra√Ænement trouv√©e\")\n",
        "                return\n",
        "\n",
        "            epochs, train_loss, eval_loss, eval_accuracy = [], [], [], []\n",
        "            for entry in history:\n",
        "                if 'eval_loss' in entry:\n",
        "                    epochs.append(entry.get('epoch', 0))\n",
        "                    eval_loss.append(entry.get('eval_loss', 0))\n",
        "                    eval_accuracy.append(entry.get('eval_accuracy', 0))\n",
        "                elif 'loss' in entry:\n",
        "                    train_loss.append(entry.get('loss', 0))\n",
        "\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "            fig.suptitle('üìà √âvolution de l\\'entra√Ænement', fontsize=16)\n",
        "\n",
        "            # Loss\n",
        "            if train_loss:\n",
        "                axes[0].plot(range(len(train_loss)), train_loss, 'b-', label='Train Loss', marker='o')\n",
        "            if eval_loss:\n",
        "                axes[0].plot(epochs[:len(eval_loss)], eval_loss, 'r-', label='Eval Loss', marker='s')\n",
        "            axes[0].set_title('Perte (Loss)')\n",
        "            axes[0].set_xlabel('Epoch')\n",
        "            axes[0].set_ylabel('Loss')\n",
        "            axes[0].legend()\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "            # Accuracy\n",
        "            if eval_accuracy:\n",
        "                axes[1].plot(epochs[:len(eval_accuracy)], eval_accuracy, 'g-', label='Accuracy', marker='^')\n",
        "            axes[1].set_title('Pr√©cision')\n",
        "            axes[1].set_xlabel('Epoch')\n",
        "            axes[1].set_ylabel('Accuracy')\n",
        "            axes[1].legend()\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur lors de l'affichage des courbes : {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def show_confusion_matrix(trainer, test_dataset, label_names):\n",
        "        \"\"\"Affiche la matrice de confusion\"\"\"\n",
        "        try:\n",
        "            from sklearn.metrics import confusion_matrix, classification_report\n",
        "            preds_output = trainer.predict(test_dataset)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            cm = confusion_matrix(labels, preds)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=label_names, yticklabels=label_names, ax=ax)\n",
        "            ax.set_title('Matrice de Confusion')\n",
        "            ax.set_xlabel('Pr√©dictions')\n",
        "            ax.set_ylabel('Vraies √©tiquettes')\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Rapport de classification\n",
        "            st.subheader(\"üìä Rapport de Classification\")\n",
        "            report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
        "            report_df = pd.DataFrame(report).transpose()\n",
        "            st.dataframe(report_df)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur matrice : {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_class_distribution(labels, label_names=None, title=\"Distribution des classes\"):\n",
        "        \"\"\"Histogramme des classes\"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.countplot(x=labels, ax=ax)\n",
        "        if label_names:\n",
        "            ax.set_xticklabels(label_names)\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel(\"Classes\")\n",
        "        ax.set_ylabel(\"Nombre d'exemples\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_f1_per_class(labels_true, labels_pred, label_names):\n",
        "        \"\"\"Barplot F1-score par classe\"\"\"\n",
        "        scores = f1_score(labels_true, labels_pred, average=None)\n",
        "        fig, ax = plt.subplots(figsize=(8, 4))\n",
        "        sns.barplot(x=label_names, y=scores, ax=ax)\n",
        "        ax.set_title(\"F1-score par classe\")\n",
        "        ax.set_ylabel(\"F1-score\")\n",
        "        st.pyplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ycovcgRkfPG"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SufjM84RkdQT"
      },
      "source": [
        "\n",
        "**Enjeux principaux :**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Rendre accessibles les logs de Hugging Face √† travers une UI conviviale.\n",
        "*   Permettre une analyse rapide et fiable des pr√©dictions du mod√®le sur le jeu de test.\n",
        "*   Cr√©er des visualisations robustes qui ne plantent pas si un fichier est absent ou mal form√©.\n",
        "\n",
        "\n",
        "**Difficult√©s techniques :**\n",
        "\n",
        "\n",
        "*   Lecture et structuration des logs (trainer_state.json) : il faut interpr√©ter les √©tapes d'entra√Ænement parfois d√©sy\n",
        "\n",
        "\n",
        "*   Synchronisation des m√©triques (loss, accuracy, learning\n",
        "rate) sur plusieurs epochs.\n",
        "\n",
        "*   Adaptation de matplotlib et seaborn √† l‚Äôenvironnement Streamlit.\n",
        "*   Affichage conditionnel (fallbacks, gestion d‚Äôerreurs utilisateur ou absence de logs).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoTMpu1rlJMR"
      },
      "source": [
        "**Gr√¢ce √† ce module :**\n",
        "\n",
        "Les courbes de loss et d‚Äôaccuracy montrent si le mod√®le converge correctement ou pas.\n",
        "\n",
        "Le learning rate peut √™tre analys√© pour d√©tecter un mauvais taux d‚Äôapprentissage.\n",
        "\n",
        "La matrice de confusion permet d‚Äôidentifier les classes les plus confondues.\n",
        "\n",
        "Le rapport de classification offre une vue granul√©e sur la pr√©cision, le rappel et le F1-score de chaque classe.\n",
        "\n",
        "Ces sorties visuelles sont essentielles pour diagnostiquer les faiblesses du mod√®le (ex. biais vers la classe majoritaire, difficult√© √† d√©tecter certaines √©motions/sentiments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QsD2dF2lTSW"
      },
      "source": [
        "**Conclusion :**\n",
        "[visualization_modules.py] agit comme un miroir du comportement du mod√®le. C‚Äôest un pont entre l‚Äôapprentissage automatique et la compr√©hension humaine.\n",
        "\n",
        "Il permet :\n",
        "\n",
        "\n",
        "*   un monitoring transparent de l'entra√Ænement,\n",
        "*   une analyse fine des erreurs,\n",
        "*   et une prise de d√©cision √©clair√©e sur les prochaines √©tapes (plus de donn√©es, changement de mod√®le, etc.).\n",
        "\n",
        "\n",
        "\n",
        "En rendant les r√©sultats lisibles et interactifs, ce module transforme un projet IA technique en outil intelligible et valorisable, aussi bien pour des data scientists que pour des d√©cideurs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1n1zZTClo0P"
      },
      "source": [
        "**Int√©r√™t m√©tier :**\n",
        "Ce module permet aux parties prenantes non techniques (produit, marketing, direction) de comprendre la valeur et les limites du mod√®le, favorisant ainsi l‚Äôadoption, la confiance et les d√©cisions strat√©giques bas√©es sur la donn√©e.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1N0-XlXUKD"
      },
      "source": [
        "# 5. **qa_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COTLVwUdm7ga"
      },
      "source": [
        "Le module qa_modules.py constitue le pilier de la composante de recherche et de g√©n√©ration de r√©ponses contextuelles du projet. Il met en ≈ìuvre une recherche s√©mantique intelligente, permettant de r√©cup√©rer les documents les plus pertinents √† une requ√™te utilisateur. C‚Äôest une brique essentielle pour assurer que l‚Äôagent IA ne r√©ponde pas de mani√®re g√©n√©rique, mais bien en s‚Äôappuyant sur le contexte pertinent extrait des donn√©es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCZRxJpSCQ2u"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3XsARR6CNVK"
      },
      "source": [
        "\n",
        "**L'objectif de ce module est double :**\n",
        "\n",
        "- Indexer un corpus de textes avec un mod√®le d‚Äôembeddings (SentenceTransformer) de mani√®re √† pouvoir retrouver rapidement les passages les plus proches s√©mantiquement d‚Äôune question ou d‚Äôun texte.\n",
        "\n",
        "- Fournir une interface simple pour interroger ce corpus, r√©cup√©rer les r√©sultats les plus pertinents, et les utiliser pour alimenter des prompts dans une √©tape de g√©n√©ration (RAG ‚Äì Retrieval-Augmented Generation).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HCljPBZCU5B"
      },
      "source": [
        "**√âtapes cl√©s de la d√©marche :**\n",
        "\n",
        "\n",
        "\n",
        "*   Chargement d‚Äôun mod√®le l√©ger (all-MiniLM-L6-v2) pour encoder les textes.\n",
        "*   Indexation des documents via fit(), produisant un tableau d‚Äôembeddings.\n",
        "*   Interrogation via query(), comparant un embedding de la question aux documents via cosine_similarity.\n",
        "*   Extraction des meilleurs r√©sultats (top_k) avec leurs labels et scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdRC_3s7Ncy6",
        "outputId": "2c10926e-2f9c-4bb9-a666-c499a18aacaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting qa_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile qa_modules.py\n",
        "# qa_modules.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "import streamlit as st\n",
        "\n",
        "class QAModule:\n",
        "    \"\"\"Module de recherche et Q&A bas√© sur sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"Initialisation avec mod√®le d'embedding\"\"\"\n",
        "        try:\n",
        "            self.encoder = SentenceTransformer(model_name)\n",
        "            self.corpus_embeddings = None\n",
        "            self.corpus_texts = []\n",
        "            self.labels = []\n",
        "            self.model_name = model_name\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur chargement mod√®le Q&A: {e}\")\n",
        "            # Fallback\n",
        "            self.encoder = None\n",
        "            self.model_name = \"fallback\"\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        \"\"\"Indexe le dataset pour la recherche\"\"\"\n",
        "        if self.encoder is None:\n",
        "            st.warning(\"Module Q&A non disponible\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.corpus_texts = [item['text'] for item in dataset]\n",
        "            self.labels = [item['label_id'] for item in dataset]\n",
        "\n",
        "            with st.spinner(\"üìä Indexation des donn√©es pour la recherche...\"):\n",
        "                self.corpus_embeddings = self.encoder.encode(\n",
        "                    self.corpus_texts,\n",
        "                    convert_to_tensor=False,\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "            st.success(f\"‚úÖ {len(self.corpus_texts)} √©l√©ments index√©s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur indexation Q&A: {e}\")\n",
        "\n",
        "    def query(self, question: str, top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Recherche les textes les plus similaires √† la question\"\"\"\n",
        "        if self.encoder is None or self.corpus_embeddings is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            question_embedding = self.encoder.encode([question], convert_to_tensor=False)\n",
        "            similarities = cosine_similarity(question_embedding, self.corpus_embeddings)[0]\n",
        "\n",
        "            # Top K indices\n",
        "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "            results = []\n",
        "            for idx in top_indices:\n",
        "                results.append({\n",
        "                    \"text\": self.corpus_texts[idx],\n",
        "                    \"label_id\": int(self.labels[idx]),\n",
        "                    \"score\": float(similarities[idx]),\n",
        "                    \"rank\": len(results) + 1\n",
        "                })\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Erreur recherche Q&A: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Statistiques du module Q&A\"\"\"\n",
        "        return {\n",
        "            \"model_name\": self.model_name,\n",
        "            \"indexed_items\": len(self.corpus_texts),\n",
        "            \"embedding_dim\": len(self.corpus_embeddings[0]) if self.corpus_embeddings else 0\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqcriP0C-co"
      },
      "source": [
        "**Enjeux et Challenges**\n",
        "**Enjeux :**\n",
        "- Obtenir des r√©sultats contextuels de qualit√© pour am√©liorer la g√©n√©ration (r√©duction des hallucinations).\n",
        "\n",
        "- Permettre un acc√®s rapide √† l‚Äôinformation √† partir d‚Äôun corpus non structur√©.\n",
        "\n",
        "- Int√©grer un m√©canisme simple mais robuste de RAG, sans passer par des solutions co√ªteuses ou complexes (ex : Pinecone).\n",
        "\n",
        "**Challenges :**\n",
        "- La qualit√© des embeddings : il faut un bon √©quilibre entre l√©g√®ret√© du mod√®le (pour la vitesse) et pertinence s√©mantique.\n",
        "\n",
        "- La gestion des grands corpus : encode() peut devenir lent ou gourmand si le dataset est volumineux.\n",
        "\n",
        "- La gestion des erreurs : si l‚Äôencodage √©choue, tout le syst√®me de g√©n√©ration contextuelle est compromis.\n",
        "\n",
        "- Rendre le tout accessible via Streamlit sans alourdir l‚ÄôUX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQoQCPSDRDA"
      },
      "source": [
        "**L'utilisation de ce module permet de :**\n",
        "\n",
        "- Identifier les documents les plus pertinents pour enrichir une requ√™te utilisateur.\n",
        "\n",
        "- G√©n√©rer une base contextuelle solide pour la g√©n√©ration de texte contr√¥l√©e et inform√©e.\n",
        "\n",
        "- Alimenter des tableaux de bord ou des fonctions de r√©ponse intelligente dans l‚Äôapp (comme un chatbot enrichi ou un moteur d‚Äôaide).\n",
        "\n",
        "La m√©thode query() renvoie une liste class√©e de passages textuels, avec leur score de similarit√©, leur √©tiquette, et leur rang. Cette approche est bien plus efficace que de se baser uniquement sur des mots-cl√©s ou des r√®gles fixes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1MBtYWuDc9v"
      },
      "source": [
        "**Conclusion :**\n",
        "`qa_modules.py` incarne l‚Äôintelligence contextuelle du projet :\n",
        "il permet √† un mod√®le de g√©n√©ration de ne pas inventer des faits, mais de s‚Äôappuyer sur des passages v√©rifi√©s, proches s√©mantiquement de la question.\n",
        "\n",
        "- Il est simple, modulaire et efficace :\n",
        "\n",
        "- Il ne d√©pend pas d‚Äôun back-end lourd.\n",
        "\n",
        "- Il est facilement extensible (autres mod√®les d‚Äôembeddings, base vectorielle plus avanc√©e‚Ä¶).\n",
        "\n",
        "- Il augmente drastiquement la pertinence m√©tier des r√©ponses g√©n√©r√©es par le mod√®le LLM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGaqFer3EHQQ"
      },
      "source": [
        "**Int√©r√™t m√©tier :**\n",
        "Ce module est particuli√®rement utile dans des cas d‚Äôusage tels que :\n",
        "\n",
        "- FAQ automatis√©es intelligentes\n",
        "\n",
        "- hatbots d‚Äôassistance document√©e\n",
        "\n",
        "- Recherche d‚Äôinformation rapide pour agents internes\n",
        "\n",
        "- Syst√®mes de RAG (Retrieval-Augmented Generation) en production\n",
        "\n",
        "En clair, ce module transforme un mod√®le de g√©n√©ration standard en assistant intelligent, document√© et adapt√© au contexte utilisateur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHtCva_kXd6d"
      },
      "source": [
        "# 6. **Module Knowledge Base - knowledge_modules.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Emsl3kTEyJj"
      },
      "source": [
        "Le module `knowledge_modules.py` constitue une base de connaissances embarqu√©e ‚Äî une alternative l√©g√®re aux m√©thodes classiques de retrieval par embeddings. Il est fond√© sur des techniques simples de recherche textuelle par similarit√© lexicale, permettant d‚Äôenrichir les r√©ponses g√©n√©r√©es par un mod√®le LLM avec des faits cl√©s issus d‚Äôun corpus pr√©d√©fini.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVCI7IpE-f7"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_cytQ25E7JE"
      },
      "source": [
        "\n",
        "**Ce module vise √† :**\n",
        "\n",
        "- Fournir une base de connaissances interpr√©table, personnalisable et rapide √† int√©grer, sans d√©pendance √† des biblioth√®ques externes lourdes.\n",
        "\n",
        "- R√©cup√©rer les faits les plus pertinents par rapport √† une requ√™te utilisateur, √† l‚Äôaide d‚Äôun score de similarit√© Jaccard bas√© sur les mots communs.\n",
        "\n",
        "- Proposer un fallback utile dans des environnements √† faibles ressources (CPU only) ou sans acc√®s Internet.\n",
        "\n",
        "**D√©marche :**\n",
        "- Initialisation manuelle d‚Äôune base de 10 faits scientifiques sur le climat (dans setup_knowledge_base()).\n",
        "\n",
        "- Analyse d‚Äôune requ√™te utilisateur (via find_context()) par tokenisation simple + scoring Jaccard.\n",
        "\n",
        "- Tri des r√©sultats et renvoi des top_k plus pertinents si leur score d√©passe un seuil minimal.\n",
        "\n",
        "- Ajout dynamique possible de nouveaux faits (via add_knowledge()).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B03nOJeFI_US",
        "outputId": "161d8f92-8ee9-43ec-e909-8859513a9751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting knowledge_modules.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile knowledge_modules.py\n",
        "\n",
        "# knowledge_modules.py\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import re\n",
        "\n",
        "class KnowledgeBase:\n",
        "    \"\"\"Gestion de la base de connaissances sans sentence-transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.knowledge_base = []\n",
        "        self.setup_knowledge_base()\n",
        "\n",
        "    def setup_knowledge_base(self):\n",
        "        \"\"\"Configuration de la base de connaissances\"\"\"\n",
        "        self.knowledge_base = [\n",
        "            \"Le r√©chauffement climatique est principalement caus√© par les √©missions de gaz √† effet de serre d'origine humaine.\",\n",
        "            \"Les √©nergies renouvelables comme le solaire et l'√©olien sont essentielles pour d√©carboner notre √©conomie.\",\n",
        "            \"La d√©forestation massive contribue significativement au changement climatique.\",\n",
        "            \"Le secteur des transports repr√©sente environ 24% des √©missions mondiales de gaz √† effet de serre.\",\n",
        "            \"L'am√©lioration de l'efficacit√© √©nerg√©tique des b√¢timents peut r√©duire jusqu'√† 50% de leur consommation.\",\n",
        "            \"L'agriculture durable et r√©g√©n√©ratrice peut s√©questrer du carbone tout en produisant de la nourriture.\",\n",
        "            \"Les oc√©ans absorbent 25% du CO2 atmosph√©rique mais s'acidifient, mena√ßant les √©cosyst√®mes marins.\",\n",
        "            \"Les politiques de taxation du carbone incitent les entreprises √† r√©duire leurs √©missions.\",\n",
        "            \"L'adaptation au changement climatique est aussi cruciale que l'att√©nuation des √©missions.\",\n",
        "            \"Les technologies de capture et stockage du carbone pourraient permettre d'atteindre la neutralit√© carbone.\"\n",
        "        ]\n",
        "        print(\"‚úÖ Base de connaissances initialis√©e avec recherche par mots-cl√©s\")\n",
        "\n",
        "    def find_context(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Recherche de contexte pertinent par similarit√© textuelle simple\"\"\"\n",
        "        if not query or not self.knowledge_base:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Nettoyage et tokenisation simple\n",
        "            query_clean = query.lower()\n",
        "            query_words = set(re.findall(r'\\b\\w+\\b', query_clean))\n",
        "\n",
        "            # Score de similarit√© bas√© sur les mots communs\n",
        "            scored_docs = []\n",
        "\n",
        "            for doc in self.knowledge_base:\n",
        "                doc_clean = doc.lower()\n",
        "                doc_words = set(re.findall(r'\\b\\w+\\b', doc_clean))\n",
        "\n",
        "                # Calcul du score Jaccard\n",
        "                intersection = len(query_words & doc_words)\n",
        "                union = len(query_words | doc_words)\n",
        "\n",
        "                if union > 0:\n",
        "                    jaccard_score = intersection / union\n",
        "                    scored_docs.append((doc, jaccard_score))\n",
        "\n",
        "            # Tri par score d√©croissant\n",
        "            scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Retour des top_k documents avec score > 0.1\n",
        "            relevant_docs = []\n",
        "            for doc, score in scored_docs[:top_k]:\n",
        "                if score > 0.1:  # Seuil de pertinence\n",
        "                    relevant_docs.append(doc)\n",
        "\n",
        "            return relevant_docs\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur recherche contexte: {e}\")\n",
        "            return []\n",
        "\n",
        "    def add_knowledge(self, new_knowledge: str):\n",
        "        \"\"\"Ajouter une nouvelle connaissance\"\"\"\n",
        "        if new_knowledge and new_knowledge not in self.knowledge_base:\n",
        "            self.knowledge_base.append(new_knowledge)\n",
        "            print(f\"‚úÖ Nouvelle connaissance ajout√©e: {new_knowledge[:50]}...\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Statistiques de la base de connaissances\"\"\"\n",
        "        return {\n",
        "            \"total_documents\": len(self.knowledge_base),\n",
        "            \"avg_length\": np.mean([len(doc) for doc in self.knowledge_base]) if self.knowledge_base else 0,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1erCbaJtFkoQ"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEsFefgFFhKa"
      },
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- R√©pondre rapidement √† des questions fr√©quentes √† partir d‚Äôun corpus contr√¥l√©.\n",
        "\n",
        "- Permettre √† un LLM de g√©n√©rer du texte document√© et pr√©cis, m√™me sans pipeline de RAG complet.\n",
        "\n",
        "- √âviter les hallucinations en injectant des faits fiables dans les prompts.\n",
        "\n",
        "**Challenges :**\n",
        "- La limitation du matching lexical : sans embeddings, la pertinence s√©mantique est r√©duite.\n",
        "\n",
        "- L‚Äôapproche ne g√®re pas la polys√©mie ou la synonymie, ce qui peut faire rater des r√©sultats importants.\n",
        "\n",
        "- La base doit √™tre structur√©e √† la main, ce qui peut devenir contraignant √† grande √©chelle.\n",
        "\n",
        "- Le seuil de pertinence (score > 0.1) est empirique et n√©cessite un ajustement selon les cas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoNYOARjF4M3"
      },
      "source": [
        "Ce module renvoie une liste tri√©e des documents les plus proches d‚Äôune requ√™te, ce qui peut √™tre utilis√© pour :\n",
        "\n",
        "- Construire un prompt enrichi dans un agent conversationnel.\n",
        "\n",
        "- Justifier une r√©ponse automatique (avec la source en annexe).\n",
        "\n",
        "- Alimenter un syst√®me de recommandation ou un moteur de suggestion.\n",
        "\n",
        "Il fournit √©galement des statistiques sur la base, comme le nombre total de faits et la longueur moyenne des documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUwKlVPsGD6F"
      },
      "source": [
        "**Conclusion :**\n",
        "- `knowledge_modules.py` illustre un compromis ing√©nieux entre simplicit√©, interpr√©tabilit√© et efficacit√© :\n",
        "\n",
        "- Il √©vite le recours √† des mod√®les lourds (pas de FAISS, pas de vector store).\n",
        "\n",
        "- Il est parfaitement adapt√© pour des MVP, des environnements d√©connect√©s, ou des prototypes embarqu√©s.\n",
        "\n",
        "- Il offre une base extensible via l‚Äôajout dynamique de connaissances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDyCNTKZGYo7"
      },
      "source": [
        "**Interpr√©tation m√©tier :**\n",
        "Pour un utilisateur m√©tier (ex. : responsable RSE, charg√© de mission climat), ce module permet :\n",
        "\n",
        "- De construire un r√©f√©rentiel de faits v√©rifiables, facilement enrichissable.\n",
        "\n",
        "- De garantir que les mod√®les g√©n√©ratifs s‚Äôappuient sur des informations valid√©es, donc coh√©rentes avec la politique de l‚Äôorganisation.\n",
        "\n",
        "- De servir de fallback contextuel si la recherche par embeddings est indisponible.\n",
        "\n",
        "En somme, ce module agit comme un filet de s√©curit√© cognitif pour l‚ÄôIA g√©n√©rative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELT7z04LX4d5"
      },
      "source": [
        "# 7. **Module Streamlit - streamlit_app.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9v1uCM0G-oq"
      },
      "source": [
        "Le fichier streamlit_app.py est l'orchestrateur final du projet Climate Sentiment Analyzer, une application interactive d√©velopp√©e avec Streamlit qui permet de visualiser, entra√Æner, tester et interpr√©ter un mod√®le de classification de sentiments appliqu√© √† des textes sur le climat, tout en int√©grant des modules de g√©n√©ration de contexte et de question/r√©ponse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRoy3D5KHVBu"
      },
      "source": [
        "**Intention et D√©marche**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G1Q-RMjHSzi"
      },
      "source": [
        "\n",
        "L‚Äôobjectif de ce module est d‚Äôint√©grer de mani√®re fluide tous les composants pr√©c√©demment d√©velopp√©s (pr√©traitement, entra√Ænement, visualisation, pr√©diction, Q&A, etc.) dans une interface utilisateur accessible, sans comp√©tences techniques requises.\n",
        "\n",
        "**Les √©tapes-cl√©s de la d√©marche :**\n",
        "1. Structuration en classes :\n",
        "\n",
        "- `ClimateAnalyzerApp` : g√®re l‚Äôinterface utilisateur, les interactions et les affichages.\n",
        "\n",
        "- `PipelineOrchestrator` : sert de point d‚Äôentr√©e principal du script.\n",
        "\n",
        "2. Navigation par onglets (via selectbox) :\n",
        "\n",
        "- Pipeline Complet\n",
        "\n",
        "- Traitement des donn√©es\n",
        "\n",
        "- Gestion du mod√®le\n",
        "\n",
        "- Analyse de texte\n",
        "\n",
        "- Q&A intelligent\n",
        "\n",
        "- Visualisations\n",
        "\n",
        "3. Personnalisation UX/UI :\n",
        "\n",
        "- CSS int√©gr√© pour am√©liorer l‚Äôapparence (gradient, cards, layout responsive).\n",
        "\n",
        "- Affichage dynamique des √©tapes, erreurs, barres de progression.\n",
        "\n",
        "4. Optimisation de l‚Äôexp√©rience :\n",
        "\n",
        "- Upload de CSV pour lancer le pipeline.\n",
        "\n",
        "- Lancement √† la demande de l‚Äôentra√Ænement.\n",
        "\n",
        "- Visualisation des courbes d‚Äôapprentissage.\n",
        "\n",
        "- Interface de question-r√©ponse par similarit√©.\n",
        "\n",
        "- Analyse fine de texte en temps r√©el avec contexte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAHSrmUDJLRG",
        "outputId": "343054cd-68fc-494b-b5b7-f818b16e5266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('/content')\n",
        "from core_modules import ClimateConfig\n",
        "from data_modules import DataProcessor\n",
        "from model_modules import ModelManager\n",
        "from knowledge_modules import KnowledgeBase\n",
        "from visualization_modules import VisualizationManager\n",
        "from qa_modules import QAModule\n",
        "\n",
        "st.set_page_config(page_title=\"üåç Climate Analyzer ‚Äì Complet\", page_icon=\"üåç\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>.main-header{background:linear-gradient(135deg,#667eea,#764ba2);padding:2rem;border-radius:15px;color:white;text-align:center}</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "class ClimateAnalyzerApp:\n",
        "    def __init__(self):\n",
        "        self.config = ClimateConfig()\n",
        "        self.data_processor = DataProcessor()\n",
        "        self.model_manager = ModelManager(self.config)\n",
        "        self.knowledge_base = KnowledgeBase()\n",
        "        self.visualizer = VisualizationManager()\n",
        "        self.qa_module = QAModule()\n",
        "\n",
        "    def run(self):\n",
        "        # üîÅ Persistance via session_state\n",
        "        for key in [\"trained\", \"trainer\", \"test_ds\", \"label_mapping\"]:\n",
        "            if key not in st.session_state:\n",
        "                st.session_state[key] = None if key != \"trained\" else False\n",
        "\n",
        "        st.markdown('<div class=\"main-header\"><h1>üåç Climate Sentiment Analyzer</h1><h3>Pipeline Complet</h3></div>', unsafe_allow_html=True)\n",
        "        mode = st.sidebar.selectbox(\"Mode\", [\"üöÄ Pipeline Complet\", \"üìä Data Processing\", \"‚ùì Q&A\", \"üìà Visualisations\"])\n",
        "\n",
        "        if mode == \"üöÄ Pipeline Complet\":\n",
        "            self.run_complete_pipeline()\n",
        "        elif mode == \"üìä Data Processing\":\n",
        "            self.run_data_processing()\n",
        "        elif mode == \"‚ùì Q&A\":\n",
        "            self.run_qa_interface()\n",
        "        elif mode == \"üìà Visualisations\":\n",
        "            self.run_visualizations()\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        st.header(\"üöÄ Pipeline Complet\")\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre fichier CSV\", type=[\"csv\"])\n",
        "\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"‚úÖ Fichier charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes\")\n",
        "            st.dataframe(df.head())\n",
        "\n",
        "            sample_size = st.slider(\"Taille √©chantillon\", 1000, 10000, 4000)\n",
        "            epochs = st.slider(\"Epochs\", 1, 5, 3)\n",
        "            self.config.epochs = epochs\n",
        "\n",
        "            if st.button(\"üöÄ Lancer l'entra√Ænement\", type=\"primary\"):\n",
        "                self.run_real_training(df, sample_size)\n",
        "\n",
        "    def run_real_training(self, df, sample_size):\n",
        "        progress = st.progress(0)\n",
        "        status = st.empty()\n",
        "\n",
        "        try:\n",
        "            status.text(\"üìä Pr√©paration des donn√©es...\")\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df, sample_size)\n",
        "            progress.progress(20)\n",
        "\n",
        "            status.text(\"ü§ñ Configuration du mod√®le...\")\n",
        "            self.model_manager.setup_tokenizer()\n",
        "            num_labels = len(self.data_processor.label_mapping)\n",
        "            self.model_manager.setup_model(num_labels)\n",
        "            progress.progress(40)\n",
        "\n",
        "            def prepare_dataset(ds):\n",
        "                ds = ds.map(\n",
        "                    self.model_manager.tokenize_function,\n",
        "                    batched=True,\n",
        "                    remove_columns=[\"text\"]\n",
        "                )\n",
        "                ds = ds.rename_column(\"label_id\", \"labels\")\n",
        "                ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "                return ds\n",
        "\n",
        "            train_ds = prepare_dataset(train_ds)\n",
        "            val_ds = prepare_dataset(val_ds)\n",
        "            test_ds = prepare_dataset(test_ds)\n",
        "            progress.progress(60)\n",
        "\n",
        "            trainer = self.model_manager.setup_trainer(train_ds, val_ds)\n",
        "            progress.progress(70)\n",
        "\n",
        "            with st.spinner(\"Entra√Ænement du mod√®le...\"):\n",
        "                trainer.train()\n",
        "            progress.progress(90)\n",
        "\n",
        "            metrics = trainer.evaluate(test_ds)\n",
        "\n",
        "            qa_data = [{\"text\": item[\"text\"], \"label_id\": item[\"label_id\"]}\n",
        "                       for item in self.data_processor.prepare_datasets(df, sample_size)[0]]\n",
        "            self.qa_module.fit(qa_data)\n",
        "\n",
        "            model_path = \"outputs/final_model\"\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "            trainer.save_model(model_path)\n",
        "\n",
        "            # üîÅ Mise √† jour session_state\n",
        "            st.session_state.trained = True\n",
        "            st.session_state.trainer = trainer\n",
        "            st.session_state.test_ds = test_ds\n",
        "            st.session_state.label_mapping = self.data_processor.label_mapping\n",
        "\n",
        "            progress.progress(100)\n",
        "            st.success(\"üéâ Entra√Ænement termin√© avec succ√®s!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur d'entra√Ænement: {e}\")\n",
        "\n",
        "    def run_data_processing(self):\n",
        "        st.header(\"üìä Data Processing\")\n",
        "        uploaded_file = st.file_uploader(\"T√©l√©chargez votre fichier CSV\", type=[\"csv\"])\n",
        "        if uploaded_file:\n",
        "            df = pd.read_csv(uploaded_file)\n",
        "            st.success(f\"‚úÖ Fichier charg√©: {df.shape}\")\n",
        "            st.dataframe(df.head())\n",
        "            train_ds, val_ds, test_ds = self.data_processor.prepare_datasets(df)\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            col1.metric(\"Train\", len(train_ds))\n",
        "            col2.metric(\"Validation\", len(val_ds))\n",
        "            col3.metric(\"Test\", len(test_ds))\n",
        "\n",
        "    def run_qa_interface(self):\n",
        "        st.header(\"‚ùì Interface Q&A\")\n",
        "        if not st.session_state.get(\"trained\", False):\n",
        "            st.warning(\"‚ö†Ô∏è Veuillez d'abord entra√Æner un mod√®le.\")\n",
        "            return\n",
        "\n",
        "        question = st.text_input(\"Votre question:\", placeholder=\"Ex: Quelles sont les causes du r√©chauffement climatique?\")\n",
        "        if question:\n",
        "            results = self.qa_module.query(question, top_k=5)\n",
        "            if results:\n",
        "                for i, result in enumerate(results, 1):\n",
        "                    with st.expander(f\"R√©sultat {i} - Score: {result['score']:.3f}\"):\n",
        "                        st.write(f\"**Texte:** {result['text']}\")\n",
        "                        st.write(f\"**Label ID:** {result['label_id']}\")\n",
        "            else:\n",
        "                st.warning(\"Aucun r√©sultat trouv√©.\")\n",
        "\n",
        "    def run_visualizations(self):\n",
        "        st.header(\"üìà Visualisations\")\n",
        "        if not st.session_state.get(\"trained\", False):\n",
        "            st.warning(\"‚ö†Ô∏è Aucune donn√©e d'entra√Ænement disponible.\")\n",
        "            return\n",
        "\n",
        "        viz_option = st.selectbox(\n",
        "            \"Choisir le type de visualisation:\",\n",
        "            [\"Matrice de confusion\", \"Rapport de classification\", \"Distribution des classes\", \"F1-score par classe\", \"Courbes d'entra√Ænement\"]\n",
        "        )\n",
        "\n",
        "        trainer = st.session_state.trainer\n",
        "        test_ds = st.session_state.test_ds\n",
        "        label_mapping = st.session_state.label_mapping\n",
        "        label_names = list(label_mapping.keys())\n",
        "\n",
        "        try:\n",
        "            preds_output = trainer.predict(test_ds)\n",
        "            preds = preds_output.predictions.argmax(axis=1)\n",
        "            labels = preds_output.label_ids\n",
        "\n",
        "            if viz_option == \"Matrice de confusion\":\n",
        "                self.visualizer.show_confusion_matrix(trainer, test_ds, label_names)\n",
        "\n",
        "            elif viz_option == \"Rapport de classification\":\n",
        "                from sklearn.metrics import classification_report\n",
        "                report = classification_report(labels, preds, target_names=label_names, output_dict=True, zero_division=0)\n",
        "                st.dataframe(pd.DataFrame(report).transpose())\n",
        "\n",
        "            elif viz_option == \"Distribution des classes\":\n",
        "                self.visualizer.plot_class_distribution([label_mapping[i] for i in labels])\n",
        "\n",
        "            elif viz_option == \"F1-score par classe\":\n",
        "                self.visualizer.plot_f1_per_class(labels, preds, label_names)\n",
        "\n",
        "            elif viz_option == \"Courbes d'entra√Ænement\":\n",
        "                self.visualizer.plot_training_curves(\"outputs/runs/logs\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"‚ùå Erreur de visualisation : {e}\")\n",
        "\n",
        "class PipelineOrchestrator:\n",
        "    def __init__(self):\n",
        "        self.app = ClimateAnalyzerApp()\n",
        "\n",
        "    def run(self):\n",
        "        self.app.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    orchestrator = PipelineOrchestrator()\n",
        "    orchestrator.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIuOqTiBH9EQ"
      },
      "source": [
        "**Enjeux et Challenges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvurmPkhH7Ko"
      },
      "source": [
        "\n",
        "**Enjeux :**\n",
        "- Rendre le projet utilisable par tous : chercheurs, d√©cideurs, √©tudiants...\n",
        "\n",
        "- Permettre un test rapide du mod√®le sur des donn√©es r√©elles.\n",
        "\n",
        "- Fournir une exp√©rience fluide et compl√®te du cycle ML : de la data √† la pr√©diction.\n",
        "\n",
        "- G√©rer robustement les erreurs, les exceptions et les chemins absents.\n",
        "\n",
        "**Challenges :**\n",
        "- Int√©grer des composants h√©t√©rog√®nes (PyTorch, HuggingFace, PEFT, Streamlit, Pandas‚Ä¶).\n",
        "\n",
        "- Garder une structure lisible, modulaire et maintenable malgr√© la complexit√© croissante.\n",
        "\n",
        "- Assurer une compatibilit√© GPU/CPU sans casser le pipeline.\n",
        "\n",
        "- Offrir une exp√©rience utilisateur intuitive, sans sacrifier la puissance technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSszDp0iIHcD"
      },
      "source": [
        "Ce module donne naissance √† une web app de NLP compl√®te, dans laquelle on peut :\n",
        "\n",
        "- Charger ses propres donn√©es climatiques textuelles.\n",
        "\n",
        "- Lancer un entra√Ænement LoRA all√©g√©.\n",
        "\n",
        "- Observer les courbes d‚Äôapprentissage (loss, accuracy, LR).\n",
        "\n",
        "- Obtenir des m√©triques (f1, precision, recall).\n",
        "\n",
        "- Interroger le mod√®le pour des pr√©dictions individuelles.\n",
        "\n",
        "- Rechercher des √©l√©ments similaires gr√¢ce au module Q&A.\n",
        "\n",
        "- Obtenir un contexte s√©mantique enrichi via une base de connaissances int√©gr√©e.\n",
        "\n",
        "Le tout sans toucher √† une seule ligne de code Python, ce qui en fait un v√©ritable outil m√©tier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_mTK6LIUCS"
      },
      "source": [
        "**Conclusion :**\n",
        "\n",
        "streamlit_app.py est l'aboutissement de tout le projet : unifier les briques IA, orchestrer les √©tapes, simplifier l‚Äôexp√©rience utilisateur. C‚Äôest ce module qui transforme un ensemble de scripts en produit pr√™t √† l‚Äôemploi, d√©montrant toute la puissance d‚Äôun prototype Low-Code + IA pour des enjeux climatiques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGrc4-MnIbMG"
      },
      "source": [
        "**Interpr√©tation M√©tiers**\n",
        "Pour un utilisateur m√©tier (chercheur, communicant, analyste RSE) :\n",
        "\n",
        "- Cette interface devient un laboratoire num√©rique interactif : il peut exp√©rimenter avec des jeux de donn√©es, tester ses hypoth√®ses, comprendre la tonalit√© de certains discours.\n",
        "\n",
        "- Elle offre un moyen rapide de valider la perception des messages li√©s au climat.\n",
        "\n",
        "- Le module de Q&A permet une exploration s√©mantique fine, utile pour identifier les repr√©sentations collectives, tensions, ou id√©es dominantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu2IYPH2YF6K"
      },
      "source": [
        "# 8. **Script d'Installation - setup_pipeline.py**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKJOPexFJTP2"
      },
      "source": [
        "Ce module est con√ßu pour automatiser l'installation de toutes les d√©pendances n√©cessaires au bon fonctionnement du projet d'analyse de sentiment climatique bas√© sur un pipeline IA complet. L'objectif est de garantir que tout utilisateur puisse configurer son environnement sans erreurs ni oublis, avec une commande unique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fysRkseqJbuk"
      },
      "source": [
        "**D√©marche technique**\n",
        "Liste des d√©pendances critiques :\n",
        "- Le fichier contient une liste structur√©e des biblioth√®ques indispensables :\n",
        "\n",
        "- transformers, datasets, torch ‚Üí pour le fine-tuning des LLMs.\n",
        "\n",
        "- peft, sentence-transformers, faiss-cpu ‚Üí pour le LoRA et la recherche s√©mantique.\n",
        "\n",
        "- streamlit, plotly, matplotlib, seaborn ‚Üí pour l‚Äôinterface utilisateur et les visualisations.\n",
        "\n",
        "- scikit-learn, pandas, numpy ‚Üí pour les m√©triques, le traitement de donn√©es et les structures fondamentales.\n",
        "\n",
        "Installation dynamique avec subprocess :\n",
        "\n",
        "- Le script utilise subprocess.check_call pour lancer des commandes pip install de fa√ßon ind√©pendante, module par module.\n",
        "\n",
        "- Si une erreur survient, elle est captur√©e, signal√©e sans arr√™ter l‚Äôinstallation des autres paquets (try/except).\n",
        "\n",
        "Ex√©cution autonome :\n",
        "\n",
        "- Le if __name__ == \"__main__\" permet de lancer l‚Äôinstallation avec une seule commande :\n",
        "\n",
        "`python setup_pipeline.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUJ_36HSJTQ1",
        "outputId": "45881458-58dd-4d9a-d301-51770925b7e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting setup_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile setup_pipeline.py\n",
        "# setup_pipeline.py\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installation compl√®te des d√©pendances\"\"\"\n",
        "    packages = [\n",
        "        \"transformers>=4.36.0\",\n",
        "        \"datasets>=2.16.0\",\n",
        "        \"torch>=2.1.0\",\n",
        "        \"peft>=0.7.0\",\n",
        "        \"sentence-transformers>=2.2.0\",\n",
        "        \"faiss-cpu>=1.7.0\",\n",
        "        \"streamlit>=1.29.0\",\n",
        "        \"plotly>=5.17.0\",\n",
        "        \"scikit-learn>=1.3.0\",\n",
        "        \"matplotlib>=3.7.0\",\n",
        "        \"seaborn>=0.12.0\",\n",
        "        \"pandas>=1.5.0\",\n",
        "        \"numpy>=1.24.0\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "            print(f\"‚úÖ {package} install√©\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur avec {package}: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Installation compl√®te termin√©e!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_dependencies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUaKEEjmQEQb"
      },
      "source": [
        "**Enjeux et challenges**\n",
        "- Synchronisation des versions : Il est crucial de figer les versions compatibles pour √©viter les conflits ou des comportements instables.\n",
        "\n",
        "- Robustesse multiplateforme : Utiliser subprocess rend le script plus portable que des solutions type requirements.txt, surtout en contexte programmatique ou en notebook.\n",
        "\n",
        "- Exp√©rience utilisateur : Le module √©vite aux utilisateurs d‚Äôavoir √† g√©rer manuellement l‚Äôinstallation, source d‚Äôerreurs fr√©quentes dans les projets IA complexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDuxTb0yQLBN"
      },
      "source": [
        "**Interpr√©tation des r√©sultats**\n",
        "- Chaque d√©pendance install√©e avec succ√®s affiche un ‚úÖ.\n",
        "\n",
        "- En cas de souci (par exemple, d√©pendance manquante, connexion, conflit), le message est explicite avec ‚ö†Ô∏è.\n",
        "\n",
        "- Une fois le processus termin√©, un message de confirmation final s'affiche :\n",
        "‚úÖ Installation compl√®te termin√©e!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWQqNG7zQU-b"
      },
      "source": [
        "**Conclusion :**\n",
        "\n",
        "Ce module incarne une meilleure pratique de d√©ploiement dans tout projet IA : il centralise, fiabilise et simplifie la mise en place de l‚Äôenvironnement technique. Gr√¢ce √† ce script, toute personne ou √©quipe peut r√©pliquer l'environnement de d√©veloppement en un clic, favorisant la collaboration, la portabilit√© et la reproductibilit√© des r√©sultats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM5RW4GHLxwP",
        "outputId": "7210f0ff-38b7-4d0b-97ef-9d1c438a85a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.33.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.36.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0) (2025.7.14)\n",
            "‚úÖ transformers>=4.36.0 install√©\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (0.33.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (3.12.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.16.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0) (1.17.0)\n",
            "‚úÖ datasets>=2.16.0 install√©\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0) (3.0.2)\n",
            "‚úÖ torch>=2.1.0 install√©\n",
            "Requirement already satisfied: peft>=0.7.0 in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.7.0) (0.33.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.7.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.7.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.7.0) (0.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft>=0.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft>=0.7.0) (2025.7.14)\n",
            "‚úÖ peft>=0.7.0 install√©\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (0.33.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.0) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.0) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.0) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=2.2.0) (2025.7.14)\n",
            "‚úÖ sentence-transformers>=2.2.0 install√©\n",
            "Requirement already satisfied: faiss-cpu>=1.7.0 in /usr/local/lib/python3.11/dist-packages (1.11.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.7.0) (25.0)\n",
            "‚úÖ faiss-cpu>=1.7.0 install√©\n",
            "Requirement already satisfied: streamlit>=1.29.0 in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit>=1.29.0) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit>=1.29.0) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.29.0) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit>=1.29.0) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.29.0) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit>=1.29.0) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.29.0) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.29.0) (1.17.0)\n",
            "‚úÖ streamlit>=1.29.0 install√©\n",
            "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
            "‚úÖ plotly>=5.17.0 install√©\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0) (3.6.0)\n",
            "‚úÖ scikit-learn>=1.3.0 install√©\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
            "‚úÖ matplotlib>=3.7.0 install√©\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn>=0.12.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn>=0.12.0) (1.17.0)\n",
            "‚úÖ seaborn>=0.12.0 install√©\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0) (1.17.0)\n",
            "‚úÖ pandas>=1.5.0 install√©\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "‚úÖ numpy>=1.24.0 install√©\n",
            "‚úÖ Installation compl√®te termin√©e!\n"
          ]
        }
      ],
      "source": [
        "!python setup_pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MEnnZWsOuL5",
        "outputId": "2f203569-4fc2-472e-8afe-27d486492fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3wskx_fZLEp",
        "outputId": "282cd127-5a68-43f8-e7d2-9a67e3ab1a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pZxSn9_Q9kT"
      },
      "source": [
        "# **Streamlit + ngrok**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5BckmEJREOX"
      },
      "source": [
        "Ce script sert √† d√©marrer une application Streamlit en local et √† la rendre accessible via un lien public gr√¢ce √† ngrok, un outil de tunneling tr√®s utile en d√©veloppement collaboratif, d√©monstration ou test depuis un cloud (comme Colab, GCP ou un serveur distant)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkbykT4rRM29"
      },
      "source": [
        "|  Avantage                        |  Explication                                                            |\n",
        "| --------------------------------- | ------------------------------------------------------------------------- |\n",
        "| **Accessible depuis Internet**    | Vous pouvez tester ou faire une d√©mo √† distance, m√™me depuis un notebook. |\n",
        "| **Simple et rapide**              | Pas besoin de configurer un serveur web ou de modifier les DNS.           |\n",
        "| **Compatible Colab**              | Fonctionne parfaitement depuis un environnement Google Colab ou serveur.  |\n",
        "| **Pas besoin d'ouvrir des ports** | Id√©al en r√©seau d‚Äôentreprise ou cloud priv√© (ports souvent bloqu√©s).      |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyKJzjISjWG",
        "outputId": "cc93240d-20ba-41bc-8bc6-57ece074959a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "üöÄ Interface Streamlit disponible √† :\n",
            "NgrokTunnel: \"https://1961fef2d158.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# üîß Lancement Streamlit + ngrok (version corrig√©e)\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# 1Ô∏è‚É£ Token ngrok\n",
        "TOKEN = \"30Nciu2LDo3NzmKva2zibt2sCFL_7Ag5r9kUYyBCha12WSZ3\"\n",
        "!ngrok authtoken {TOKEN}\n",
        "\n",
        "# 2Ô∏è‚É£ Lancer l'application principale\n",
        "subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Attendre et cr√©er le tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"üöÄ Interface Streamlit disponible √† :\")\n",
        "print(public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qoix6etRWpw"
      },
      "source": [
        "Cette d√©marche est indispensable pour d√©ployer rapidement et temporairement une interface Streamlit sur le web, sans infrastructure complexe. Elle facilite les d√©mos en direct, les tests collaboratifs ou les livraisons rapides de POC (Proof of Concept) IA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1DOx7hrSKCy"
      },
      "source": [
        "# **CONCLUSION GLOBALE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7T_H3XbSOIg"
      },
      "source": [
        "Ce projet propose un pipeline complet et modulaire pour l‚Äôanalyse de sentiments climatiques √† partir de textes. Il int√®gre plusieurs √©tapes cl√©s : le pr√©traitement intelligent des donn√©es (avec d√©tection automatique des colonnes), la mod√©lisation optimis√©e avec LoRA pour fine-tuning all√©g√©, la visualisation des performances, une interface de pr√©diction et de contexte, ainsi qu‚Äôun moteur de questions-r√©ponses s√©mantiques. Chaque module a √©t√© pens√© pour √™tre robuste, r√©utilisable et facilement d√©ployable gr√¢ce √† Streamlit et ngrok. Les principaux d√©fis rencontr√©s concernent la gestion des donn√©es h√©t√©rog√®nes, la configuration efficace du mod√®le, et l'interpr√©tabilit√© des r√©sultats. Gr√¢ce √† des choix techniques adapt√©s (PEFT, Streamlit, embeddings), ce projet rend l‚Äôintelligence artificielle accessible et interactive, avec des b√©n√©fices imm√©diats pour l'exploration, la sensibilisation ou l‚Äôanalyse d'opinion sur des enjeux environnementaux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh-bUUfHSQYy"
      },
      "source": [
        "Et si demain, ces outils devenaient des assistants de d√©cision pour la transition √©cologique, jusqu‚Äôo√π pourrions-nous aller collectivement ?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
